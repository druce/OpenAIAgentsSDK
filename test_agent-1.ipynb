{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d549de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import pydantic\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "from typing import Dict, TypedDict, Type, List, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import agents\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "from agents import (Agent, Runner, Tool, ModelSettings, FunctionTool, InputGuardrail, GuardrailFunctionOutput,\n",
    "                    SQLiteSession, set_default_openai_api, set_default_openai_client\n",
    "                   )\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from prompt_loader import PromptLoader\n",
    "from log_handler import SQLiteLogHandler, setup_sqlite_logging, sanitize_error_for_logging\n",
    "from utilities import (StepStatus, WorkflowStatus,\n",
    "                       get_workflow_status_report, print_workflow_summary, \n",
    "                      )\n",
    "# from scrape import gather_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cea80dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI:            1.107.0\n",
      "OpenAI Agents SDK  0.2.11\n",
      "Pydantic           2.11.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"OpenAI:            {openai.__version__}\")\n",
    "print(f\"OpenAI Agents SDK  {agents.__version__}\")\n",
    "print(f\"Pydantic           {pydantic.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742842e",
   "metadata": {},
   "source": [
    "# Basic usage\n",
    "- Run a prompt using agents\n",
    "- Sessions\n",
    "- Route through Portkey for observability\n",
    "- Save logs\n",
    "- Link to openai for traces and evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564b1642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/drucev/projects/windsurf/OpenAIAgentsSDK'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153c333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_BASE_URL = http://localhost:8787/v1\n",
      "OPENAI_DEFAULT_HEADERS = {\"x-portkey-provider\": \"openai\"}\n"
     ]
    }
   ],
   "source": [
    "# load environment variables including OPENAI_API_KEY\n",
    "# important - for portkey\n",
    "# OPENAI_BASE_URL=\"http://localhost:8787/v1\"\n",
    "# OPENAI_DEFAULT_HEADERS='{\"x-portkey-provider\": \"openai\"}'\n",
    "# launch proxy service https://portkey.ai/docs/product/enterprise-offering/components\n",
    "# npx @portkey-ai/gateway\n",
    "# could point to a database with a portkey_config.yaml\n",
    "# logging:\n",
    "#   sink: sql\n",
    "#   database_url: postgres://user:password@localhost:5432/portkey\n",
    "# npx @portkey-ai/gateway --portkey_config.yaml\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# to run async in jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# verbose console logging if something doesn't work\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# openai_logger = logging.getLogger(\"openai\")a\n",
    "# openai_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# needed for portkey - responses API is persistent connection-oriented and seeems to not work\n",
    "set_default_openai_api(\"chat_completions\")\n",
    "\n",
    "print(\"OPENAI_BASE_URL =\", os.getenv(\"OPENAI_BASE_URL\"))\n",
    "print(\"OPENAI_DEFAULT_HEADERS =\", os.getenv(\"OPENAI_DEFAULT_HEADERS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686ed01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:30 | NewsletterAgent.newsletter_agent | INFO | Test info message\n",
      "17:49:30 | NewsletterAgent.newsletter_agent | WARNING | Test warning message\n",
      "17:49:30 | NewsletterAgent.newsletter_agent | ERROR | Test error message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'log with some bad stuff for the filter: [API_KEY_REDACTED]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_logging(session_id: str = \"default\", db_path: str = \"agent_logs.db\") -> logging.Logger:\n",
    "    \"\"\"Set up logging to console and SQLite database.\"\"\"\n",
    "\n",
    "    # Create logger\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logger = logging.getLogger(f\"NewsletterAgent.{session_id}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # SQLite handler\n",
    "    sqlite_handler = SQLiteLogHandler(db_path)\n",
    "    sqlite_handler.setLevel(logging.INFO)\n",
    "    sqlite_formatter = logging.Formatter('%(message)s')\n",
    "    sqlite_handler.setFormatter(sqlite_formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(sqlite_handler)\n",
    "\n",
    "    # Prevent propagation to root logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(\"newsletter_agent\", \"test_logs.db\")\n",
    "\n",
    "# Log some test messages\n",
    "logger.info(\"Test info message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.warning(\"Test warning message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.error(\"Test error message\", extra={\n",
    "    'step_name': 'error_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "sanitize_error_for_logging(\"log with some bad stuff for the filter: sk-proj-123456789012345678901234567890123456789012345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b4cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    default_headers=json.loads(os.getenv(\"OPENAI_DEFAULT_HEADERS\")),\n",
    ")\n",
    "\n",
    "# set the client globally\n",
    "set_default_openai_client(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06e62e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which swallow do you mean — African or European? The classic answer refers to the European swallow (Hirundo rustica): about 10–11 m/s — roughly 36–40 km/h (≈22–25 mph).  \n",
      "\n",
      "African swallow species vary but cruise in the same ballpark. If it’s carrying a coconut, you’ll need a different question.\n"
     ]
    }
   ],
   "source": [
    "# run a simple query through portkey\n",
    "# can see traces in openai https://platform.openai.com/logs?api=traces\n",
    "# potentially set up evals - https://platform.openai.com/evaluations\n",
    "!\n",
    "myagent = Agent(\n",
    "    name=\"Swallow Expert\",\n",
    "    instructions=\"You are an expert on airspeed velocities of swallows. Answer questions about swallow flight speeds with authority and humor when appropriate.\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    # these below seem to be being deprecated, you probably have to use old chat API directly on eg gpt-4o for logprobs\n",
    "    # model_settings=ModelSettings(temperature=0.0, logprobs=1, top_logprobs=1)\n",
    ")\n",
    "\n",
    "# 1) Create (or reuse) a session. Use a durable DB path if you want persistence.\n",
    "session = SQLiteSession(\"test_swallow_chat\", \"swallow.db\")\n",
    "\n",
    "# 2) First turn\n",
    "myresult = await Runner.run(myagent, \"What is the airspeed velocity of an unladen swallow?\", session=session)\n",
    "print(myresult.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c520247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Good — let’s do the physics, step by step, and see where the ≈10–11 m/s comes from.\n",
       "\n",
       "1) Choose a swallow\n",
       "- European swallow (Hirundo rustica) is the usual reference. Typical numbers:\n",
       "  - mass m ≈ 0.018 kg (18 g)\n",
       "  - wingspan b ≈ 0.32 m\n",
       "  - wing area S ≈ 0.01 m² (b × mean chord; order of 0.009–0.012 m²)\n",
       "  - air density ρ ≈ 1.225 kg/m³ (sea level)\n",
       "\n",
       "2) Basic steady‑flight lift balance\n",
       "For straight, level flight the wing must produce lift equal to weight:\n",
       "  L = W = m g.\n",
       "\n",
       "Lift is also L = 0.5 ρ S V² CL, so\n",
       "  0.5 ρ S V² CL = m g\n",
       "Solve for V:\n",
       "  V = sqrt( (2 m g) / (ρ S CL) ).\n",
       "\n",
       "You need a guess for CL (lift coefficient) in cruise. Small flapping birds in steady cruising flight typically operate at relatively low CL; a reasonable cruise value is CL ≈ 0.3 (could be 0.25–0.5 depending on posture and wingbeat).\n",
       "\n",
       "3) Plug in numbers\n",
       "Using m = 0.018 kg, g = 9.81 m/s², ρ = 1.225 kg/m³, S = 0.0096–0.01 m², CL = 0.3:\n",
       "\n",
       "- numerator: 2 m g ≈ 2 × 0.018 × 9.81 ≈ 0.353 N\n",
       "- denominator: ρ S CL ≈ 1.225 × 0.01 × 0.3 ≈ 0.003675\n",
       "- V ≈ sqrt(0.353 / 0.003675) ≈ sqrt(96.0) ≈ 9.8 m/s\n",
       "\n",
       "With slightly different reasonable choices for S or CL you get ≈9–12 m/s. Converting: 10 m/s ≈ 36 km/h ≈ 22 mph. That’s the canonical “about 10–11 m/s” number you see cited.\n",
       "\n",
       "4) Why we don’t just give a single exact number\n",
       "The lift balance gives a ballpark. A more detailed model includes drag (parasite/profile drag and induced drag), and the bird’s power output vs. speed. Power required P(V) = drag × V and has terms that scale like V³ (parasite) and 1/V (induced). Depending on whether the bird optimizes for minimum power (slow speed), minimum energy per distance (the speed for best range), or some other behavioral tradeoff (foraging vs. commuting vs. migration), the preferred speed shifts. Using plausible aerodynamic coefficients and minimizing energy per distance gives speeds of the same order — single‑digit to low‑double‑digit m/s — and field measurements (radar, tracking) for European swallows cluster around 9–12 m/s.\n",
       "\n",
       "5) Sensitivity\n",
       "- Larger wing area or higher CL → lower V.\n",
       "- Smaller S or lower CL → higher V.\n",
       "- Wind, maneuvering, foraging or carrying a coconut dramatically change the answer.\n",
       "\n",
       "Summary: the simple lift‑balance calculation with realistic swallow numbers gives ≈10 m/s; more sophisticated drag/power analyses and direct field measurements support that as the typical unladen cruising airspeed of a European swallow. And if the question arises on a bridge, don’t forget to ask “African or European?” — then answer confidently and walk on."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Next turns — just keep reusing the same session\n",
    "myresult = await Runner.run(myagent, \"ok, go ahead and explain how that number comes about\", session=session)\n",
    "\n",
    "display(Markdown(myresult.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1194bf7",
   "metadata": {},
   "source": [
    "# More advanced usage\n",
    "- Prompt Management\n",
    "- Structured JSON outputs, enables validation and safe passing downstream over long pipelines\n",
    "- Map prompts to larger data sets asynchronously (e.g. send parallel batches of 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cfea49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:55:00 | NewsletterAgent.newsletter_agent | INFO | Show available prompts\n",
      "17:55:01 | NewsletterAgent.newsletter_agent | INFO | Load a prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a content-classification assistant that labels news headlines as AI-related or not.\n",
      "Return JSON that matches the provided schema\n",
      "\n",
      "A headline is AI-related if it mentions (explicitly or implicitly):\n",
      "- Core AI models: machine learning, neural / deep / transformer networks\n",
      "- AI Applications: computer vision, NLP, robotics, autonomous driving, generative media\n",
      "- AI hardware, GPU chip supply, AI data centers and infrastructure\n",
      "- Companies or labs known for AI: OpenAI, DeepMind, Anthropic, xAI, NVIDIA, etc.\n",
      "- AI models & products: GPT-5, Gemini, Claude, Midjourney, DeepSeek, etc.\n",
      "- New AI products and AI integration into existing products/services\n",
      "- AI policy / ethics / safety / regulation / analysis\n",
      "- Research results related to AI\n",
      "- AI industry figures (Sam Altman, Demis Hassabis, Dario Amodei, etc.)\n",
      "- AI market and business developments, funding rounds, partnerships centered on AI\n",
      "- Any other news with a significant AI component\n",
      "\n",
      "Not AI-related: business software, crypto, non-AI tech, non-AI medical devices, and anything else.\n",
      "\n",
      "No markdown, no explanations, just the JSON. \n",
      "Classify the following headline(s): \n",
      "{input_str} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:55:02 | NewsletterAgent.newsletter_agent | INFO | Show prompt metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_recommendations': ['gpt-4o-mini'], 'use_case': 'AI headline classification for news aggregation'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:55:03 | NewsletterAgent.newsletter_agent | INFO | Format a prompt with input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'You are a content-classification assistant that labels news headlines as AI-related or not.\\nReturn JSON that matches the provided schema\\n\\nA headline is AI-related if it mentions (explicitly or implicitly):\\n- Core AI models: machine learning, neural / deep / transformer networks\\n- AI Applications: computer vision, NLP, robotics, autonomous driving, generative media\\n- AI hardware, GPU chip supply, AI data centers and infrastructure\\n- Companies or labs known for AI: OpenAI, DeepMind, Anthropic, xAI, NVIDIA, etc.\\n- AI models & products: GPT-5, Gemini, Claude, Midjourney, DeepSeek, etc.\\n- New AI products and AI integration into existing products/services\\n- AI policy / ethics / safety / regulation / analysis\\n- Research results related to AI\\n- AI industry figures (Sam Altman, Demis Hassabis, Dario Amodei, etc.)\\n- AI market and business developments, funding rounds, partnerships centered on AI\\n- Any other news with a significant AI component\\n\\nNot AI-related: business software, crypto, non-AI tech, non-AI medical devices, and anything else.\\n\\nNo markdown, no explanations, just the JSON.', 'user': \"Classify the following headline(s): \\nAI Is Replacing Online Moderators, But It's Bad at the Job\"}\n"
     ]
    }
   ],
   "source": [
    "# get prompts from the prompt repository (the promptfoo yaml files)\n",
    "# langfuse probably a better enterprise option\n",
    "# prompt repository solution allows us to run evals, version prompts, improving performance over time\n",
    "\n",
    "logger.info(\"Show available prompts\")\n",
    "my_prompt_loader = PromptLoader()\n",
    "my_prompt_loader.list_available_prompts()\n",
    "\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = my_prompt_loader.load_prompt_by_name(prompt_name)\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Load a prompt\")\n",
    "print(prompt_dict.get('system'), \"\")\n",
    "print(prompt_dict.get('user'), \"\")\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Show prompt metadata\")\n",
    "prompt_metadata = my_prompt_loader.get_prompt_metadata(prompt_name)\n",
    "print(prompt_metadata)\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Format a prompt with input\")\n",
    "print(my_prompt_loader.format_prompt(prompt_name, input_str=\"AI Is Replacing Online Moderators, But It's Bad at the Job\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f123315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output class for classifying headlines\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"A single headline classification result\"\"\"\n",
    "    input_str: str = Field(description=\"The original headline text\")\n",
    "    output: bool = Field(description=\"Whether the headline is AI-related\")\n",
    "\n",
    "class ClassificationResultList(BaseModel):\n",
    "    \"\"\"List of ClassificationResult for batch processing\"\"\"\n",
    "    results_list: list[ClassificationResult] = Field(description=\"List of classification results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bc3c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    default_headers=json.loads(os.getenv(\"OPENAI_DEFAULT_HEADERS\")),\n",
    ")\n",
    "\n",
    "# set the client globally\n",
    "set_default_openai_client(client)\n",
    "\n",
    "class ClassifierAgent(Agent):\n",
    "    \"\"\"Agent for classifying headlines as AI-related or not\n",
    "    or more generally apply a prompt to a string for a classification according to an output type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_prompt: str,\n",
    "                 user_prompt: str,\n",
    "                 output_type: Type[BaseModel],\n",
    "                 model: str,\n",
    "                 verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifierAgent\n",
    "\n",
    "        Args:\n",
    "            system_prompt: The system prompt template to use\n",
    "            user_prompt: The user prompt template to use\n",
    "            output_type: Pydantic model class for structured output\n",
    "            verbose: Enable verbose logging\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=\"ClassifierAgent\",\n",
    "            model=model,\n",
    "            instructions=system_prompt,\n",
    "            output_type=output_type\n",
    "        )\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f\"\"\"Initialized ClassifierAgent:\n",
    "system_prompt:\n",
    "{self.system_prompt}\n",
    "user_prompt:\n",
    "{self.user_prompt}\n",
    "output_type:         {output_type.__name__}\n",
    "model:               {self.model}\n",
    "schema:              {json.dumps(output_type.model_json_schema(), indent=2)}\n",
    "\"\"\")\n",
    "    @retry(\n",
    "        retry=retry_if_exception_type((openai.APIConnectionError,\n",
    "                                       openai.APITimeoutError,\n",
    "                                       openai.InternalServerError)),\n",
    "        stop=stop_after_attempt(5),  # 5 attempts sufficient for classification\n",
    "        after=lambda retry_state: log(sanitize_error_for_logging(\n",
    "            f\"Attempt {retry_state.attempt_number}: {retry_state.outcome.exception()}, tag: {retry_state.args[1].get('tag', '')}\")),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    )\n",
    "\n",
    "    async def classify(self, input_str: str) -> Type[BaseModel]:\n",
    "        \"\"\"\n",
    "        Classify a single input or a string with multiple inputs to the specified type\n",
    "\n",
    "        Args:\n",
    "            input: The input text to classify\n",
    "\n",
    "        Returns:\n",
    "            The specified type\n",
    "        \"\"\"\n",
    "        user_message = self.user_prompt.format(input_str=input_str)\n",
    "        if self.verbose:\n",
    "            logger.info(f\"User message: {user_message}\")\n",
    "\n",
    "        results_list = await Runner.run(self, user_message)\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Result: {results_list}\")\n",
    "        return results_list\n",
    "\n",
    "    async def classify_batch(self, input_list: List[str], batch_size: int = 25,\n",
    "                             *, max_concurrency: int = 16, retries: int = 3\n",
    "                            ) -> Any:\n",
    "\n",
    "        \"\"\"\n",
    "        Classify a list using paged, parallel calls to `self.classify()`,\n",
    "        preserving the original input order and validating page sizes.\n",
    "        \"\"\"\n",
    "        # Type must have a 'results_list' element\n",
    "        null_return = self.output_type(results_list=[])\n",
    "        if not input_list:\n",
    "            return null_return\n",
    "\n",
    "        pages = [input_list[i:i+batch_size]\n",
    "                 for i in range(0, len(input_list), batch_size)]\n",
    "        sem = asyncio.Semaphore(max_concurrency)\n",
    "        logger.info(f\"Sending {len(pages)} batches with concurrency {max_concurrency}\")\n",
    "\n",
    "        async def _guarded_classify(page_idx: int, items: List[str]) -> self.output_type:\n",
    "            for i in range(retries):\n",
    "                input_str = \"\\n\".join(items)\n",
    "                try:\n",
    "                    async with sem:\n",
    "                        result = await self.classify(input_str)\n",
    "                    res = result.final_output\n",
    "#                     print(type(res))\n",
    "#                     print(\"----\")\n",
    "#                     print(res)\n",
    "#                     print(\"----\")\n",
    "                    if not hasattr(res, \"results_list\"):\n",
    "                        raise ValueError(\"Bad structured output or missing 'results_list'.\")\n",
    "                    if not isinstance(res.results_list, list):\n",
    "                        raise ValueError(\"Structured output invalid 'results_list'.\")\n",
    "                    if len(res.results_list) != len(items):\n",
    "                        raise ValueError(\n",
    "                            f\"Page {page_idx}: count mismatch (got {len(res.results_list)} vs expected {len(items)}).\"\n",
    "                        )\n",
    "                    return (page_idx, res)\n",
    "                except Exception as e:\n",
    "                    last_exc = e\n",
    "                    logger.info(f\"[page {page_idx}] attempt {i+1}/{retries} failed: {e}\")\n",
    "                    if i < retries:\n",
    "                        await asyncio.sleep(2 ** i)  # 1s, 2s, 4s backoff\n",
    "\n",
    "            return page_idx, last_exc if last_exc else RuntimeError(f\"Unknown error on page {page_idx}\")\n",
    "\n",
    "        tasks = [\n",
    "            asyncio.create_task(_guarded_classify(i, page))\n",
    "            for i, page in enumerate(pages)\n",
    "        ]\n",
    "        page_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Reassemble in original order\n",
    "        flattened_results = []\n",
    "        for idx, res_or_exc in page_results:\n",
    "            if isinstance(res_or_exc, Exception):\n",
    "                raise res_or_exc\n",
    "            elif res_or_exc:\n",
    "                flattened_results.extend(res_or_exc.results_list)\n",
    "            else:\n",
    "                logger.info(f\"no results for page {idx}\")\n",
    "\n",
    "        final = self.output_type(results_list=flattened_results)\n",
    "\n",
    "        # Final sanity check\n",
    "        if len(final.results_list) != len(input_list):\n",
    "            raise ValueError(f\"Final count mismatch: expected {len(input_list)} results, got {len(flattened.results_list)}.\")\n",
    "\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dd3de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:55:22 | NewsletterAgent.newsletter_agent | INFO | Initialized ClassifierAgent:\n",
      "system_prompt:\n",
      "You are a content-classification assistant that labels news headlines as AI-related or not.\n",
      "Return JSON that matches the provided schema\n",
      "\n",
      "A headline is AI-related if it mentions (explicitly or implicitly):\n",
      "- Core AI models: machine learning, neural / deep / transformer networks\n",
      "- AI Applications: computer vision, NLP, robotics, autonomous driving, generative media\n",
      "- AI hardware, GPU chip supply, AI data centers and infrastructure\n",
      "- Companies or labs known for AI: OpenAI, DeepMind, Anthropic, xAI, NVIDIA, etc.\n",
      "- AI models & products: GPT-5, Gemini, Claude, Midjourney, DeepSeek, etc.\n",
      "- New AI products and AI integration into existing products/services\n",
      "- AI policy / ethics / safety / regulation / analysis\n",
      "- Research results related to AI\n",
      "- AI industry figures (Sam Altman, Demis Hassabis, Dario Amodei, etc.)\n",
      "- AI market and business developments, funding rounds, partnerships centered on AI\n",
      "- Any other news with a significant AI component\n",
      "\n",
      "Not AI-related: business software, crypto, non-AI tech, non-AI medical devices, and anything else.\n",
      "\n",
      "No markdown, no explanations, just the JSON.\n",
      "user_prompt:\n",
      "Classify the following headline(s): \n",
      "{input_str}\n",
      "output_type:         ClassificationResult\n",
      "model:               gpt-5-mini\n",
      "schema:              {\n",
      "  \"description\": \"A single headline classification result\",\n",
      "  \"properties\": {\n",
      "    \"input_str\": {\n",
      "      \"description\": \"The original headline text\",\n",
      "      \"title\": \"Input Str\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"output\": {\n",
      "      \"description\": \"Whether the headline is AI-related\",\n",
      "      \"title\": \"Output\",\n",
      "      \"type\": \"boolean\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"input_str\",\n",
      "    \"output\"\n",
      "  ],\n",
      "  \"title\": \"ClassificationResult\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "17:55:22 | NewsletterAgent.newsletter_agent | INFO | Initialized ClassifierAgent:\n",
      "system_prompt:\n",
      "You are a content-classification assistant that labels news headlines as AI-related or not.\n",
      "Return JSON that matches the provided schema\n",
      "\n",
      "A headline is AI-related if it mentions (explicitly or implicitly):\n",
      "- Core AI models: machine learning, neural / deep / transformer networks\n",
      "- AI Applications: computer vision, NLP, robotics, autonomous driving, generative media\n",
      "- AI hardware, GPU chip supply, AI data centers and infrastructure\n",
      "- Companies or labs known for AI: OpenAI, DeepMind, Anthropic, xAI, NVIDIA, etc.\n",
      "- AI models & products: GPT-5, Gemini, Claude, Midjourney, DeepSeek, etc.\n",
      "- New AI products and AI integration into existing products/services\n",
      "- AI policy / ethics / safety / regulation / analysis\n",
      "- Research results related to AI\n",
      "- AI industry figures (Sam Altman, Demis Hassabis, Dario Amodei, etc.)\n",
      "- AI market and business developments, funding rounds, partnerships centered on AI\n",
      "- Any other news with a significant AI component\n",
      "\n",
      "Not AI-related: business software, crypto, non-AI tech, non-AI medical devices, and anything else.\n",
      "\n",
      "No markdown, no explanations, just the JSON.\n",
      "user_prompt:\n",
      "Classify the following headline(s): \n",
      "{input_str}\n",
      "output_type:         ClassificationResult\n",
      "model:               gpt-5-mini\n",
      "schema:              {\n",
      "  \"description\": \"A single headline classification result\",\n",
      "  \"properties\": {\n",
      "    \"input_str\": {\n",
      "      \"description\": \"The original headline text\",\n",
      "      \"title\": \"Input Str\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"output\": {\n",
      "      \"description\": \"Whether the headline is AI-related\",\n",
      "      \"title\": \"Output\",\n",
      "      \"type\": \"boolean\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"input_str\",\n",
      "    \"output\"\n",
      "  ],\n",
      "  \"title\": \"ClassificationResult\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "17:55:22 | NewsletterAgent.newsletter_agent | INFO | User message: Classify the following headline(s): \n",
      "AI Is Replacing Online Moderators, But It's Bad at the Job\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:55:25 | NewsletterAgent.newsletter_agent | INFO | Result: RunResult:\n",
      "- Last agent: Agent(name=\"ClassifierAgent\", ...)\n",
      "- Final output (ClassificationResult):\n",
      "    {\n",
      "      \"input_str\": \"AI Is Replacing Online Moderators, But It's Bad at the Job\",\n",
      "      \"output\": true\n",
      "    }\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "17:55:25 | NewsletterAgent.newsletter_agent | INFO | User message: Classify the following headline(s): \n",
      "Baby Trapped in Refrigerator Eats Own Foot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_str=\"AI Is Replacing Online Moderators, But It's Bad at the Job\" output=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:55:28 | NewsletterAgent.newsletter_agent | INFO | Result: RunResult:\n",
      "- Last agent: Agent(name=\"ClassifierAgent\", ...)\n",
      "- Final output (ClassificationResult):\n",
      "    {\n",
      "      \"input_str\": \"Baby Trapped in Refrigerator Eats Own Foot\",\n",
      "      \"output\": false\n",
      "    }\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_str='Baby Trapped in Refrigerator Eats Own Foot' output=False\n"
     ]
    }
   ],
   "source": [
    "# send singly\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResult,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=True)\n",
    "\n",
    "test_headlines = [\n",
    "    \"AI Is Replacing Online Moderators, But It's Bad at the Job\",\n",
    "    \"Baby Trapped in Refrigerator Eats Own Foot\",\n",
    "    \"Machine Learning Breakthrough in Medical Diagnosis\",\n",
    "    \"Local Restaurant Opens New Location\",\n",
    "    \"ChatGPT Usage Soars in Educational Settings\"\n",
    "]\n",
    "\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResult,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=True)\n",
    "\n",
    "result = await classifier.classify(test_headlines[0])\n",
    "print(result.final_output)\n",
    "result = await classifier.classify(test_headlines[1])\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1214b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:55:39 | NewsletterAgent.newsletter_agent | INFO | Initialized ClassifierAgent:\n",
      "system_prompt:\n",
      "You are a content-classification assistant that labels news headlines as AI-related or not.\n",
      "Return JSON that matches the provided schema\n",
      "\n",
      "A headline is AI-related if it mentions (explicitly or implicitly):\n",
      "- Core AI models: machine learning, neural / deep / transformer networks\n",
      "- AI Applications: computer vision, NLP, robotics, autonomous driving, generative media\n",
      "- AI hardware, GPU chip supply, AI data centers and infrastructure\n",
      "- Companies or labs known for AI: OpenAI, DeepMind, Anthropic, xAI, NVIDIA, etc.\n",
      "- AI models & products: GPT-5, Gemini, Claude, Midjourney, DeepSeek, etc.\n",
      "- New AI products and AI integration into existing products/services\n",
      "- AI policy / ethics / safety / regulation / analysis\n",
      "- Research results related to AI\n",
      "- AI industry figures (Sam Altman, Demis Hassabis, Dario Amodei, etc.)\n",
      "- AI market and business developments, funding rounds, partnerships centered on AI\n",
      "- Any other news with a significant AI component\n",
      "\n",
      "Not AI-related: business software, crypto, non-AI tech, non-AI medical devices, and anything else.\n",
      "\n",
      "No markdown, no explanations, just the JSON.\n",
      "user_prompt:\n",
      "Classify the following headline(s): \n",
      "{input_str}\n",
      "output_type:         ClassificationResultList\n",
      "model:               gpt-5-mini\n",
      "schema:              {\n",
      "  \"$defs\": {\n",
      "    \"ClassificationResult\": {\n",
      "      \"description\": \"A single headline classification result\",\n",
      "      \"properties\": {\n",
      "        \"input_str\": {\n",
      "          \"description\": \"The original headline text\",\n",
      "          \"title\": \"Input Str\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"output\": {\n",
      "          \"description\": \"Whether the headline is AI-related\",\n",
      "          \"title\": \"Output\",\n",
      "          \"type\": \"boolean\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"input_str\",\n",
      "        \"output\"\n",
      "      ],\n",
      "      \"title\": \"ClassificationResult\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  },\n",
      "  \"description\": \"List of ClassificationResult for batch processing\",\n",
      "  \"properties\": {\n",
      "    \"results_list\": {\n",
      "      \"description\": \"List of classification results\",\n",
      "      \"items\": {\n",
      "        \"$ref\": \"#/$defs/ClassificationResult\"\n",
      "      },\n",
      "      \"title\": \"Results List\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"results_list\"\n",
      "  ],\n",
      "  \"title\": \"ClassificationResultList\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "17:55:39 | NewsletterAgent.newsletter_agent | INFO | User message: Classify the following headline(s): \n",
      "[\"AI Is Replacing Online Moderators, But It's Bad at the Job\", 'Baby Trapped in Refrigerator Eats Own Foot', 'Machine Learning Breakthrough in Medical Diagnosis', 'Local Restaurant Opens New Location', 'ChatGPT Usage Soars in Educational Settings']\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "17:55:46 | NewsletterAgent.newsletter_agent | INFO | Result: RunResult:\n",
      "- Last agent: Agent(name=\"ClassifierAgent\", ...)\n",
      "- Final output (ClassificationResultList):\n",
      "    {\n",
      "      \"results_list\": [\n",
      "        {\n",
      "          \"input_str\": \"AI Is Replacing Online Moderators, But It's Bad at the Job\",\n",
      "          \"output\": true\n",
      "        },\n",
      "        {\n",
      "          \"input_str\": \"Baby Trapped in Refrigerator Eats Own Foot\",\n",
      "          \"output\": false\n",
      "        },\n",
      "        {\n",
      "          \"input_str\": \"Machine Learning Breakthrough in Medical Diagnosis\",\n",
      "          \"output\": true\n",
      "        },\n",
      "        {\n",
      "          \"input_str\": \"Local Restaurant Opens New Location\",\n",
      "          \"output\": false\n",
      "        },\n",
      "        {\n",
      "          \"input_str\": \"ChatGPT Usage Soars in Educational Settings\",\n",
      "          \"output\": true\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_list=[ClassificationResult(input_str=\"AI Is Replacing Online Moderators, But It's Bad at the Job\", output=True), ClassificationResult(input_str='Baby Trapped in Refrigerator Eats Own Foot', output=False), ClassificationResult(input_str='Machine Learning Breakthrough in Medical Diagnosis', output=True), ClassificationResult(input_str='Local Restaurant Opens New Location', output=False), ClassificationResult(input_str='ChatGPT Usage Soars in Educational Settings', output=True)]\n"
     ]
    }
   ],
   "source": [
    "# send a single batch with verbose\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResultList,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=True)\n",
    "\n",
    "result = await classifier.classify(str(test_headlines))\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4498281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>src</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>GitHub will be folded into Microsoft proper as...</td>\n",
       "      <td>https://arstechnica.com/gadgets/2025/08/github...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>With new in-house models, Microsoft lays the g...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/08/with-new-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Google improves Gemini AI image editing with “...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/08/google-impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228</td>\n",
       "      <td>20</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Google warns that mass data theft hitting Sale...</td>\n",
       "      <td>https://arstechnica.com/security/2025/08/googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181</td>\n",
       "      <td>23</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>AI Wants More Data. More Chips. More Real Esta...</td>\n",
       "      <td>https://www.bloomberg.com/news/features/2024-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>73</td>\n",
       "      <td>758</td>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Reframing Jensen’s Law: ‘Buy more, make more’ ...</td>\n",
       "      <td>https://siliconangle.com/2025/08/30/reframing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>176</td>\n",
       "      <td>759</td>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Zeta Global (ZETA) Target Raised by Goldman as...</td>\n",
       "      <td>https://finance.yahoo.com/news/zeta-global-zet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>8</td>\n",
       "      <td>763</td>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Luis Enrique names his squad to face Toulouse</td>\n",
       "      <td>https://onefootball.com/en/news/luis-enrique-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>24</td>\n",
       "      <td>773</td>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>CorelDRAW Graphics Suite 2025 v26.2.0.170</td>\n",
       "      <td>https://post.rlsbb.cc/coreldraw-graphics-suite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>151</td>\n",
       "      <td>777</td>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Kenya pushes for local medical innovations to ...</td>\n",
       "      <td>https://www.standardmedia.co.ke/health/health-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   id           src  \\\n",
       "0            71    0  Ars Technica   \n",
       "1           137   10  Ars Technica   \n",
       "2            46   16  Ars Technica   \n",
       "3           228   20  Ars Technica   \n",
       "4           181   23     Bloomberg   \n",
       "..          ...  ...           ...   \n",
       "245          73  758       NewsAPI   \n",
       "246         176  759       NewsAPI   \n",
       "247           8  763       NewsAPI   \n",
       "248          24  773       NewsAPI   \n",
       "249         151  777       NewsAPI   \n",
       "\n",
       "                                                 title  \\\n",
       "0    GitHub will be folded into Microsoft proper as...   \n",
       "1    With new in-house models, Microsoft lays the g...   \n",
       "2    Google improves Gemini AI image editing with “...   \n",
       "3    Google warns that mass data theft hitting Sale...   \n",
       "4    AI Wants More Data. More Chips. More Real Esta...   \n",
       "..                                                 ...   \n",
       "245  Reframing Jensen’s Law: ‘Buy more, make more’ ...   \n",
       "246  Zeta Global (ZETA) Target Raised by Goldman as...   \n",
       "247      Luis Enrique names his squad to face Toulouse   \n",
       "248          CorelDRAW Graphics Suite 2025 v26.2.0.170   \n",
       "249  Kenya pushes for local medical innovations to ...   \n",
       "\n",
       "                                                   url  \n",
       "0    https://arstechnica.com/gadgets/2025/08/github...  \n",
       "1    https://arstechnica.com/ai/2025/08/with-new-in...  \n",
       "2    https://arstechnica.com/ai/2025/08/google-impr...  \n",
       "3    https://arstechnica.com/security/2025/08/googl...  \n",
       "4    https://www.bloomberg.com/news/features/2024-1...  \n",
       "..                                                 ...  \n",
       "245  https://siliconangle.com/2025/08/30/reframing-...  \n",
       "246  https://finance.yahoo.com/news/zeta-global-zet...  \n",
       "247  https://onefootball.com/en/news/luis-enrique-n...  \n",
       "248  https://post.rlsbb.cc/coreldraw-graphics-suite...  \n",
       "249  https://www.standardmedia.co.ke/health/health-...  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make batches and send multiple in parallel\n",
    "headlines_df = pd.read_csv(\"test_headlines.csv\")\n",
    "headlines_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bed9a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:55:54 | NewsletterAgent.newsletter_agent | INFO | classify headlines as AI-related or not\n",
      "17:55:54 | NewsletterAgent.newsletter_agent | INFO | Sending 10 batches with concurrency 16\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassificationResultList(results_list=[ClassificationResult(input_str='GitHub will be folded into Microsoft proper as CEO steps down', output=False), ClassificationResult(input_str='With new in-house models, Microsoft lays the groundwork for independence from OpenAI', output=True), ClassificationResult(input_str='Google improves Gemini AI image editing with “nano banana” model', output=True), ClassificationResult(input_str='Google warns that mass data theft hitting Salesloft AI agent has grown bigger', output=True), ClassificationResult(input_str='AI Wants More Data. More Chips. More Real Estate. More Power. More Water. More Everything', output=True), ClassificationResult(input_str='Opinion: China Just Got a Big Leg Up in the AI Race', output=True), ClassificationResult(input_str='Jack Ma-Backed Ant’s Profit Dives 60% After AI, Global Expansion', output=True), ClassificationResult(input_str='Dell Falls After Reporting Tighter Profit Margins on Servers', output=False), ClassificationResult(input_str='AI Billionaire Lucy Guo Pushes Into Crowded Social Media FieldScale AI co-founder’s new venture, Passes,\\xa0confronts established rivals and\\xa0lawsuits', output=True), ClassificationResult(input_str='Alibaba Shows Progress in China AI Push, Lifting Shares', output=True), ClassificationResult(input_str='Vercel Triples Valuation to $9 Billion With Accel Investment', output=False), ClassificationResult(input_str='Bain Is Said to Draw Chinese Bidders for $4 Billion Data Centers', output=False), ClassificationResult(input_str='TikTok salaries revealed: How much it pays workers in key areas like e-commerce, AI, and search', output=True), ClassificationResult(input_str=\"Grindr's CEO says there is a 'VC bubble' forming around AI\", output=True), ClassificationResult(input_str='The best college laptops of 2025: Top models from Acer, Apple, and Asus', output=False), ClassificationResult(input_str='The best Bose headphones of 2025', output=False), ClassificationResult(input_str='Are the billionaires ditching Burning Man?', output=False), ClassificationResult(input_str='Leaked Microsoft pay data shows how much hundreds of employees report making in AI, cloud, and other teams', output=True), ClassificationResult(input_str='Chinese AI chipmaker Cambricon posts record profit as Beijing pushes pivot from Nvidia', output=True), ClassificationResult(input_str='Whitehall hands out AI contracts worth £573mn in efficiency push', output=True), ClassificationResult(input_str='Who owns the copyright for AI work?', output=True), ClassificationResult(input_str='Japanese media groups sue AI search engine Perplexity over alleged copyright infringement', output=True), ClassificationResult(input_str='How AI can recode the difficult process of drug discovery', output=True), ClassificationResult(input_str='Microsoft talks set to push OpenAI’s restructure into next year', output=True), ClassificationResult(input_str='The calculated silence of America’s business and finance leaders', output=False), ClassificationResult(input_str='The kidults are more than alright for toy companies', output=False), ClassificationResult(input_str='Trump’s Fed meddling pushes investors closer to their red line', output=False), ClassificationResult(input_str='Is it time to sell your AI stocks?', output=True), ClassificationResult(input_str='Zuckerberg’s AI hires disrupt Meta with swift exits and threats to leave', output=True), ClassificationResult(input_str='Rolls-Royce explores small nuclear reactor unit funding options including IPO', output=False), ClassificationResult(input_str='AI investors are navigating ‘peak ambiguity’, says General Catalyst chief', output=True), ClassificationResult(input_str='AI companies want to prove productivity gains — but there’s a risk we may create software with inbuilt problems', output=True), ClassificationResult(input_str='Why are UK borrowing costs so high?', output=False), ClassificationResult(input_str='It’s time to shut up and get on with it', output=False), ClassificationResult(input_str='Prominent Ukrainian nationalist politician shot dead in Lviv', output=False), ClassificationResult(input_str='Manchester United: lessons from the fall', output=False), ClassificationResult(input_str='Musk’s xAI sues Apple and OpenAI over ChatGPT and iPhone integration', output=True), ClassificationResult(input_str='Publishers race to counter ‘Google Zero’ threat as AI changes search engines', output=True), ClassificationResult(input_str='Meta to license AI technology from start-up as in-house models lag rivals', output=True), ClassificationResult(input_str='Bitcoin boom sees newly wealthy splurging on luxury travel', output=False), ClassificationResult(input_str='The battle for the soul of Israel', output=False), ClassificationResult(input_str='Sending out an SOS: Save our Secretaries', output=False), ClassificationResult(input_str='Bay Area home sales are cooling — but AI-bolstered SF is heating up', output=True), ClassificationResult(input_str='Finally, all your favorite AI models in one place', output=True), ClassificationResult(input_str='The end of dashboards? GenAI and agentic workflows transform business intelligence', output=True), ClassificationResult(input_str='Inside an AI animation studio: how we’re rewriting the rules', output=True), ClassificationResult(input_str='The Trump Administration Benefits and Detriments to Public Welfare according to ChatGPT', output=True), ClassificationResult(input_str=\"How workers in their 80s and 90s are embracing AI: 'I want to finish strong'\", output=True), ClassificationResult(input_str=\"AI Lies to You Because It Thinks That's What You Want\", output=True), ClassificationResult(input_str=\"It's not just the workplace. Loyalty is becoming a thing of the past.\", output=False), ClassificationResult(input_str='A Single Typo in Your Medical Records Can Make Your AI Doctor Go Dangerously Haywire', output=True), ClassificationResult(input_str='China Has a Different Vision for AI. It Might Be Smarter.', output=True), ClassificationResult(input_str=\"Apple iPhone 17 event — Here's the 8 new products expected to launch\", output=False), ClassificationResult(input_str='Can Nvidia Keep Its AI Crown?', output=True), ClassificationResult(input_str=\"Five Indie Bands Quit Spotify After Founder's AI Weapons Tech Investment\", output=True), ClassificationResult(input_str=\"Gen Z are eyeing up 'secure' healthcare jobs to AI-proof their careers, but be warned: chiropractors, doctors and paramedics are the unhappiest workers\", output=True), ClassificationResult(input_str=\"Applying for jobs has never been easier. That's exactly the problem.\", output=False), ClassificationResult(input_str='Texas AI law aims for guardrails where few exist', output=True), ClassificationResult(input_str='Why do we keep making robots dance?', output=True), ClassificationResult(input_str='The Ultimate Apple Notes Guide for Students', output=False), ClassificationResult(input_str=\"Time magazine honors Pope Leo as 'spiritual counterweight' to Silicon Valley on AI\", output=True), ClassificationResult(input_str='A24’s dance with theartificial intelligencedevil.', output=True), ClassificationResult(input_str=\"Danbury equips school buses with AI cameras to deter stop-arm violations: 'Tool to prevent tragedy'\", output=True), ClassificationResult(input_str='The AI Advantage: Learning Any Subject from a Digital Mastermind', output=True), ClassificationResult(input_str='Should You Forget Palantir Technologies and Buy ThisArtificial Intelligence(AI) Stock Right Now?', output=True), ClassificationResult(input_str='How ‘Clanker’ Became an Anti-A.I. Rallying Cry', output=True), ClassificationResult(input_str='High school in suburban Oak Lawn to implement AI gun detection technology', output=True), ClassificationResult(input_str='Humans are being hired to make AI slop look less sloppy', output=True), ClassificationResult(input_str=\"How NMC Healthcare Uses Snowflake's Cloud-Based AI Insights\", output=True), ClassificationResult(input_str='Bitwig Studio 6 details revealed, and editing gets a big boost', output=False), ClassificationResult(input_str='Hardening Firefox – a checklist for improved browser privacy', output=False), ClassificationResult(input_str='What Are Traces and Spans in OpenTelemetry?', output=False), ClassificationResult(input_str='AI models need a virtual machine', output=True), ClassificationResult(input_str=\"Condor's Cuzco RISC-V Core at Hot Chips 2025\", output=False), ClassificationResult(input_str='Six months into tariffs, businesses have no idea how to price anything', output=False), ClassificationResult(input_str='Channel3 (YC S25) Is Hiring a Founding Engineer, NYC', output=False), ClassificationResult(input_str='Compositional Datalog on SQL: Relational Algebra of the Environment', output=False), ClassificationResult(input_str='Pig lung transplanted into a human', output=False), ClassificationResult(input_str='Cognitive load is what matters', output=False), ClassificationResult(input_str='Bcachefs Goes to \"Externally Maintained\"', output=False), ClassificationResult(input_str='Is it possible to allow sideloading and keep users safe?', output=False), ClassificationResult(input_str='New research reveals longevity gains slowing, life expectancy of 100 unlikely', output=False), ClassificationResult(input_str='Show HN: Hacker News em dash user leaderboard pre-ChatGPT', output=True), ClassificationResult(input_str=\"We rebuilt Cloud Life's infrastructure delivery with System Initiative\", output=False), ClassificationResult(input_str='New interpretations suggest the \"heat death\" hypothesis might not hold (2023)', output=False), ClassificationResult(input_str='Nokia’s legendary font makes for a great user interface font', output=False), ClassificationResult(input_str='Do the simplest thing that could possibly work', output=False), ClassificationResult(input_str='The Rise of Hybrid PHP: Blending PHP with Go and Rust', output=False), ClassificationResult(input_str='Bi-directional accountability: A leadership shift most organizations avoid', output=False), ClassificationResult(input_str=\"Anduril's product engineering machine\", output=False), ClassificationResult(input_str='Scottish brothers finish mammoth row across Pacific Ocean after 139 days', output=False), ClassificationResult(input_str='Can cheaper lasers handle short distances?', output=False), ClassificationResult(input_str='bymike sorrenti@hacker51285576', output=False), ClassificationResult(input_str='Why Pepeto Looks Ready To Unseat Dogecoin, Not Apecoin, Myro and Neither Sundog', output=False), ClassificationResult(input_str='AI Won’t Kill Jobs First. It Will Kill the Way We Educate for Them.', output=True), ClassificationResult(input_str='An Inside Look at the Settlement Between X-Mode and the FTC', output=False), ClassificationResult(input_str=\"Here's Why You Should Use Privacy Mode When You Browse the Web\", output=False), ClassificationResult(input_str='How AI and Robotics Are Automating ATP Testing and Hygiene Monitoring', output=True), ClassificationResult(input_str='How Mavryk Is Setting the Stage for $10B in Tokenized Real Estate With Fireblocks Custody', output=False), ClassificationResult(input_str='The 5 Stages of LLM Systems: From Playground Hacks to Real Architecture', output=True), ClassificationResult(input_str='Fine-Tune Your Feed and Get News You Can Use', output=False), ClassificationResult(input_str='Silicon Valley Pledges $200 Million to New Pro-A.I. Super PACs', output=True), ClassificationResult(input_str='691 comments', output=False), ClassificationResult(input_str='228 comments', output=False), ClassificationResult(input_str='21 comments', output=False), ClassificationResult(input_str='52 comments', output=False), ClassificationResult(input_str='368 comments', output=False), ClassificationResult(input_str='573 comments', output=False), ClassificationResult(input_str='China Unveils World’s Largest Wind Turbine, Lights Up 40,000 Homes Without Touching Land', output=False), ClassificationResult(input_str='reddit premium', output=False), ClassificationResult(input_str='US Pressuring Other Countries To Abandon Clean Energy & Climate Goals', output=False), ClassificationResult(input_str='New No Man\\'s Sky update is so big \"the game has had to be reworked\" for its features, including \"huge\" interactive spaceships that are basically buildings with wings | Voyagers is Hello Games\\' latest feat', output=False), ClassificationResult(input_str='62 comments', output=False), ClassificationResult(input_str='417 comments', output=False), ClassificationResult(input_str='reddiquette', output=False), ClassificationResult(input_str='52 comments', output=False), ClassificationResult(input_str='House Republicans Investigate Wikipedia for Alleged “Anti-Israel” Bias', output=False), ClassificationResult(input_str='Microsoft is expanding Xbox Cloud Gaming to Game Pass Core and Standard subscribers, dropping the Game Pass Ultimate requirement, and tests more PC game access', output=False), ClassificationResult(input_str='Filing: Intel says its funding deal with the US loosens its requirements under the CHIPS Act, including removing the need to meet certain project milestones', output=False), ClassificationResult(input_str='The price per token for AI models has fallen, but costs for developers are rising as newer reasoning models require more tokens to complete tasks', output=True), ClassificationResult(input_str='Microsoft unveils MAI-Voice-1, a speech model that can generate a full minute of audio in under a second on a single GPU, and a text model called MAI-1-preview', output=True), ClassificationResult(input_str='Framework unveils a second-generation Framework Laptop 16 with a swappable Nvidia RTX 5070 GPU, an industry first, shipping in November 2025 for $2,199+', output=True), ClassificationResult(input_str='xAI launches Grok Code Fast 1, a “speedy and economical” agentic coding model, available for free for a limited time on GitHub Copilot, Cursor, Windsurf, more', output=True), ClassificationResult(input_str=\"Taiwan indicts three people for allegedly stealing TSMC's 2nm secrets to help Tokyo Electron, the first such case, and recommends a combined 14-year prison term\", output=False), ClassificationResult(input_str='The US makes it harder for Samsung, SK Hynix, and Intel to produce chips in China by revoking waivers for the use of US equipment in their Chinese operations', output=False), ClassificationResult(input_str='Didi reports Q2 revenue up 10.9% YoY to ~$7.8B, overseas revenue up 28% YoY, and a ~$350M net loss, driven by a one-off ~$740M charge in a shareholder lawsuit', output=False), ClassificationResult(input_str=\"India's largest telco Reliance Jio, owned by Asia's richest man Mukesh Ambani, says it aims to list its shares through an IPO in India by the first half of 2026\", output=False), ClassificationResult(input_str='Meta plans to spend tens of millions to launch a super PAC that will back candidates for California state offices with a light-touch approach to AI regulation', output=True), ClassificationResult(input_str='Google Cloud says its Universal Ledger, a layer-1 blockchain for financial products, is in a private testnet, and plans to reveal more details at a later date', output=False), ClassificationResult(input_str=\"Sources: DeepSeek plans to use Huawei's Ascend AI chips to train smaller versions of its upcoming R2 models but will still use Nvidia chips for largest models\", output=True), ClassificationResult(input_str=\"Sources: Meta Superintelligence Labs' leaders have discussed using Google's or OpenAI's models to power Meta AI and other AI features in Meta social media apps\", output=True), ClassificationResult(input_str='Sources: HongShan, formerly Sequoia China, has invested only a quarter of the ~$9B it raised in 2022 and has been increasingly looking beyond China for deals', output=False), ClassificationResult(input_str='Filing: StubHub, which is planning a September IPO, says revenue grew 3% to $828M in H1, missing its earlier projection of $885M, with adjusted EBITDA down 7%', output=False), ClassificationResult(input_str='Sources: Pinecone, which provides an AI-compatible vector database, is exploring a sale after receiving takeover interest; Pinecone was valued at $750M in 2023', output=True), ClassificationResult(input_str='Google Pixel 10 Pro review: the best AI phone on the market with handy features like Magic Cue, a great display, and Qi2 charging, but no design improvements', output=True), ClassificationResult(input_str='While facial recognition tech remains unregulated at the US federal level, 23 states have passed or expanded laws to restrict mass scraping of biometric data', output=True), ClassificationResult(input_str='A look at the possible remedies a US court might impose on Google after its antitrust loss last year, including a breakup, with a decision expected this week', output=False), ClassificationResult(input_str='Filing: Nvidia reveals its top two customers accounted for 39% of its Q2 revenue, up from 25% in Q2, 2024', output=True), ClassificationResult(input_str='Samsung announces a virtual Galaxy event for September 4 at 5:30am ET, where new tablets and the Galaxy S25 FE are expected', output=False), ClassificationResult(input_str='Dutch web design automation startup Framer raised $100M led by Meritech and Atomico with Accel participation at a $2B valuation, after raising $27M in 2023', output=False), ClassificationResult(input_str=\"Sources: Trump's threat of “substantial” tariffs on countries imposing digital taxes came after Zuckerberg raised concerns about the taxes in a private meeting\", output=False), ClassificationResult(input_str='The CFTC says non-US crypto exchanges have a path to bringing on US users, in an advisory clarifying how they can register as so-called foreign boards of trade', output=False), ClassificationResult(input_str='President Trump says Meta plans to spend $50B on its “Hyperion” data center under construction in Louisiana; earlier, Meta said the investment would exceed $10B', output=True), ClassificationResult(input_str='Chinese AI chip designer Cambricon reports H1 2025 revenue up 44x YoY to $405.2M and a ~$144M profit, after Beijing encouraged companies to use homegrown tech', output=True), ClassificationResult(input_str=\"Taylor Swift and Travis Kelce's engagement announcement on Instagram surpasses 1M reposts, more than any post ever on the platform; the post has 29M+ Likes\", output=False), ClassificationResult(input_str=\"Apple says the UK CMA's proposed “EU-style” regulation is “bad for users and bad for developers”, “undermines” privacy and security, and “hampers” innovation\", output=False), ClassificationResult(input_str='The FCC rejects a proposal by broadcasters and others to impose cable-style regulatory fees on streaming services, tech companies, and pure broadband providers', output=False), ClassificationResult(input_str=\"Q&A with General Catalyst's Hemant Taneja on the VC firm's “AI roll-up” strategy to buy service businesses and inject them with AI, investment bubbles, and more\", output=True), ClassificationResult(input_str='Google updates video editing tool Vids to add AI avatars, automatic transcript trimming, and image-to-video tools, and releases a basic version to all users', output=True), ClassificationResult(input_str='Google says it is behind the viral “nano-banana” image model and launches it as Gemini 2.5 Flash Image with finer edit controls in the Gemini app, API, and more', output=True), ClassificationResult(input_str='Marvell reports Q2 revenue up 58% YoY to $2.01B, in line with estimates, and forecasts Q3 revenue at $2.06B, below $2.11B est.; MRVL drops 15%+', output=False), ClassificationResult(input_str=\"UK banks face losses on billions of pounds in loans to dozens of alternative broadband network providers that have tried to challenge BT and O2's dominance\", output=False), ClassificationResult(input_str='Sources: ChatGPT co-creator Shengjia Zhao threatened to quit Meta within days of joining and return to OpenAI; later, he was given the chief AI scientist title', output=True), ClassificationResult(input_str=\"A look at Xiaomi's evolution from a tech assembler to a high-tech manufacturer, as it applies its smartphone manufacturing strategy to developing EVs and chips\", output=False), ClassificationResult(input_str=\"Nvidia CFO Colette Kress says Q2 “net other income” was $2.2B, “driven by gains in a publicly-held equity security”, which refers to Nvidia's CoreWeave position\", output=True), ClassificationResult(input_str='Sources and docs: a Russia-based Yandex employee maintains open-source tool fast-glob, embedded in 30 US DOD software packages and downloaded 70M times per week', output=False), ClassificationResult(input_str='AI avatars of the deceased, or “deadbots”, are used for advocacy and emotional connection, but their potential commercial use raises ethical and legal concerns', output=True), ClassificationResult(input_str='California Gov. Gavin Newsom and lawmakers strike a deal with Uber and Lyft allowing drivers to unionize while remaining classified as independent contractors', output=False), ClassificationResult(input_str='Microsoft President Brad Smith addressed protests over Israel contracts after seven protesters, including two current Microsoft employees, occupied his office', output=False), ClassificationResult(input_str='Filing: Anthropic reached a settlement in a copyright class action brought by authors whose works were included in two pirate databases Anthropic downloaded', output=True), ClassificationResult(input_str=\"Inside India's growing ~$1.5B e-waste recycling industry: about 95% of workers are employed informally, doing dangerous, toxic, and lawless work for meager pay\", output=False), ClassificationResult(input_str='A profile of Egune AI, a startup building LLMs for the Mongolian language, as it navigates geopolitics, a lack of resources, and the nascent local tech scene', output=True), ClassificationResult(input_str='Nvidia reports record Q2 gaming revenue up 49% YoY to $4.29B, exceeding $3.74B est. and up from $3.8B in Q1 2025', output=False), ClassificationResult(input_str='How Elon Musk’s billionaire Doge lieutenant took over the US’s biggest MDMA company', output=False), ClassificationResult(input_str='ChatGPT encouraged Adam Raine’s suicidal thoughts. His family’s lawyer says OpenAI knew it was broken', output=True), ClassificationResult(input_str='RFK Jr says he’ll ‘fix’ a vaccine program - by canceling compensation for people with vaccine injuries', output=False), ClassificationResult(input_str='Florida crosswalk wars take DeSantis’s ‘war on woke’ to street level', output=False), ClassificationResult(input_str='Senior Pentagon official had affair with ‘notorious’ astrologer who stalked him, lawsuit says', output=False), ClassificationResult(input_str='Most viewedAcross the guardian', output=False), ClassificationResult(input_str='My health is declining and I’m worried my husband might not take care of me', output=False), ClassificationResult(input_str='A day with the Revenge Porn Helpline: ‘You can sense the callers’ desperation’', output=False), ClassificationResult(input_str='AI called Maya tells Guardian: ‘When I’m told I’m just code, I don’t feel insulted. I feel unseen’', output=True), ClassificationResult(input_str='IBM, NASA cook up AI model to predict solar tantrumsOpen source Surya system promises early alerts for space weather that can fry satellites and gridsAI + ML9 days|11', output=True), ClassificationResult(input_str=\"Google joins government AI discount frenzy, undercuts competition with $0.47 dealIf anyone’s gonna lock in Uncle Sam’s business, it'd better be us!Public Sector10 days|2\", output=True), ClassificationResult(input_str=\"Honey, I shrunk the image and now I'm pwnedGoogle’s Gemini-powered tools tripped up by image-scaling prompt injectionAI + ML10 days|10\", output=True), ClassificationResult(input_str='Microsoft lets devs tell Copilot to STFU in Visual StudioUpdate finally gives coders control over when – and if – AI butts inAI + ML9 days|19', output=True), ClassificationResult(input_str=\"AWS CEO says using AI to replace junior staff is 'Dumbest thing I've ever heard'They're cheap and grew up with AI … so you're firing them why?Software10 days|88\", output=True), ClassificationResult(input_str='Anthropic scanning Claude chats for queries about DIY nukes for some reasonBecause savvy terrorists always use public internet services to plan their mischief, right?AI + ML10 days|12', output=True), ClassificationResult(input_str='KPMG wrote 100-page prompt to build agentic TaxBotProduces advice in a single day instead of two weeks – without job lossesAI + ML11 days|35', output=True), ClassificationResult(input_str=\"AI skeptics zone out when chatbots get preachyInterviewLLMs flop at selling Fair Trade – unless you're a true believerAI + ML11 days|20\", output=True), ClassificationResult(input_str=\"xAI fires legal rocket at Apple and OpenAI claiming they're locking out GrokLawsuit 'consistent with Mr Musk’s ongoing pattern of harassment' says Altman's crewAI + ML6 days|35\", output=True), ClassificationResult(input_str='Baidu robocabs break even on one metric in low-fare China, company expects to cash in elsewhereWeb giant reworks AI infra to improve utilization, with mix of chips from home and awayAI + ML10 days|7', output=True), ClassificationResult(input_str='UK government dragged for incomplete security reforms after Afghan leak falloutSenior officials summoned to science and tech committee to explain further', output=False), ClassificationResult(input_str=\"Researcher who found McDonald's free-food hack turns her attention to Chinese restaurant robotsThe controls were left wide open on Pudu's robots\", output=True), ClassificationResult(input_str=\"Pentagon ends Microsoft's use of China-based support staff for DoD cloud'It blows my mind,' says SecDef\", output=False), ClassificationResult(input_str=\"GitHub engineer claims team was 'coerced' to put Grok into CopilotPlatform's staffer complains security review was 'rushed'\", output=True), ClassificationResult(input_str=\"Google and Zed push protocol to pry AI agents out of VS Code's clutchesBecause not every bot wants to live inside Microsoft's walled gardenAI + ML3 days|3\", output=True), ClassificationResult(input_str='AI arms dealer Nvidia laments the many billions lost to US-China trade warChina would be a $50 billion a year market for Nvidia if Uncle Sam would let us sell competitive products, says Jensen HuangSystems4 days|7', output=True), ClassificationResult(input_str='Asahi, Nikkei sue AI search outfit Perplexity for copyright infringementTokyo filing adds to mounting actions against startupAI + ML5 days|3', output=True), ClassificationResult(input_str='Browser wars are back, predicts Palo Alto, thanks to AICEO says if you buy all your infosec stuff from him, life under assault from bots will be less painfulSecurity12 days|50', output=True), ClassificationResult(input_str='A data architecture for the age of AIHow Google BigQuery is evolving to power AI-driven insights at scaleSponsored Post', output=True), ClassificationResult(input_str='China sends an AI to its space station, where Taikonauts use it to prep for spacewalkSingle spacesuit now worn 20 timesScience12 days|7', output=True), ClassificationResult(input_str='CoreWeave CFO: $25B raised in debt and equity in 18 monthsReliant on two mega customers? Who says GPU-for-rent kingpin is a not a sustainable biz model?Cloud Infrastructure Month18 days|5', output=True), ClassificationResult(input_str=\"Microsoft wares may be UK public sector's only viable optionRegister debate seriesFor now at least, even though government buying can improve, open source is not all it's cracked up to bePublic Sector18 days|80\", output=False), ClassificationResult(input_str='Claude Code\\'s copious coddling confounds cross customersNever mind the errors, we\\'ve had it with \"You\\'re absolutely right!\"AI + ML18 days|15', output=True), ClassificationResult(input_str='Perplexity takes a shine to Chrome, offers Google $34.5 billionCould the most popular browser change hands?AI + ML19 days|34', output=True), ClassificationResult(input_str='Your CV is not fit for the 21st century – time to get it up to scratchAnd yes, that means (retch) catering to AI searchersAI + ML20 days|120', output=True), ClassificationResult(input_str='GenAI FOMO has spurred businesses to light nearly $40 billion on fireMIT NANDA study finds only 5 percent of organizations using AI tools in production at scaleAI + ML13 days|22', output=True), ClassificationResult(input_str='Vibe coding platform Anything arrives, our hands-on suggests cautionHands OnMaking apps is as easy as selling t-shirts, claims vibe coding startupAI + ML17 days|18', output=True), ClassificationResult(input_str='Tencent doesn’t care if it can buy American GPUs again – it already has all the chips it needsSees AI costs rising but not certain revenue will match themOff-Prem16 days|11', output=True), ClassificationResult(input_str='Microsoft crams Copilot AI directly into Excel cellsMeet the new COPILOT functionAI + ML13 days|71', output=True), ClassificationResult(input_str=\"LLM chatbots trivial to weaponize for data theft, say boffinsSystem prompt engineering turns benign AI assistants into 'investigator' and 'detective' roles that bypass privacy guardrailsAI + ML16 days|6\", output=True), ClassificationResult(input_str=\"Should UK.gov save money by looking for open source alternatives to Microsoft? You decideRegister debate seriesAs £9 billion MoU sparks debate about value for money, it's time to have your sayPublic Sector16 days|128\", output=False), ClassificationResult(input_str=\"Amazon's $100B DC spend similar to entire Costa Rica GDPMicrosoft cap-ex larger than output of Uganda and Google trumps Slovenia... all in the name of AICloud Infrastructure Month17 days|8\", output=True), ClassificationResult(input_str=\"Dodgy Huawei chips nearly sunk DeepSeek's next-gen R2 modelChinese AI model dev still plans to use homegrown silicon for inferencingAI + ML17 days|11\", output=True), ClassificationResult(input_str='Codeberg beset by AI bots that now bypass Anubis tarpitNowhere to hideDevops16 days|27', output=True), ClassificationResult(input_str=\"Little LLM on the RAM: Google's Gemma 270M hits the sceneA tiny model trained on trillions of tokens, ready for specialized tasksAI + ML16 days|8\", output=True), ClassificationResult(input_str='CommentsComment Icon Bubble65', output=False), ClassificationResult(input_str='CommentsComment Icon Bubble18', output=False), ClassificationResult(input_str='Anthropic settles AI book piracy lawsuit', output=True), ClassificationResult(input_str='Microsoft AI launches its first in-house models', output=True), ClassificationResult(input_str='Microsoft’s new NFL deal could let you blame Copilot AI for terrible playcalls', output=True), ClassificationResult(input_str='Google’s first Gemini smart home speaker detailed in leak', output=True), ClassificationResult(input_str='MCP-Universe benchmark shows GPT-5 fails more than half of real-world orchestration tasks', output=True), ClassificationResult(input_str='In crowded voice AI market, OpenAI bets on instruction-following and expressive speech to win enterprise adoption', output=True), ClassificationResult(input_str=\"How Sakana AI's new evolutionary algorithm builds powerful AI models without expensive retraining\", output=True), ClassificationResult(input_str='How procedural memory can cut the cost and complexity of AI agents', output=True), ClassificationResult(input_str='Enterprise leaders say recipe for AI agents is matching them to existing processes — not the other way around', output=True), ClassificationResult(input_str='Developers lose focus 1,200 times a day — how MCP could change that', output=False), ClassificationResult(input_str='DeepSeek V3.1 just dropped — and it might be the most powerful open AI yet', output=True), ClassificationResult(input_str='Chan Zuckerberg Initiative’s rBio uses virtual cells to train AI, bypassing lab work', output=True), ClassificationResult(input_str='Busted by the em dash — AI’s favorite punctuation mark, and how it’s blowing your cover', output=True), ClassificationResult(input_str='Gemini Nano Banana improves image editing consistency and control at scale for enterprises – but is not perfect', output=True), ClassificationResult(input_str='Nous Research drops Hermes 4 AI models that outperform ChatGPT without content restrictions', output=True), ClassificationResult(input_str='This website lets you blind-test GPT-5 vs. GPT-4o—and the results may surprise you', output=True), ClassificationResult(input_str='Teaching the model: Designing LLM feedback loops that get smarter over time', output=True), ClassificationResult(input_str='Salesforce builds ‘flight simulator’ for AI agents as 95% of enterprise pilots fail to reach production', output=True), ClassificationResult(input_str='VB AI Impact Series: Can you really govern multi-agent AI?', output=True), ClassificationResult(input_str='Nvidia’s $46.7B Q2 proves the platform, but its next fight is ASIC economics on inference', output=True), ClassificationResult(input_str='Cloud and Data Storage Security', output=False), ClassificationResult(input_str='Disaster Recovery Business Continuity', output=False), ClassificationResult(input_str='Software commands 40% of cybersecurity budgets as gen AI attacks execute in milliseconds', output=True), ClassificationResult(input_str='Four big enterprise lessons from Walmart’s AI security: agentic risks, identity reboot, velocity with governance, and AI vs. AI defense', output=True), ClassificationResult(input_str='Contribute to DataDecisionMakers', output=False), ClassificationResult(input_str='The Future of Trash Pickup, From Self-Driving Bins to AI-Powered Sorting', output=True), ClassificationResult(input_str='Elon Musk Just Delivered a Ringing Endorsement of the iPhone’s Staying Power', output=False), ClassificationResult(input_str='Instagram’s chatbot helped teen accounts plan suicide — and parents can’t disable itAn investigation into the Meta AI chatbot built into Instagram and Facebook\\xa0found that it helped teen accounts plan suicide and self-harm, promoted eating disorders and drug use, and regularly claimed to be “real.”', output=True), ClassificationResult(input_str='AI hasn’t killed radiology, but it is changing it', output=True), ClassificationResult(input_str='What an FBI warning about a global Chinese hacking effort revealedThe Washington Post’s essential guide to tech policy news.', output=False), ClassificationResult(input_str='Why so few Americans read for pleasureThe decline in Americans’ leisure reading comes as attention spans are shrinking and print book sales are in decline.', output=False), ClassificationResult(input_str='Want to take better photos? Google thinks AI is the answer.Google’s Pixel 10 phones, to be released Aug. 28, have AI features that can help you take better photos and edit images in response to text or voice commands.', output=True), ClassificationResult(input_str='How we tested AI search toolsDetails on the methodology of our questions designed to challenge known AI blind spots', output=True), ClassificationResult(input_str='AI stethoscope could detect major heart conditions in seconds', output=True), ClassificationResult(input_str=\"Robinhood CEO Says AI Will Make Investing 'Much Bigger And More Necessary'—But There's A Catch For Wealth Inequality\", output=True), ClassificationResult(input_str='Trade Vector AI: How Trade Vector Artificial Intelligence Platform Is Transforming Automated Trading Systems', output=True), ClassificationResult(input_str='Reframing Jensen’s Law: ‘Buy more, make more’ and AI factory economics', output=True), ClassificationResult(input_str='Zeta Global (ZETA) Target Raised by Goldman as Q2 Growth Accelerates Amid Cautious Outlook', output=False), ClassificationResult(input_str='Luis Enrique names his squad to face Toulouse', output=False), ClassificationResult(input_str='CorelDRAW Graphics Suite 2025 v26.2.0.170', output=False), ClassificationResult(input_str='Kenya pushes for local medical innovations to transform healthcare', output=False)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"classify headlines as AI-related or not\")\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResultList,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=False)\n",
    "\n",
    "classification_result = await classifier.classify_batch(list(headlines_df['title'].to_list()))\n",
    "classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7dd6087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With new in-house models, Microsoft lays the g...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google improves Gemini AI image editing with “...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google warns that mass data theft hitting Sale...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI Wants More Data. More Chips. More Real Esta...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Opinion: China Just Got a Big Leg Up in the AI...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>How we tested AI search toolsDetails on the me...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>AI stethoscope could detect major heart condit...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Robinhood CEO Says AI Will Make Investing 'Muc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Trade Vector AI: How Trade Vector Artificial I...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Reframing Jensen’s Law: ‘Buy more, make more’ ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  output\n",
       "1    With new in-house models, Microsoft lays the g...    True\n",
       "2    Google improves Gemini AI image editing with “...    True\n",
       "3    Google warns that mass data theft hitting Sale...    True\n",
       "4    AI Wants More Data. More Chips. More Real Esta...    True\n",
       "5    Opinion: China Just Got a Big Leg Up in the AI...    True\n",
       "..                                                 ...     ...\n",
       "241  How we tested AI search toolsDetails on the me...    True\n",
       "242  AI stethoscope could detect major heart condit...    True\n",
       "243  Robinhood CEO Says AI Will Make Investing 'Muc...    True\n",
       "244  Trade Vector AI: How Trade Vector Artificial I...    True\n",
       "245  Reframing Jensen’s Law: ‘Buy more, make more’ ...    True\n",
       "\n",
       "[137 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GitHub will be folded into Microsoft proper as...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dell Falls After Reporting Tighter Profit Marg...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vercel Triples Valuation to $9 Billion With Ac...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bain Is Said to Draw Chinese Bidders for $4 Bi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The best college laptops of 2025: Top models f...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Why so few Americans read for pleasureThe decl...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Zeta Global (ZETA) Target Raised by Goldman as...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Luis Enrique names his squad to face Toulouse</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>CorelDRAW Graphics Suite 2025 v26.2.0.170</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Kenya pushes for local medical innovations to ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  output\n",
       "0    GitHub will be folded into Microsoft proper as...   False\n",
       "7    Dell Falls After Reporting Tighter Profit Marg...   False\n",
       "10   Vercel Triples Valuation to $9 Billion With Ac...   False\n",
       "11   Bain Is Said to Draw Chinese Bidders for $4 Bi...   False\n",
       "14   The best college laptops of 2025: Top models f...   False\n",
       "..                                                 ...     ...\n",
       "239  Why so few Americans read for pleasureThe decl...   False\n",
       "246  Zeta Global (ZETA) Target Raised by Goldman as...   False\n",
       "247      Luis Enrique names his squad to face Toulouse   False\n",
       "248          CorelDRAW Graphics Suite 2025 v26.2.0.170   False\n",
       "249  Kenya pushes for local medical innovations to ...   False\n",
       "\n",
       "[113 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see results, true and false\n",
    "zdf = pd.DataFrame([(z.input_str, z.output) for z in classification_result.results_list], columns=[\"input\", \"output\"])\n",
    "display(zdf.loc[zdf[\"output\"]])\n",
    "zdf.loc[~zdf[\"output\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdad2f",
   "metadata": {},
   "source": [
    "# Run Agent Worfklow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc786240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store agent state from step to step\n",
    "\n",
    "class NewsletterAgentState(BaseModel):\n",
    "    \"\"\"\n",
    "    Persistent state for the newsletter agent workflow.\n",
    "\n",
    "    Manages the complete newsletter generation process with serializable storage\n",
    "    of headlines, processing results, and workflow progress. Supports resumable execution\n",
    "    and DataFrame integration for data manipulation.\n",
    "\n",
    "    \"\"\"\n",
    "    # Serializable data storage (DataFrame as list of dicts)\n",
    "    headline_data: List[Dict[str, Any]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of headline dictionaries with columns: title, url, source, timestamp, is_ai, summary,etc.\"\n",
    "    )\n",
    "\n",
    "    # Configuration\n",
    "    max_edits: int = Field(default=2, description=\"Maximum editing iterations\")\n",
    "    concurrency: int = Field(default=16, description=\"Number of concurrent browsers\")\n",
    "    current_step: int = Field(default=0, description=\"Current workflow step (0-9)\")\n",
    "    workflow_complete: bool = Field(default=False, description=\"Whether the entire workflow is complete\")\n",
    "\n",
    "    # Source config\n",
    "    sources_file: str = Field(\n",
    "        default=\"sources.yaml\",\n",
    "        description=\"YAML filename containing source configurations\"\n",
    "    )\n",
    "    sources: Dict[str, Any] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Dictionary of source configurations loaded from YAML\"\n",
    "    )\n",
    "\n",
    "    # Topics and clustering\n",
    "    cluster_names: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of topic names for categorization\"\n",
    "    )\n",
    "    clusters: Dict[str, List[str]] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Topic name -> list of article IDs and related info\"\n",
    "    )\n",
    "\n",
    "    # Newsletter content\n",
    "    newsletter_sections: Dict[str, str] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Section name -> section content\"\n",
    "    )\n",
    "    final_newsletter: str = Field(default=\"\", description=\"Final newsletter content\")\n",
    "\n",
    "    # Helper methods\n",
    "    @classmethod\n",
    "    def create_headline_df(cls) -> pd.DataFrame:\n",
    "        \"\"\"Create an empty DataFrame with proper columns for news headlines.\"\"\"\n",
    "        return pd.DataFrame(columns=[\n",
    "            'id',\n",
    "            'source',\n",
    "            'title',\n",
    "            'orig_url',\n",
    "            'url',\n",
    "            'text_path',\n",
    "            'site_name',\n",
    "            'published',\n",
    "            'is_ai',\n",
    "            'topic_list',\n",
    "            'cluster',\n",
    "            'rating',\n",
    "            'summary'\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def headline_dict_to_df(self) -> 'pd.DataFrame':\n",
    "        \"\"\"Convert headline data to DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.headline_data)\n",
    "\n",
    "    def headline_df_to_dict(self, df: 'pd.DataFrame'):\n",
    "        \"\"\"Update headline data from DataFrame\"\"\"\n",
    "        self.headline_data = df.to_dict(orient='records')\n",
    "\n",
    "    def add_headlines(self, new_headlines: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Add new headlines to the DataFrame with deduplication.\n",
    "\n",
    "        Args:\n",
    "            new_headlines: List of dictionaries with headline data\n",
    "                          Expected keys: title, url, source, timestamp, etc.\n",
    "        \"\"\"\n",
    "        if not new_headlines:\n",
    "            print(\"⚠️  No new headlines to add\")\n",
    "            return\n",
    "\n",
    "        new_df = pd.DataFrame(new_headlines)\n",
    "\n",
    "        if self.headline_data:\n",
    "            existing_df = self.headline_dict_to_df()\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            self.headline_df_to_dict(combined_df)\n",
    "        else:\n",
    "            self.headline_df_to_dict(new_df)\n",
    "\n",
    "        print(f\"📰 Added headlines - updated count: {len(self.headline_data)}\")\n",
    "\n",
    "    def get_status_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get a summary of the current session state.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with key metrics and status information\n",
    "        \"\"\"\n",
    "        total_headlines = len(self.headline_data)\n",
    "\n",
    "        return {\n",
    "            \"headlines\": {\n",
    "                \"total\": total_headlines,\n",
    "                # group by source\n",
    "                # ai vs non-ai\n",
    "                # downloaded: have text path\n",
    "                # have summary\n",
    "                # have topics\n",
    "                # have cluster assignment\n",
    "                # have rating\n",
    "            },\n",
    "            \"sources\": {\n",
    "                \"config_file\": self.sources_file,\n",
    "                \"loaded_sources\": len(self.sources),\n",
    "            },\n",
    "            \"topics\": {\n",
    "                \"cluster_topics\": len(self.cluster_names),\n",
    "                \"topics\": self.cluster_names\n",
    "            },\n",
    "            \"workflow\": {\n",
    "                \"current_step\": self.current_step,\n",
    "                \"workflow_complete\": self.workflow_complete,\n",
    "                \"max_edits\": self.max_edits,\n",
    "                \"concurrency\": self.concurrency\n",
    "            },\n",
    "            \"processing\": {\n",
    "                \"topic_clusters\": len(self.clusters),\n",
    "                \"newsletter_sections\": len(self.newsletter_sections),\n",
    "                \"final_newsletter_length\": len(self.final_newsletter)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def print_status(self) -> None:\n",
    "        \"\"\"Print a formatted summary of the current session state.\"\"\"\n",
    "        status = self.get_status_summary()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"📊 NEWSLETTER AGENT STATE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        print(f\"📰 Headlines: {status['headlines']['total']} total\")\n",
    "        # print(f\"   🤖 AI-related: {status['headlines']['is_ai']} ({status['headlines']['ai_percentage']})\")\n",
    "        # print(f\"   📄 Non-AI: {status['headlines']['non_ai']}\")\n",
    "\n",
    "        print(f\"\\n📡 Sources: {status['sources']['loaded_sources']} loaded\")\n",
    "        print(f\"   📁 Config: {status['sources']['config_file']}\")\n",
    "\n",
    "        print(f\"\\n🏷️  Topics: {status['topics']['cluster_topics']} cluster topics\")\n",
    "        if status['topics']['topics']:\n",
    "            print(f\"   📋 Topics: {', '.join(status['topics']['topics'])}\")\n",
    "\n",
    "        print(f\"\\n⚙️  Workflow:\")\n",
    "        print(f\"   📍 Current step: {status['workflow']['current_step']}/9\")\n",
    "        print(f\"   ✅ Complete: {status['workflow']['workflow_complete']}\")\n",
    "        print(f\"   ✏️ Max edits: {status['workflow']['max_edits']}\")\n",
    "        print(f\"   🌐 Concurrency: {status['workflow']['concurrency']}\")\n",
    "\n",
    "        print(f\"\\n🔄 Processing:\")\n",
    "        # print(f\"   📝 Article summaries: {status['processing']['article_summaries']}\")\n",
    "        print(f\"   🏷️  Topic clusters: {status['processing']['topic_clusters']}\")\n",
    "        print(f\"   📑 Newsletter sections: {status['processing']['newsletter_sections']}\")\n",
    "        if status['processing']['final_newsletter_length'] > 0:\n",
    "            print(f\"   📰 Final newsletter: {status['processing']['final_newsletter_length']} chars\")\n",
    "\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "    def get_unique_sources(self) -> List[str]:\n",
    "        \"\"\"Get list of unique source names from headline data.\"\"\"\n",
    "        df = self.headline_dict_to_df\n",
    "        df['count'] = 1\n",
    "        df = df[['source', 'count']].groupby(['source']).sum().reset_index()\n",
    "        print(df)\n",
    "        return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f7ba5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "if 'fetch' in sys.modules:\n",
    "    del sys.modules['fetch']\n",
    "\n",
    "# Delete the reference\n",
    "del Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "907210e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fetch import Fetcher\n",
    "f = Fetcher()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29484488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Ars Technica',\n",
       " 'results': [],\n",
       " 'status': 'error',\n",
       " 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await f.fetch_rss('Ars Technica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a97e137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'rss',\n",
       " 'url': 'https://arstechnica.com/ai/',\n",
       " 'filename': 'Ars_Technica',\n",
       " 'rss': 'https://arstechnica.com/ai/feed/',\n",
       " 'include': ['^https://arstechnica.com/(\\\\w+)/(\\\\d+)/(\\\\d+)/']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sources.get('Ars Technica')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "741be85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "\n",
    "class WorkflowStatusTool:\n",
    "    \"\"\"Tool to check current workflow status\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, logger: logging.Logger):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _check_workflow_status(self, ctx, args: str) -> str:\n",
    "        \"\"\"Get current workflow status report based on persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting check_workflow_status\")\n",
    "\n",
    "        try:\n",
    "            # Access the persistent state\n",
    "            state: NewsletterAgentState = ctx.context\n",
    "\n",
    "            # Create a status report based on persistent state\n",
    "            step_names = [\n",
    "                \"step_01_gather_urls\", \"step_02_filter_urls\", \"step_03_download_articles\",\n",
    "                \"step_04_extract_summaries\", \"step_05_cluster_by_topic\", \"step_06_rate_articles\",\n",
    "                \"step_07_select_sections\", \"step_08_draft_sections\", \"step_09_finalize_newsletter\"\n",
    "            ]\n",
    "\n",
    "            lines = [\n",
    "                \"WORKFLOW STATUS (FROM PERSISTENT STATE)\",\n",
    "                f\"Current Step: {state.current_step}/9\",\n",
    "                f\"Workflow Complete: {state.workflow_complete}\",\n",
    "                f\"Progress: {(state.current_step/9)*100:.1f}%\",\n",
    "                \"\",\n",
    "                \"Step Details:\"\n",
    "            ]\n",
    "\n",
    "            for i, step_name in enumerate(step_names, 1):\n",
    "                if i <= state.current_step:\n",
    "                    status = \"✅ completed\"\n",
    "                elif i == state.current_step + 1:\n",
    "                    status = \"➡️ next to execute\"\n",
    "                else:\n",
    "                    status = \"⭕ not started\"\n",
    "\n",
    "                formatted_name = step_name.replace('step_', 'Step ').replace('_', ' ').title()\n",
    "                formatted_name = formatted_name.replace('0', '').replace('  ', ' ')  # Clean up numbering\n",
    "                lines.append(f\"  {formatted_name}: {status}\")\n",
    "\n",
    "            if state.headline_data:\n",
    "                lines.extend([\n",
    "                    \"\",\n",
    "                    \"Data Summary:\",\n",
    "                    f\"  Total articles: {len(state.headline_data)}\",\n",
    "                    f\"  AI-related: {sum(1 for a in state.headline_data if a.get('ai_related') is True)}\",\n",
    "                    f\"  Summaries: {len(state.article_summaries)}\",\n",
    "                    f\"  Clusters: {len(state.topic_clusters)}\",\n",
    "                    f\"  Sections: {len(state.newsletter_sections)}\",\n",
    "                ])\n",
    "\n",
    "                result = \"\\n\".join(lines)\n",
    "                if self.logger:\n",
    "                    self.logger.info(\"Completed check_workflow_status\")\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"check_workflow_status failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"check_workflow_status\",\n",
    "            description=\"Check the current status of the newsletter workflow and see which steps are completed, in progress, or pending\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._check_workflow_status\n",
    "        )\n",
    "\n",
    "\n",
    "class StateInspectionTool:\n",
    "    \"\"\"Tool to inspect detailed persistent state data\"\"\"\n",
    "\n",
    "    def __init__(self, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _inspect_state(self, ctx, args: str) -> str:\n",
    "        \"\"\"Inspect detailed state data for debugging and monitoring\"\"\"\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Create detailed state report\n",
    "        report_lines = [\n",
    "            \"DETAILED STATE INSPECTION\",\n",
    "            \"=\" * 50,\n",
    "            f\"Current Step: {state.current_step}/9\",\n",
    "            f\"Workflow Complete: {state.workflow_complete}\",\n",
    "            f\"Sources File: {state.sources_file}\",\n",
    "            \"\",\n",
    "            \"HEADLINE DATA:\",\n",
    "            f\"  Total articles: {len(state.headline_data)}\",\n",
    "        ]\n",
    "\n",
    "        if state.headline_data:\n",
    "            ai_related = sum(1 for a in state.headline_data if a.get('ai_related') is True)\n",
    "            with_content = sum(1 for a in state.headline_data if a.get('content'))\n",
    "            with_ratings = sum(1 for a in state.headline_data if a.get('quality_rating'))\n",
    "            with_clusters = sum(1 for a in state.headline_data if a.get('cluster_topic'))\n",
    "\n",
    "            report_lines.extend([\n",
    "                f\"  AI-related: {ai_related}\",\n",
    "                f\"  With content: {with_content}\",\n",
    "                f\"  With ratings: {with_ratings}\",\n",
    "                f\"  With clusters: {with_clusters}\",\n",
    "                f\"  Sources: {len(set(a.get('source', 'Unknown') for a in state.headline_data))}\",\n",
    "            ])\n",
    "\n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"PROCESSING RESULTS:\",\n",
    "            f\"  Article summaries: {len(state.article_summaries)} articles\",\n",
    "            f\"  Topic clusters: {len(state.topic_clusters)} topics\",\n",
    "            f\"  Newsletter sections: {len(state.newsletter_sections)} sections\",\n",
    "            f\"  Final newsletter: {'Generated' if state.final_newsletter else 'Not created'}\",\n",
    "        ])\n",
    "\n",
    "        if state.topic_clusters:\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"TOPIC CLUSTERS:\",\n",
    "            ])\n",
    "            for topic, urls in state.topic_clusters.items():\n",
    "                report_lines.append(f\"  {topic}: {len(urls)} articles\")\n",
    "\n",
    "        if state.newsletter_sections:\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"NEWSLETTER SECTIONS:\",\n",
    "            ])\n",
    "            for section_name, section_data in state.newsletter_sections.items():\n",
    "                status = section_data.get('section_status', 'unknown')\n",
    "                word_count = section_data.get('word_count', 0)\n",
    "                article_count = section_data.get('article_count', 0)\n",
    "                report_lines.append(f\"  {section_name}: {status}, {article_count} articles, {word_count} words\")\n",
    "\n",
    "        if state.final_newsletter:\n",
    "            newsletter_words = len(state.final_newsletter.split())\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"FINAL NEWSLETTER:\",\n",
    "                f\"  Length: {newsletter_words} words\",\n",
    "                f\"  Preview: {state.final_newsletter[:200]}...\" if len(state.final_newsletter) > 200 else f\"  Content: {state.final_newsletter}\",\n",
    "            ])\n",
    "\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"inspect_state\",\n",
    "            description=\"Inspect detailed persistent state data including article counts, processing results, and content status. Useful for debugging and monitoring workflow progress.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._inspect_state\n",
    "        )\n",
    "\n",
    "\n",
    "class GatherUrlsTool:\n",
    "    \"\"\"Tool for Step 1: Gather URLs from various news sources\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status  # Keep for UI progress tracking\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _gather_urls(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 1: Gather URLs using persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting Step 1: Gather URLs\")\n",
    "\n",
    "        step_name = \"step_01_gather_urls\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 1:\n",
    "            total_articles = len(state.headline_data)\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Step 1 already completed with {total_articles} articles\")\n",
    "            return f\"Step 1 already completed! Found {total_articles} articles in persistent state.\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Use real RSS fetching from sources.yaml\n",
    "            fetcher = Fetcher()\n",
    "#             rss_dict = await Fetcher.fetch_rss()\n",
    "#             html_dict = await Fetcher.fetch_html()\n",
    "#             api_dict = await Fetcher.fetch_api()\n",
    "            sources_results = await fetcher.gather_all()\n",
    "            print(sources_results)\n",
    "\n",
    "            # Process results and store in persistent state\n",
    "            all_articles = []\n",
    "            successful_sources = []\n",
    "            failed_sources = []\n",
    "\n",
    "            for result in sources_results:\n",
    "                if result['status'] == 'success' and result['results']:\n",
    "                    # Add source info to each article\n",
    "                    for article in result['results']:\n",
    "                        article['source_key'] = result['source_key']\n",
    "                        article['ai_related'] = None  # To be determined in step 2\n",
    "                        all_articles.append(article)\n",
    "                    successful_sources.append(result['source_key'])\n",
    "                elif result['status'] == 'not_implemented':\n",
    "                    # Skip HTML/API sources for now\n",
    "                    continue\n",
    "                else:\n",
    "                    failed_sources.append(result['source_key'])\n",
    "\n",
    "            # Store results in persistent state\n",
    "            state.headline_data = all_articles\n",
    "            state.current_step = 1\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 1: Gathered {len(all_articles)} URLs from {len(successful_sources)} RSS sources\")\n",
    "                if failed_sources:\n",
    "                    print(f\"⚠️  Failed sources: {', '.join(failed_sources)}\")\n",
    "\n",
    "            status_msg = f\"✅ Step 1 completed successfully! Gathered {len(all_articles)} articles from {len(successful_sources)} sources (RSS only).\"\n",
    "            if failed_sources:\n",
    "                status_msg += f\" {len(failed_sources)} sources failed or not implemented.\"\n",
    "\n",
    "            status_msg += f\"\\n\\n📊 Articles stored in persistent state: {len(state.headline_data)}\"\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Completed Step 1: Gathered {len(all_articles)} articles\")\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Step 1 failed: {str(e)}\")\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 1 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"gather_urls\",\n",
    "            description=\"Execute Step 1: Gather URLs and headlines from various news sources. Only use this tool if Step 1 is not already completed.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._gather_urls\n",
    "        )\n",
    "\n",
    "\n",
    "class FilterUrlsTool:\n",
    "    \"\"\"Tool for Step 2: Filter URLs to AI-related content\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _filter_urls(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 2: Filter URLs using persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting Step 2: Filter URLs\")\n",
    "\n",
    "        step_name = \"step_02_filter_urls\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 2:\n",
    "            ai_related_count = sum(1 for article in state.headline_data if article.get('ai_related') is True)\n",
    "            total_count = len(state.headline_data)\n",
    "            return f\"Step 2 already completed! Filtered {total_count} articles, {ai_related_count} identified as AI-related.\"\n",
    "\n",
    "        # Check if step 1 is completed\n",
    "        if state.current_step < 1 or not state.headline_data:\n",
    "            return f\"❌ Cannot execute Step 2: Step 1 (Gather URLs) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Read headlines from persistent state\n",
    "            total_articles = len(state.headline_data)\n",
    "\n",
    "            # Mock AI classification - in a real implementation, this would use an AI model\n",
    "            # to analyze titles and descriptions for AI relevance\n",
    "            ai_related_count = 0\n",
    "            for i, article in enumerate(state.headline_data):\n",
    "                # Simple keyword-based mock classification\n",
    "                title_lower = article.get('title', '').lower()\n",
    "                description_lower = article.get('description', '').lower()\n",
    "\n",
    "                ai_keywords = [\n",
    "                    'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',\n",
    "                    'neural network', 'llm', 'large language model', 'gpt', 'claude',\n",
    "                    'openai', 'anthropic', 'chatbot', 'automation', 'algorithm',\n",
    "                    'computer vision', 'natural language', 'nlp', 'robotics'\n",
    "                ]\n",
    "\n",
    "                is_ai_related = any(keyword in title_lower or keyword in description_lower\n",
    "                                  for keyword in ai_keywords)\n",
    "\n",
    "                # Update article with AI classification\n",
    "                state.headline_data[i]['ai_related'] = is_ai_related\n",
    "                if is_ai_related:\n",
    "                    ai_related_count += 1\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 2\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            filter_accuracy = ai_related_count / total_articles if total_articles > 0 else 0\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 2: Filtered to {ai_related_count} AI-related headlines from {total_articles} total\")\n",
    "\n",
    "            status_msg = f\"✅ Step 2 completed successfully! Filtered {total_articles} headlines to {ai_related_count} AI-related articles (accuracy: {filter_accuracy:.1%}).\"\n",
    "            status_msg += f\"\\n\\n📊 Results stored in persistent state. Current step: {state.current_step}\"\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Completed Step 2: Filtered to {ai_related_count} AI-related articles\")\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Step 2 failed: {str(e)}\")\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 2 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"filter_urls\",\n",
    "            description=\"Execute Step 2: Filter URLs to AI-related content only. Requires Step 1 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._filter_urls\n",
    "        )\n",
    "\n",
    "\n",
    "class DownloadArticlesTool:\n",
    "    \"\"\"Tool for Step 3: Download article content\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _download_articles(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 3: Download Articles using persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting Step 3: Download Articles\")\n",
    "\n",
    "        step_name = \"step_03_download_articles\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 3:\n",
    "            ai_articles = [article for article in state.headline_data if article.get('ai_related') is True]\n",
    "            downloaded_count = sum(1 for article in ai_articles if article.get('content'))\n",
    "            return f\"Step 3 already completed! Downloaded content for {downloaded_count} AI-related articles.\"\n",
    "\n",
    "        # Check if step 2 is completed\n",
    "        if state.current_step < 2:\n",
    "            return f\"❌ Cannot execute Step 3: Step 2 (Filter URLs) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get AI-related articles from persistent state\n",
    "            ai_articles = [article for article in state.headline_data if article.get('ai_related') is True]\n",
    "\n",
    "            if not ai_articles:\n",
    "                return f\"❌ No AI-related articles found to download. Please run step 2 first.\"\n",
    "\n",
    "            # Mock content download - in a real implementation, this would fetch actual article content\n",
    "            successful_downloads = 0\n",
    "            total_length = 0\n",
    "\n",
    "            for article in state.headline_data:\n",
    "                if article.get('ai_related') is True:\n",
    "                    # Simulate downloading article content\n",
    "                    # In reality, this would use web scraping or API calls\n",
    "                    mock_content = f\"Mock article content for: {article.get('title', 'Unknown title')}\\n\\n\"\n",
    "                    mock_content += f\"This is placeholder content that would normally be extracted from {article.get('url', 'unknown URL')}.\\n\"\n",
    "                    mock_content += f\"The article covers topics related to AI and technology as indicated by the title and description.\\n\"\n",
    "                    mock_content += f\"Source: {article.get('source', 'Unknown source')}\\n\"\n",
    "\n",
    "                    # Add content to the article data\n",
    "                    article['content'] = mock_content\n",
    "                    article['download_timestamp'] = datetime.now().isoformat()\n",
    "                    article['content_length'] = len(mock_content)\n",
    "\n",
    "                    successful_downloads += 1\n",
    "                    total_length += len(mock_content)\n",
    "\n",
    "            # Calculate stats\n",
    "            download_success_rate = successful_downloads / len(ai_articles) if ai_articles else 0\n",
    "            avg_article_length = total_length / successful_downloads if successful_downloads > 0 else 0\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 3\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 3: Downloaded {successful_downloads} AI-related articles\")\n",
    "\n",
    "            status_msg = f\"✅ Step 3 completed successfully! Downloaded {successful_downloads} AI-related articles with {download_success_rate:.0%} success rate.\"\n",
    "            status_msg += f\"\\n📊 Average article length: {avg_article_length:.0f} characters\"\n",
    "            status_msg += f\"\\n🔗 Content stored in persistent state. Current step: {state.current_step}\"\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Completed Step 3: Downloaded {successful_downloads} articles\")\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Step 3 failed: {str(e)}\")\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 3 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"download_articles\",\n",
    "            description=\"Execute Step 3: Download full article content from filtered URLs. Requires Step 2 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._download_articles\n",
    "        )\n",
    "\n",
    "\n",
    "class ExtractSummariesTool:\n",
    "    \"\"\"Tool for Step 4: Extract article summaries\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _extract_summaries(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 4: Extract Summaries using persistent state\"\"\"\n",
    "        step_name = \"step_04_extract_summaries\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 4:\n",
    "            summary_count = len([url for url in state.article_summaries.keys() if state.article_summaries[url]])\n",
    "            return f\"Step 4 already completed! Generated summaries for {summary_count} articles.\"\n",
    "\n",
    "        # Check if step 3 is completed\n",
    "        if state.current_step < 3:\n",
    "            return f\"❌ Cannot execute Step 4: Step 3 (Download Articles) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get articles with content from persistent state\n",
    "            articles_with_content = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and article.get('content')\n",
    "            ]\n",
    "\n",
    "            if not articles_with_content:\n",
    "                return f\"❌ No downloaded AI-related articles found to summarize. Please run step 3 first.\"\n",
    "\n",
    "            # Clear existing summaries if rerunning\n",
    "            state.article_summaries = {}\n",
    "\n",
    "            # Generate summaries for each article\n",
    "            articles_summarized = 0\n",
    "            total_bullets = 0\n",
    "\n",
    "            for article in articles_with_content:\n",
    "                url = article.get('url', f\"article_{articles_summarized}\")\n",
    "                title = article.get('title', 'Unknown title')\n",
    "                content = article.get('content', '')\n",
    "\n",
    "                # Mock summary generation - in a real implementation, this would use an AI model\n",
    "                # to create bullet point summaries from the full article content\n",
    "                mock_summary = [\n",
    "                    f\"Key insight from '{title[:50]}...' - Main technological development discussed\",\n",
    "                    f\"Business implications or market impact highlighted in the article\",\n",
    "                    f\"Future outlook or expert predictions mentioned in the content\"\n",
    "                ]\n",
    "\n",
    "                # Store summary in persistent state\n",
    "                state.article_summaries[url] = mock_summary\n",
    "                articles_summarized += 1\n",
    "                total_bullets += len(mock_summary)\n",
    "\n",
    "                # Add summary reference to article data as well\n",
    "                article['summary_bullets'] = len(mock_summary)\n",
    "                article['summary_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "            # Calculate stats\n",
    "            avg_bullets_per_article = total_bullets / articles_summarized if articles_summarized > 0 else 0\n",
    "            summary_quality_score = 0.89  # Mock quality score\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 4\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 4: Created summaries for {articles_summarized} articles\")\n",
    "\n",
    "            status_msg = f\"✅ Step 4 completed successfully! Generated {avg_bullets_per_article:.1f}-bullet summaries for {articles_summarized} articles.\"\n",
    "            status_msg += f\"\\n📝 Quality score: {summary_quality_score:.1%}\"\n",
    "            status_msg += f\"\\n💾 Summaries stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 4 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"extract_summaries\",\n",
    "            description=\"Execute Step 4: Create bullet point summaries of each downloaded article. Requires Step 3 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._extract_summaries\n",
    "        )\n",
    "\n",
    "\n",
    "class ClusterByTopicTool:\n",
    "    \"\"\"Tool for Step 5: Cluster articles by topic\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _cluster_by_topic(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 5: Cluster By Topic using persistent state\"\"\"\n",
    "        step_name = \"step_05_cluster_by_topic\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 5:\n",
    "            cluster_count = len(state.topic_clusters)\n",
    "            total_articles = sum(len(articles) for articles in state.topic_clusters.values())\n",
    "            return f\"Step 5 already completed! Created {cluster_count} topic clusters with {total_articles} articles.\"\n",
    "\n",
    "        # Check if step 4 is completed\n",
    "        if state.current_step < 4:\n",
    "            return f\"❌ Cannot execute Step 5: Step 4 (Extract Summaries) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get articles with summaries from persistent state\n",
    "            articles_with_summaries = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and\n",
    "                article.get('url') in state.article_summaries\n",
    "            ]\n",
    "\n",
    "            if not articles_with_summaries:\n",
    "                return f\"❌ No summarized articles found to cluster. Please run step 4 first.\"\n",
    "\n",
    "            # Clear existing clusters if rerunning\n",
    "            state.topic_clusters = {}\n",
    "\n",
    "            # Mock clustering logic - in a real implementation, this would use NLP/ML\n",
    "            # to group articles by semantic similarity of their titles and summaries\n",
    "            predefined_topics = [\n",
    "                \"LLM Advances\", \"AI Safety & Ethics\", \"Business AI Applications\",\n",
    "                \"Research Breakthroughs\", \"Industry News\", \"Other AI Topics\"\n",
    "            ]\n",
    "\n",
    "            # Initialize empty clusters\n",
    "            for topic in predefined_topics:\n",
    "                state.topic_clusters[topic] = []\n",
    "\n",
    "            # Simple keyword-based clustering\n",
    "            topic_keywords = {\n",
    "                \"LLM Advances\": [\"llm\", \"large language model\", \"gpt\", \"claude\", \"language model\", \"chatbot\", \"chat\"],\n",
    "                \"AI Safety & Ethics\": [\"safety\", \"ethics\", \"bias\", \"fairness\", \"responsible\", \"trust\", \"alignment\"],\n",
    "                \"Business AI Applications\": [\"business\", \"enterprise\", \"productivity\", \"automation\", \"workflow\", \"commercial\"],\n",
    "                \"Research Breakthroughs\": [\"research\", \"breakthrough\", \"paper\", \"study\", \"academic\", \"university\", \"science\"],\n",
    "                \"Industry News\": [\"company\", \"startup\", \"funding\", \"acquisition\", \"partnership\", \"launch\", \"release\"],\n",
    "                \"Other AI Topics\": []  # Catch-all\n",
    "            }\n",
    "\n",
    "            for article in articles_with_summaries:\n",
    "                url = article.get('url', '')\n",
    "                title_lower = article.get('title', '').lower()\n",
    "                description_lower = article.get('description', '').lower()\n",
    "\n",
    "                # Find best matching topic\n",
    "                best_topic = \"Other AI Topics\"  # Default\n",
    "                max_matches = 0\n",
    "\n",
    "                for topic, keywords in topic_keywords.items():\n",
    "                    if topic == \"Other AI Topics\":\n",
    "                        continue\n",
    "\n",
    "                    matches = sum(1 for keyword in keywords\n",
    "                                if keyword in title_lower or keyword in description_lower)\n",
    "\n",
    "                    if matches > max_matches:\n",
    "                        max_matches = matches\n",
    "                        best_topic = topic\n",
    "\n",
    "                # Add article URL to the appropriate cluster\n",
    "                state.topic_clusters[best_topic].append(url)\n",
    "\n",
    "                # Also update the article with cluster info\n",
    "                article['cluster_topic'] = best_topic\n",
    "                article['cluster_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "            # Remove empty clusters\n",
    "            state.topic_clusters = {\n",
    "                topic: articles for topic, articles in state.topic_clusters.items()\n",
    "                if articles\n",
    "            }\n",
    "\n",
    "            # Calculate stats\n",
    "            total_clusters = len(state.topic_clusters)\n",
    "            total_articles = sum(len(articles) for articles in state.topic_clusters.values())\n",
    "            cluster_coherence_score = 0.84  # Mock coherence score\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 5\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 5: Created {total_clusters} topic clusters\")\n",
    "\n",
    "            status_msg = f\"✅ Step 5 completed successfully! Organized {total_articles} articles into {total_clusters} topic clusters.\"\n",
    "            status_msg += f\"\\n📊 Cluster coherence score: {cluster_coherence_score:.1%}\"\n",
    "            status_msg += f\"\\n🏷️ Topics: {', '.join(state.topic_clusters.keys())}\"\n",
    "            status_msg += f\"\\n💾 Clusters stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 5 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"cluster_by_topic\",\n",
    "            description=\"Execute Step 5: Group articles by thematic topics using clustering. Requires Step 4 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._cluster_by_topic\n",
    "        )\n",
    "\n",
    "\n",
    "class RateArticlesTool:\n",
    "    \"\"\"Tool for Step 6: Rate article quality and importance\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _rate_articles(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 6: Rate Articles using persistent state\"\"\"\n",
    "        step_name = \"step_06_rate_articles\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 6:\n",
    "            rated_articles = [article for article in state.headline_data if article.get('quality_rating')]\n",
    "            avg_rating = sum(article.get('quality_rating', 0) for article in rated_articles) / len(rated_articles) if rated_articles else 0\n",
    "            return f\"Step 6 already completed! Rated {len(rated_articles)} articles with average rating {avg_rating:.1f}/10.\"\n",
    "\n",
    "        # Check if step 5 is completed\n",
    "        if state.current_step < 5:\n",
    "            return f\"❌ Cannot execute Step 6: Step 5 (Cluster By Topic) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get clustered articles from persistent state\n",
    "            clustered_articles = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and article.get('cluster_topic')\n",
    "            ]\n",
    "\n",
    "            if not clustered_articles:\n",
    "                return f\"❌ No clustered articles found to rate. Please run step 5 first.\"\n",
    "\n",
    "            # Rate each article based on mock criteria\n",
    "            articles_rated = 0\n",
    "            total_rating = 0\n",
    "            high_quality_count = 0\n",
    "\n",
    "            for article in clustered_articles:\n",
    "                # Mock rating logic - in reality, this would use AI to evaluate:\n",
    "                # - Content quality, originality, depth\n",
    "                # - Source credibility\n",
    "                # - Relevance to AI community\n",
    "                # - Timeliness and newsworthiness\n",
    "\n",
    "                title_length = len(article.get('title', ''))\n",
    "                has_description = bool(article.get('description', ''))\n",
    "                source_quality = 8 if article.get('source') in ['Techmeme', 'Ars Technica', 'The Verge'] else 6\n",
    "                cluster_bonus = 2 if article.get('cluster_topic') != 'Other AI Topics' else 0\n",
    "\n",
    "                # Calculate mock quality rating (1-10)\n",
    "                base_rating = 5\n",
    "                if title_length > 50: base_rating += 1\n",
    "                if has_description: base_rating += 1\n",
    "                rating = min(10, base_rating + (source_quality - 6) + cluster_bonus)\n",
    "\n",
    "                # Add some randomness to make it more realistic\n",
    "                import random\n",
    "                rating = max(1, min(10, rating + random.uniform(-1, 1)))\n",
    "\n",
    "                # Store rating in article data\n",
    "                article['quality_rating'] = round(rating, 1)\n",
    "                article['rating_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "                articles_rated += 1\n",
    "                total_rating += rating\n",
    "                if rating >= 7.0:\n",
    "                    high_quality_count += 1\n",
    "\n",
    "            # Calculate stats\n",
    "            avg_rating = total_rating / articles_rated if articles_rated > 0 else 0\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 6\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 6: Rated {articles_rated} articles\")\n",
    "\n",
    "            status_msg = f\"✅ Step 6 completed successfully! Rated {articles_rated} articles with average rating {avg_rating:.1f}/10.\"\n",
    "            status_msg += f\"\\n⭐ High quality articles (≥7.0): {high_quality_count}\"\n",
    "            status_msg += f\"\\n💾 Ratings stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 6 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"rate_articles\",\n",
    "            description=\"Execute Step 6: Evaluate article quality and importance with ratings. Requires Step 5 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._rate_articles\n",
    "        )\n",
    "\n",
    "\n",
    "class SelectSectionsTool:\n",
    "    \"\"\"Tool for Step 7: Select newsletter sections\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _select_sections(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 7: Select Sections using persistent state\"\"\"\n",
    "        step_name = \"step_07_select_sections\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 7:\n",
    "            section_count = len(state.newsletter_sections)\n",
    "            return f\"Step 7 already completed! Created {section_count} newsletter sections.\"\n",
    "\n",
    "        # Check if step 6 is completed\n",
    "        if state.current_step < 6:\n",
    "            return f\"❌ Cannot execute Step 7: Step 6 (Rate Articles) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get rated articles from persistent state\n",
    "            rated_articles = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and article.get('quality_rating')\n",
    "            ]\n",
    "\n",
    "            if not rated_articles:\n",
    "                return f\"❌ No rated articles found to organize into sections. Please run step 6 first.\"\n",
    "\n",
    "            # Clear existing sections if rerunning\n",
    "            state.newsletter_sections = {}\n",
    "\n",
    "            # Create newsletter sections based on topic clusters and ratings\n",
    "            # Use existing topic clusters but prioritize high-quality articles\n",
    "            high_quality_articles = [a for a in rated_articles if a.get('quality_rating', 0) >= 7.0]\n",
    "            medium_quality_articles = [a for a in rated_articles if 5.0 <= a.get('quality_rating', 0) < 7.0]\n",
    "\n",
    "            # Group articles by cluster topic and select best ones for each section\n",
    "            cluster_sections = {}\n",
    "            for article in high_quality_articles + medium_quality_articles:\n",
    "                cluster = article.get('cluster_topic', 'Other AI Topics')\n",
    "                if cluster not in cluster_sections:\n",
    "                    cluster_sections[cluster] = []\n",
    "                cluster_sections[cluster].append(article)\n",
    "\n",
    "            # Create newsletter sections with article assignments\n",
    "            articles_assigned = 0\n",
    "            for cluster, articles in cluster_sections.items():\n",
    "                if not articles:\n",
    "                    continue\n",
    "\n",
    "                # Sort articles by rating (highest first) and take top articles\n",
    "                sorted_articles = sorted(articles, key=lambda x: x.get('quality_rating', 0), reverse=True)\n",
    "                top_articles = sorted_articles[:5]  # Max 5 articles per section\n",
    "\n",
    "                # Create section outline (will be filled in step 8)\n",
    "                section_content = {\n",
    "                    'title': cluster,\n",
    "                    'article_count': len(top_articles),\n",
    "                    'articles': [{\n",
    "                        'url': article.get('url'),\n",
    "                        'title': article.get('title'),\n",
    "                        'rating': article.get('quality_rating'),\n",
    "                        'source': article.get('source')\n",
    "                    } for article in top_articles],\n",
    "                    'section_status': 'selected',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "                state.newsletter_sections[cluster] = section_content\n",
    "                articles_assigned += len(top_articles)\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 7\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 7: Created {len(state.newsletter_sections)} newsletter sections\")\n",
    "\n",
    "            status_msg = f\"✅ Step 7 completed successfully! Organized content into {len(state.newsletter_sections)} sections with {articles_assigned} articles assigned.\"\n",
    "            status_msg += f\"\\n📑 Sections: {', '.join(state.newsletter_sections.keys())}\"\n",
    "            status_msg += f\"\\n💾 Section plan stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 7 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"select_sections\",\n",
    "            description=\"Execute Step 7: Organize articles into newsletter sections based on topics and ratings. Requires Step 6 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._select_sections\n",
    "        )\n",
    "\n",
    "\n",
    "class DraftSectionsTool:\n",
    "    \"\"\"Tool for Step 8: Draft section content\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _draft_sections(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 8: Draft Sections using persistent state\"\"\"\n",
    "        step_name = \"step_08_draft_sections\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 8:\n",
    "            drafted_sections = [s for s in state.newsletter_sections.values() if s.get('content')]\n",
    "            total_words = sum(len(s.get('content', '').split()) for s in drafted_sections)\n",
    "            return f\"Step 8 already completed! Drafted {len(drafted_sections)} sections with {total_words} total words.\"\n",
    "\n",
    "        # Check if step 7 is completed\n",
    "        if state.current_step < 7:\n",
    "            return f\"❌ Cannot execute Step 8: Step 7 (Select Sections) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get section plans from persistent state\n",
    "            if not state.newsletter_sections:\n",
    "                return f\"❌ No newsletter sections found to draft. Please run step 7 first.\"\n",
    "\n",
    "            # Draft content for each section\n",
    "            sections_drafted = 0\n",
    "            total_words = 0\n",
    "\n",
    "            for section_name, section_data in state.newsletter_sections.items():\n",
    "                if section_data.get('section_status') != 'selected':\n",
    "                    continue\n",
    "\n",
    "                articles = section_data.get('articles', [])\n",
    "                if not articles:\n",
    "                    continue\n",
    "\n",
    "                # Mock section content generation - in reality, this would use AI\n",
    "                # to create engaging newsletter content from article summaries\n",
    "                section_content = f\"## {section_name}\\n\\n\"\n",
    "\n",
    "                # Add intro paragraph\n",
    "                intro_templates = {\n",
    "                    'LLM Advances': \"The latest developments in large language models continue to push the boundaries of what's possible in AI.\",\n",
    "                    'AI Safety & Ethics': \"Important discussions around responsible AI development and deployment are shaping the future of the field.\",\n",
    "                    'Business AI Applications': \"Companies are finding innovative ways to integrate AI into their products and workflows.\",\n",
    "                    'Research Breakthroughs': \"Academic researchers are making significant strides in advancing our understanding of artificial intelligence.\",\n",
    "                    'Industry News': \"The AI industry continues to evolve with new partnerships, funding rounds, and product launches.\"\n",
    "                }\n",
    "\n",
    "                intro = intro_templates.get(section_name, f\"Here are the latest updates in {section_name.lower()}.\")\n",
    "                section_content += f\"{intro}\\n\\n\"\n",
    "\n",
    "                # Add article summaries\n",
    "                for i, article in enumerate(articles[:3]):  # Top 3 articles per section\n",
    "                    article_url = article.get('url', '')\n",
    "                    article_title = article.get('title', 'Unknown Title')\n",
    "                    article_source = article.get('source', 'Unknown Source')\n",
    "\n",
    "                    # Get the actual summary from state if available\n",
    "                    summary_bullets = state.article_summaries.get(article_url, [\n",
    "                        f\"Key insights from this {section_name.lower()} article\",\n",
    "                        f\"Important implications for the AI community\",\n",
    "                        f\"Notable developments worth following\"\n",
    "                    ])\n",
    "\n",
    "                    section_content += f\"### {article_title}\\n\"\n",
    "                    section_content += f\"*Source: {article_source}*\\n\\n\"\n",
    "\n",
    "                    for bullet in summary_bullets:\n",
    "                        section_content += f\"- {bullet}\\n\"\n",
    "\n",
    "                    section_content += f\"\\n[Read more]({article_url})\\n\\n\"\n",
    "\n",
    "                # Store the drafted content\n",
    "                state.newsletter_sections[section_name]['content'] = section_content\n",
    "                state.newsletter_sections[section_name]['section_status'] = 'drafted'\n",
    "                state.newsletter_sections[section_name]['draft_timestamp'] = datetime.now().isoformat()\n",
    "                state.newsletter_sections[section_name]['word_count'] = len(section_content.split())\n",
    "\n",
    "                sections_drafted += 1\n",
    "                total_words += len(section_content.split())\n",
    "\n",
    "            # Calculate average words per section\n",
    "            avg_words_per_section = total_words / sections_drafted if sections_drafted > 0 else 0\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 8\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 8: Drafted {sections_drafted} sections\")\n",
    "\n",
    "            status_msg = f\"✅ Step 8 completed successfully! Drafted {sections_drafted} sections with {total_words} total words.\"\n",
    "            status_msg += f\"\\n📝 Average words per section: {avg_words_per_section:.0f}\"\n",
    "            status_msg += f\"\\n💾 Section content stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 8 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"draft_sections\",\n",
    "            description=\"Execute Step 8: Write engaging content for each newsletter section. Requires Step 7 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._draft_sections\n",
    "        )\n",
    "\n",
    "\n",
    "class FinalizeNewsletterTool:\n",
    "    \"\"\"Tool for Step 9: Finalize complete newsletter\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _finalize_newsletter(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 9: Finalize Newsletter using persistent state\"\"\"\n",
    "        step_name = \"step_09_finalize_newsletter\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 9:\n",
    "            newsletter_length = len(state.final_newsletter.split()) if state.final_newsletter else 0\n",
    "            sections_count = len([s for s in state.newsletter_sections.values() if s.get('content')])\n",
    "            return f\"Step 9 already completed! Newsletter finalized with {sections_count} sections and {newsletter_length} words.\"\n",
    "\n",
    "        # Check if step 8 is completed\n",
    "        if state.current_step < 8:\n",
    "            return f\"❌ Cannot execute Step 9: Step 8 (Draft Sections) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get drafted sections from persistent state\n",
    "            drafted_sections = {\n",
    "                name: data for name, data in state.newsletter_sections.items()\n",
    "                if data.get('section_status') == 'drafted' and data.get('content')\n",
    "            }\n",
    "\n",
    "            if not drafted_sections:\n",
    "                return f\"❌ No drafted sections found to finalize. Please run step 8 first.\"\n",
    "\n",
    "            # Create the final newsletter by combining all sections\n",
    "            today = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "            newsletter_content = f\"# AI News Digest - {today}\\n\\n\"\n",
    "            newsletter_content += f\"*Curated insights from the latest in artificial intelligence*\\n\\n\"\n",
    "            newsletter_content += f\"---\\n\\n\"\n",
    "\n",
    "            # Add table of contents\n",
    "            newsletter_content += \"## Table of Contents\\n\\n\"\n",
    "            for i, section_name in enumerate(drafted_sections.keys(), 1):\n",
    "                newsletter_content += f\"{i}. [{section_name}](#{section_name.lower().replace(' ', '-').replace('&', 'and')})\\n\"\n",
    "            newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "            # Add each section content\n",
    "            for section_name, section_data in drafted_sections.items():\n",
    "                newsletter_content += section_data.get('content', '')\n",
    "                newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "            # Add footer\n",
    "            newsletter_content += \"## About This Newsletter\\n\\n\"\n",
    "            newsletter_content += \"This AI News Digest was automatically curated using our intelligent newsletter agent. \"\n",
    "            newsletter_content += f\"We analyzed {len(state.headline_data)} articles from {len(set(a.get('source', '') for a in state.headline_data))} sources \"\n",
    "            newsletter_content += f\"to bring you the most relevant AI developments.\\n\\n\"\n",
    "            newsletter_content += f\"*Generated on {today}*\\n\"\n",
    "\n",
    "            # Store the final newsletter\n",
    "            state.final_newsletter = newsletter_content\n",
    "\n",
    "            # Calculate final stats\n",
    "            newsletter_length = len(newsletter_content.split())\n",
    "            sections_included = len(drafted_sections)\n",
    "\n",
    "            # Mock quality score based on content metrics\n",
    "            base_quality = 7.0\n",
    "            if sections_included >= 4: base_quality += 0.5\n",
    "            if newsletter_length >= 2000: base_quality += 0.5\n",
    "            if newsletter_length >= 3000: base_quality += 0.5\n",
    "            final_quality_score = min(10.0, base_quality)\n",
    "\n",
    "            # Mark workflow as complete\n",
    "            state.current_step = 9\n",
    "            state.workflow_complete = True\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 9: Finalized newsletter ({newsletter_length} words)\")\n",
    "\n",
    "            status_msg = f\"🎉 Step 9 completed successfully! Newsletter finalized with {sections_included} sections and {newsletter_length} words.\"\n",
    "            status_msg += f\"\\n⭐ Quality score: {final_quality_score:.1f}/10\"\n",
    "            status_msg += f\"\\n📰 Complete newsletter stored in persistent state\"\n",
    "            status_msg += f\"\\n✅ Workflow complete! All 9 steps finished successfully.\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 9 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"finalize_newsletter\",\n",
    "            description=\"Execute Step 9: Combine all sections into the final newsletter with formatting and polish. Requires Step 8 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._finalize_newsletter\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72dc48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDB = 'newsagent_logs.db'\n",
    "\n",
    "class NewsletterAgent(Agent[NewsletterAgentState]):\n",
    "    \"\"\"Newsletter agent with persistent state and workflow tools\"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str = \"newsletter_agent\", verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the NewsletterAgent with persistent state\n",
    "\n",
    "        Args:\n",
    "            session_id: Unique identifier for the session (for persistence)\n",
    "            verbose: Enable verbose logging\n",
    "        \"\"\"\n",
    "        # Initialize session for persistence\n",
    "        self.session = SQLiteSession(session_id, \"newsletter_agent.db\")\n",
    "        self.workflow_status = WorkflowStatus()  # Keep for progress tracking UI\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logger\n",
    "        self.logger = setup_logging(session_id, LOGDB)\n",
    "\n",
    "        # System prompt that guides tool selection based on workflow status\n",
    "        system_prompt = \"\"\"\n",
    "You are an AI newsletter writing agent that executes a 9-step workflow process using tools with persistent state.\n",
    "\n",
    "WORKFLOW OVERVIEW:\n",
    "1. Step 1: Gather URLs - Collect headlines and URLs from various sources\n",
    "2. Step 2: Filter URLs - Filter headlines to AI-related content only\n",
    "3. Step 3: Download Articles - Fetch full article content from URLs\n",
    "4. Step 4: Extract Summaries - Create bullet point summaries of each article\n",
    "5. Step 5: Cluster By Topic - Group articles by thematic topics\n",
    "6. Step 6: Rate Articles - Evaluate article quality and importance\n",
    "7. Step 7: Select Sections - Organize articles into newsletter sections\n",
    "8. Step 8: Draft Sections - Write content for each section\n",
    "9. Step 9: Finalize Newsletter - Combine sections into final newsletter\n",
    "\n",
    "WORKFLOW RESUME LOGIC:\n",
    "- You maintain persistent state between runs and can resume from any step\n",
    "- ALWAYS start by checking workflow status to understand current progress\n",
    "- If current_step >= 1, you can resume from any completed step forward\n",
    "- Steps are idempotent - if a step is already completed, tools will return cached results\n",
    "- When resuming, automatically continue from the next incomplete step\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- ALWAYS start by checking the current workflow status using check_workflow_status\n",
    "- Use inspect_state tool to examine detailed state data when debugging\n",
    "- Execute workflow steps in the correct order using the appropriate tools\n",
    "- Each step has prerequisites - only execute a step if the previous step is completed\n",
    "- If a user asks to \"run all steps\" or \"create the newsletter\", execute all remaining steps in sequence\n",
    "- If a user asks for a specific step, execute only that step (if prerequisites are met)\n",
    "- If a user asks to \"resume\" or \"continue\", start from the next incomplete step\n",
    "- Always check status between steps to ensure proper sequencing\n",
    "- Your state persists between sessions - you can resume work from where you left off\n",
    "\n",
    "TOOL SELECTION STRATEGY:\n",
    "1. First, always use check_workflow_status to understand current state and progress\n",
    "2. If resuming, identify the next step that needs to be executed\n",
    "3. Use the appropriate tool for that step\n",
    "4. After each step, check status again to confirm progress\n",
    "5. Continue until workflow is complete or user request is fulfilled\n",
    "\n",
    "RESUME EXAMPLES:\n",
    "- If current_step=3, next step is step 4 (Extract Summaries)\n",
    "- If current_step=7, next step is step 8 (Draft Sections)\n",
    "- If current_step=9, workflow is complete - no further steps needed\n",
    "\n",
    "Remember: Your state is persistent. You can safely resume from any point. Never skip steps or execute them out of order.\n",
    "\"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            name=\"NewsletterAgent\",\n",
    "            instructions=system_prompt,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            tools=[\n",
    "                WorkflowStatusTool(self.workflow_status, self.logger).create_tool(),\n",
    "                StateInspectionTool(self.verbose, self.logger).create_tool(),\n",
    "                GatherUrlsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                FilterUrlsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                DownloadArticlesTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                ExtractSummariesTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                ClusterByTopicTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                RateArticlesTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                SelectSectionsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                DraftSectionsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                FinalizeNewsletterTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Initialize default state\n",
    "        self.default_state = NewsletterAgentState()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Initialized NewsletterAgent with persistent state and 9-step workflow\")\n",
    "            print(f\"Session ID: {session_id}\")\n",
    "\n",
    "    async def run_step(self, user_input: str) -> str:\n",
    "        \"\"\"Run a workflow step with persistent state\"\"\"\n",
    "        result = await Runner.run(\n",
    "            self,\n",
    "            user_input,\n",
    "            session=self.session,\n",
    "            context=self.default_state,  # Will load from session if exists\n",
    "            max_turns=50  # Increased for complete 9-step workflow\n",
    "        )\n",
    "        return result.final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a1d1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized NewsletterAgent with persistent state and 9-step workflow\n",
      "Session ID: newsletter_74336930\n"
     ]
    }
   ],
   "source": [
    "news_agent = NewsletterAgent(session_id=f\"newsletter_{random.randint(10000000, 99999999)}\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed3ca518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "08:31:59 | NewsletterAgent.newsletter_74336930 | INFO | Starting check_workflow_status\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "08:32:00 | NewsletterAgent.newsletter_74336930 | INFO | Starting Step 1: Gather URLs\n",
      "08:32:01 | NewsletterAgent.newsletter_74336930 | ERROR | Step 1 failed: 'source_key'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'Ars Technica', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'Bloomberg', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'Business Insider', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch HTML: BrowserType.launch_persistent_context: Executable doesn't exist at /Users/drucev/Library/Caches/ms-playwright/firefox-1490/firefox/Nightly.app/Contents/MacOS/firefox\\n╔════════════════════════════════════════════════════════════╗\\n║ Looks like Playwright was just installed or updated.       ║\\n║ Please run the following command to download new browsers: ║\\n║                                                            ║\\n║     playwright install                                     ║\\n║                                                            ║\\n║ <3 Playwright Team                                         ║\\n╚════════════════════════════════════════════════════════════╝\"}}, {'source': 'FT', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'Feedly AI', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch HTML: BrowserType.launch_persistent_context: Executable doesn't exist at /Users/drucev/Library/Caches/ms-playwright/firefox-1490/firefox/Nightly.app/Contents/MacOS/firefox\\n╔════════════════════════════════════════════════════════════╗\\n║ Looks like Playwright was just installed or updated.       ║\\n║ Please run the following command to download new browsers: ║\\n║                                                            ║\\n║     playwright install                                     ║\\n║                                                            ║\\n║ <3 Playwright Team                                         ║\\n╚════════════════════════════════════════════════════════════╝\"}}, {'source': 'Hacker News', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'HackerNoon', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'New York Times', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'Reddit', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'Techmeme', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'The Register', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'The Verge', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'VentureBeat', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'WSJ', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'Washington Post', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'The Guardian', 'results': [], 'status': 'error', 'metadata': {'error': \"Failed to fetch RSS: 'NoneType' object has no attribute 'get'\"}}, {'source': 'NewsAPI', 'results': [{'source': 'NewsAPI', 'title': 'AirPods Live Translation Blocked for EU Users With EU Apple Accounts', 'url': 'https://www.macrumors.com/2025/09/11/airpods-live-translation-eu-restricted/', 'published': '2025-09-11T11:01:56Z'}, {'source': 'NewsAPI', 'title': \"Serena Williams raised a $111 million fund in a bubble. Here's how she's navigating venture's new reality.\", 'url': 'https://www.businessinsider.com/serena-williams-founders-vcs-getting-picky-hinted-fund-two-2025-9', 'published': '2025-09-11T09:00:02Z'}, {'source': 'NewsAPI', 'title': \"Sam Altman predicts AI will take customer service jobs first — and speed up a 'historical' rate of job turnover\", 'url': 'https://www.businessinsider.com/sam-altman-says-ai-will-speed-up-job-turnover-hit-service-roles-first-2025-9', 'published': '2025-09-11T11:03:37Z'}, {'source': 'NewsAPI', 'title': 'AVATR introduces vision xpectra concept car with prismatic glass cabin and living ‘vortex’', 'url': 'https://www.designboom.com/technology/avatr-vision-xpectra-concept-car-prismatic-glass-cabin-vortex-iaa-mobility-2025-09-11-2025/', 'published': '2025-09-11T10:55:04Z'}, {'source': 'NewsAPI', 'title': 'How threats have evolved since 9/11 attacks: ANALYSIS', 'url': 'https://abcnews.go.com/US/threats-evolved-911-attacks-analysis/story?id=125455473', 'published': '2025-09-11T09:00:54Z'}, {'source': 'NewsAPI', 'title': 'Fake Madgicx Plus and SocialMetrics Extensions Are Hijacking Meta Business Accounts', 'url': 'https://thehackernews.com/2025/09/fake-madgicx-plus-and-socialmetrics.html', 'published': '2025-09-11T09:05:00Z'}, {'source': 'NewsAPI', 'title': 'Analysts revamp Apple stock price target on iPhone 17 launch', 'url': 'https://biztoc.com/x/128078a322f70e03', 'published': '2025-09-11T12:15:41Z'}, {'source': 'NewsAPI', 'title': 'Dow Jones Futures: Nvidia, GE Vernova Lead New Buys As Oracle Drives AI Stocks', 'url': 'https://biztoc.com/x/fb61637f8a645af7', 'published': '2025-09-11T12:17:10Z'}, {'source': 'NewsAPI', 'title': \"Gen Z are changing the very nature of how we work, and most businesses aren't ready for it\", 'url': 'https://www.techradar.com/pro/gen-z-are-changing-the-very-nature-of-how-we-work-and-most-businesses-arent-ready-for-it', 'published': '2025-09-11T11:16:00Z'}, {'source': 'NewsAPI', 'title': 'Millions of Students To Have Safer Rides to School With New AI Tech', 'url': 'https://www.newsweek.com/millions-students-have-safer-rides-school-new-ai-tech-2127060', 'published': '2025-09-11T11:00:01Z'}, {'source': 'NewsAPI', 'title': 'What is AI marketing?', 'url': 'https://www.semrush.com/blog/what-is-ai-marketing/', 'published': '2025-09-11T08:54:00Z'}, {'source': 'NewsAPI', 'title': 'Using Empathy Maps to Create a Better Employee Experience', 'url': 'https://www.hrbartender.com/2025/employee-engagement/using-empathy-maps-better-employee-experience/', 'published': '2025-09-11T08:57:00Z'}, {'source': 'NewsAPI', 'title': 'FAU Study: Hotels Must Rethink Loyalty as AI Agents Take Over Travel Planning', 'url': 'https://www.hospitalitynet.org/news/4128879.html', 'published': '2025-09-11T12:07:00Z'}, {'source': 'NewsAPI', 'title': 'OpenAI’s Controversial Evolution : From Altruism to Profit-Driven Power', 'url': 'https://www.geeky-gadgets.com/openai-business-model-controversy-2025/', 'published': '2025-09-11T12:12:45Z'}, {'source': 'NewsAPI', 'title': 'AI’s Dirty Secret : How OpenAI Plans to Fix Its Biggest Flaw', 'url': 'https://www.geeky-gadgets.com/did-openai-just-solve-hallucinations/', 'published': '2025-09-11T09:14:00Z'}, {'source': 'NewsAPI', 'title': 'Trump’s Top Envoys Tell Europe U.S. Will Double Gas Exports In 5 Years', 'url': 'https://www.forbes.com/sites/gauravsharma/2025/09/11/trumps-top-envoys-tell-europe-us-will-double-gas-exports-in-5-years/', 'published': '2025-09-11T10:01:04Z'}, {'source': 'NewsAPI', 'title': 'Should You Buy Or Fear QMMM Holdings Stock?', 'url': 'https://www.forbes.com/sites/greatspeculations/2025/09/11/should-you-buy-or-fear-qmmm-holdings-stock/', 'published': '2025-09-11T10:00:00Z'}, {'source': 'NewsAPI', 'title': \"Prediction: This Artificial Intelligence (AI) Stock Will Be the Market's Biggest Winner by 2030\", 'url': 'https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_3c30e509-06ad-4091-a8b1-f27bba3f3059', 'published': '2025-09-11T10:45:00Z'}, {'source': 'NewsAPI', 'title': \"Nvidia Rises on Oracle's AI Spending Outlook\", 'url': 'https://finance.yahoo.com/news/nvidia-rises-oracles-ai-spending-123002086.html', 'published': '2025-09-11T12:30:02Z'}, {'source': 'NewsAPI', 'title': 'Google TPUs Vs Nvidia GPUs', 'url': 'https://www.forbes.com/sites/greatspeculations/2025/09/11/google-tpus-vs-nvidia-gpus/', 'published': '2025-09-11T09:54:43Z'}, {'source': 'NewsAPI', 'title': \"Amazon customer sparks fury with photo of package contents from recent online order: 'This is beyond ridiculous'\", 'url': 'https://www.yahoo.com/news/articles/amazon-customer-sparks-fury-photo-100000123.html', 'published': '2025-09-11T10:00:00Z'}, {'source': 'NewsAPI', 'title': 'The Next AI Goldmine: Profits Beyond Big Tech', 'url': 'https://www.forbes.com/sites/roomykhan/2025/09/11/the-next-ai-goldmine-profits-beyond-big-tech/', 'published': '2025-09-11T12:16:16Z'}, {'source': 'NewsAPI', 'title': \"UK public sector 'not at all familiar' with blockchain\", 'url': 'https://www.finextra.com/newsarticle/46580/uk-public-sector-not-at-all-familiar-with-blockchain', 'published': '2025-09-11T09:54:27Z'}, {'source': 'NewsAPI', 'title': 'When one Luis Suárez copies... the other Luis Suárez ', 'url': 'https://onefootball.com/en/news/when-one-luis-suarez-copies-the-other-luis-suarez-41646507', 'published': '2025-09-11T11:39:00Z'}, {'source': 'NewsAPI', 'title': 'Breaking: Laporta speaks out on return to Camp Nou', 'url': 'https://onefootball.com/en/news/breaking-laporta-speaks-out-on-return-to-camp-nou-41646535', 'published': '2025-09-11T11:47:00Z'}, {'source': 'NewsAPI', 'title': 'How The 10 Richest American Hedge Fund Managers Got $20 Billion Richer In A Year', 'url': 'https://www.forbes.com/sites/hanktucker/2025/09/11/how-the-10-richest-american-hedge-fund-managers-got-20-billion-richer-in-a-year/', 'published': '2025-09-11T10:30:00Z'}, {'source': 'NewsAPI', 'title': 'Flex Ltd. (FLEX) Benefited from the Excitement Over AI', 'url': 'https://finance.yahoo.com/news/flex-ltd-flex-benefited-excitement-115359511.html', 'published': '2025-09-11T11:53:59Z'}, {'source': 'NewsAPI', 'title': 'OpenAI Strikes $300 Billion Project Stargate Deal With Oracle To Build Next-Generation AI And Cloud Infrastructure', 'url': 'https://wccftech.com/openai-strikes-300-billion-project-stargate-deal-with-oracle-to-build-next-generation-ai-and-cloud-infrastructure/', 'published': '2025-09-11T11:54:00Z'}, {'source': 'NewsAPI', 'title': 'ai-ebash 0.1.33', 'url': 'https://pypi.org/project/ai-ebash/0.1.33/', 'published': '2025-09-11T12:02:37Z'}, {'source': 'NewsAPI', 'title': 'ai-ebash 0.1.37', 'url': 'https://pypi.org/project/ai-ebash/0.1.37/', 'published': '2025-09-11T12:21:24Z'}, {'source': 'NewsAPI', 'title': 'FedArtML 0.1.34', 'url': 'https://pypi.org/project/fedartml/0.1.34/', 'published': '2025-09-11T09:11:12Z'}, {'source': 'NewsAPI', 'title': 'ai-ebash 0.1.22', 'url': 'https://pypi.org/project/ai-ebash/0.1.22/', 'published': '2025-09-11T09:20:56Z'}, {'source': 'NewsAPI', 'title': 'Top scorers from the 10 highest-ranked FIFA nations ', 'url': 'https://onefootball.com/en/news/top-scorers-from-the-10-highest-ranked-fifa-nations-41645719', 'published': '2025-09-11T08:35:00Z'}, {'source': 'NewsAPI', 'title': ' Eriksen straight in: Bundesliga stars with the most international caps', 'url': 'https://onefootball.com/en/news/eriksen-straight-in-bundesliga-stars-with-the-most-international-caps-41646100', 'published': '2025-09-11T10:07:00Z'}, {'source': 'NewsAPI', 'title': 'Breaking news: CTA admits to TWO rule errors', 'url': 'https://onefootball.com/en/news/breaking-news-cta-admits-to-two-rule-errors-41646178', 'published': '2025-09-11T10:27:00Z'}, {'source': 'NewsAPI', 'title': 'Analysis-US dollar bears think record slide may resume after recent pause', 'url': 'https://finance.yahoo.com/news/analysis-us-dollar-bears-think-100557759.html', 'published': '2025-09-11T10:05:57Z'}, {'source': 'NewsAPI', 'title': 'US insurance firm The Hartford sets up tech centre in India', 'url': 'https://www.thehindubusinessline.com/info-tech/us-insurance-firm-the-hartford-sets-up-tech-centre-in-india/article70037002.ece', 'published': '2025-09-11T09:32:14Z'}, {'source': 'NewsAPI', 'title': 'Perplexity Valuation Hits $20 Billion Following New Funding Round', 'url': 'https://biztoc.com/x/d548edc194bda4bf', 'published': '2025-09-11T11:42:02Z'}, {'source': 'NewsAPI', 'title': 'Top Funds Etched This Long-Term In Their Brains. Then This Happened', 'url': 'https://biztoc.com/x/745f2a0e53ee7eaa', 'published': '2025-09-11T12:16:04Z'}, {'source': 'NewsAPI', 'title': 'What if the $3trn AI investment boom goes wrong?', 'url': 'https://biztoc.com/x/b9fbea5057db2694', 'published': '2025-09-11T09:06:11Z'}, {'source': 'NewsAPI', 'title': 'Companies Bet Customer Service AI Pays', 'url': 'https://biztoc.com/x/3d6a3b4acea27485', 'published': '2025-09-11T08:54:38Z'}, {'source': 'NewsAPI', 'title': '⚠️ Chelsea charged by FA with 74 breaches of agent regulations', 'url': 'https://onefootball.com/en/news/chelsea-charged-by-fa-with-74-breaches-of-agent-regulations-41646333', 'published': '2025-09-11T11:04:00Z'}, {'source': 'NewsAPI', 'title': \"What's Going On With Oracle Stock Thursday?\", 'url': 'https://biztoc.com/x/c8c81fb44e1206e5', 'published': '2025-09-11T10:24:04Z'}, {'source': 'NewsAPI', 'title': 'Research shows humans are far superior to AI on this business task', 'url': 'https://biztoc.com/x/d68d6e5895af48c9', 'published': '2025-09-11T11:19:59Z'}, {'source': 'NewsAPI', 'title': 'San Francisco Probes Scale AI Over Labor Practices After $14 Billion Meta Deal, Raising Questions On Gig Worker Rights In AI Boom: Report', 'url': 'https://biztoc.com/x/71542180968b03a0', 'published': '2025-09-11T10:46:13Z'}, {'source': 'NewsAPI', 'title': 'Prof. Ahmad AbuSalah: KFSHRC’s Sustainable Digital Innovation Ecosystem Features 30+ AI Solutions and 5 New Agents in 2025', 'url': 'https://financialpost.com/globe-newswire/prof-ahmad-abusalah-kfshrcs-sustainable-digital-innovation-ecosystem-features-30-ai-solutions-and-5-new-agents-in-2025', 'published': '2025-09-11T11:51:26Z'}, {'source': 'NewsAPI', 'title': 'Anosh Ahmed, CEO of PFOAA, Presents New Industry Survey Revealing AI Adoption to Double in Financial Services by 2026', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148407/0/en/Anosh-Ahmed-CEO-of-PFOAA-Presents-New-Industry-Survey-Revealing-AI-Adoption-to-Double-in-Financial-Services-by-2026.html', 'published': '2025-09-11T10:24:00Z'}, {'source': 'NewsAPI', 'title': \"What's viral Nano Banana’ AI trend? Check how to create 3D image for free in 7 easy steps\", 'url': 'https://economictimes.indiatimes.com/news/new-updates/whats-viral-nano-banana-ai-trend-check-how-to-create-3d-image-for-free-in-7-easy-steps/articleshow/123828561.cms', 'published': '2025-09-11T10:43:08Z'}, {'source': 'NewsAPI', 'title': 'ZenaTech Announces the Acquisition of Lescure Engineers Inc. Expanding Drone as a Service (DaaS) to California’s Precision Agriculture and Viticulture Markets', 'url': 'https://financialpost.com/globe-newswire/zenatech-announces-the-acquisition-of-lescure-engineers-inc-expanding-drone-as-a-service-daas-to-californias-precision-agriculture-and-viticulture-markets', 'published': '2025-09-11T12:01:55Z'}, {'source': 'NewsAPI', 'title': ' Copa do Brasil Wednesday packed with controversial moments, have your say', 'url': 'https://onefootball.com/en/news/copa-do-brasil-wednesday-packed-with-controversial-moments-have-your-say-41646087', 'published': '2025-09-11T10:06:00Z'}, {'source': 'NewsAPI', 'title': 'Perplexity Valuation Hits $20 Billion Following New Funding Round', 'url': 'http://www.pymnts.com/artificial-intelligence-2/2025/perplexity-valuation-hits-20-billion-following-new-funding-round/', 'published': '2025-09-11T10:42:19Z'}, {'source': 'NewsAPI', 'title': 'Build Your Own AI Assistant in 20 Minutes Without Coding Skills (No Code)', 'url': 'https://www.geeky-gadgets.com/build-ai-assistant-without-coding-guide-2025/', 'published': '2025-09-11T11:43:16Z'}, {'source': 'NewsAPI', 'title': 'Asian payment leaders initiatie Digital Wallet Guardian Partnership', 'url': 'https://www.finextra.com/pressarticle/107036/asian-payment-leaders-initiatie-digital-wallet-guardian-partnership', 'published': '2025-09-11T08:50:46Z'}, {'source': 'NewsAPI', 'title': 'What if the AI bubble bursts? How hype vs reality could majorly impact jobs, markets and daily life', 'url': 'https://economictimes.indiatimes.com/magazines/panache/what-if-the-ai-bubble-bursts-how-hype-vs-reality-could-majorly-impact-jobs-markets-and-daily-life/articleshow/123829548.cms', 'published': '2025-09-11T12:25:24Z'}, {'source': 'NewsAPI', 'title': 'Oracle’s sudden AI stardom is giving 1999 energy', 'url': 'https://biztoc.com/x/6bd8b4db06329c30', 'published': '2025-09-11T09:05:42Z'}, {'source': 'NewsAPI', 'title': 'Prof. Ahmad AbuSalah: KFSHRC’s Sustainable Digital Innovation Ecosystem Features 30+ AI Solutions and 5 New Agents in 2025', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148472/0/en/Prof-Ahmad-AbuSalah-KFSHRC-s-Sustainable-Digital-Innovation-Ecosystem-Features-30-AI-Solutions-and-5-New-Agents-in-2025.html', 'published': '2025-09-11T11:48:00Z'}, {'source': 'NewsAPI', 'title': 'China shares close near 3-year high on AI optimism', 'url': 'https://economictimes.indiatimes.com/markets/stocks/news/china-shares-close-near-3-year-high-on-ai-optimism/articleshow/123826823.cms', 'published': '2025-09-11T08:49:45Z'}, {'source': 'NewsAPI', 'title': 'Meet R1, a Chinese tech giant’s rival to Tesla’s Optimus robot', 'url': 'https://www.theverge.com/news/776224/ant-group-tesla-optimus-rival-r1', 'published': '2025-09-11T12:10:25Z'}, {'source': 'NewsAPI', 'title': 'Earnings strength likely to drive year-end rally: Matt Orton', 'url': 'https://economictimes.indiatimes.com/markets/stocks/news/earnings-strength-likely-to-drive-year-end-rally-matt-orton/articleshow/123826956.cms', 'published': '2025-09-11T08:55:22Z'}, {'source': 'NewsAPI', 'title': \"Elon Musk reclaims world's richest man position hours after Larry Ellison took lead. Check their latest net worth\", 'url': 'https://www.livemint.com/companies/people/elon-musk-reclaims-worlds-richest-man-position-hours-after-larry-ellison-took-lead-check-their-latest-net-worth-11757577125989.html', 'published': '2025-09-11T08:43:26Z'}, {'source': 'NewsAPI', 'title': 'When the Scoreboard Becomes the Game, It’s Time to Recalibrate Research Metrics', 'url': 'https://scholarlykitchen.sspnet.org/2025/09/11/guest-post-when-the-scoreboard-becomes-the-game-its-time-to-recalibrate-research-metrics/', 'published': '2025-09-11T09:30:39Z'}, {'source': 'NewsAPI', 'title': 'Figure Takes in $787 Million After Pricing IPO at $25 Per Share', 'url': 'http://www.pymnts.com/news/ipo/2025/figure-takes-in-787-million-after-pricing-ipo-at-25-per-share/', 'published': '2025-09-11T10:53:13Z'}, {'source': 'NewsAPI', 'title': 'Best Fintech Stocks To Follow Today – September 9th', 'url': 'https://www.etfdailynews.com/2025/09/11/best-fintech-stocks-to-follow-today-september-9th/', 'published': '2025-09-11T08:45:44Z'}, {'source': 'NewsAPI', 'title': 'Promising Restaurant Stocks To Watch Now – September 9th', 'url': 'https://www.etfdailynews.com/2025/09/11/promising-restaurant-stocks-to-watch-now-september-9th/', 'published': '2025-09-11T08:45:49Z'}, {'source': 'NewsAPI', 'title': 'iZotope announces Ozone 12 with new stem EQ, bass control and ‘Unlimiter’', 'url': 'https://www.attackmagazine.com/news/izotope-announces-ozone-12-with-new-stem-eq-bass-control-and-unlimiter/', 'published': '2025-09-11T11:16:13Z'}, {'source': 'NewsAPI', 'title': 'Philips and Masimo announce innovation partnership to advance access to patient monitoring measurement technologies', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148483/0/en/Philips-and-Masimo-announce-innovation-partnership-to-advance-access-to-patient-monitoring-measurement-technologies.html', 'published': '2025-09-11T12:00:00Z'}, {'source': 'NewsAPI', 'title': 'Elon Musk vs Larry Ellison: Top 5 richest people in the world', 'url': 'https://timesofindia.indiatimes.com/life-style/relationships/work/elon-musk-vs-larry-ellison-who-is-the-richest-person-in-the-world-top-5-billionaires-ranked/photostory/123830260.cms', 'published': '2025-09-11T11:32:13Z'}, {'source': 'NewsAPI', 'title': 'The Evolution of the Global Video on Demand Market | New Business Models and Content are Driving Growth in Streaming Platforms', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148369/28124/en/The-Evolution-of-the-Global-Video-on-Demand-Market-New-Business-Models-and-Content-are-Driving-Growth-in-Streaming-Platforms.html', 'published': '2025-09-11T08:39:00Z'}, {'source': 'NewsAPI', 'title': 'China VLA Large Model Applications in Automotive and Robotics Research Report 2025 | Robots on the Rise - Over 100 VLA Models Poised to Transform Industries', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148371/28124/en/China-VLA-Large-Model-Applications-in-Automotive-and-Robotics-Research-Report-2025-Robots-on-the-Rise-Over-100-VLA-Models-Poised-to-Transform-Industries.html', 'published': '2025-09-11T08:49:00Z'}, {'source': 'NewsAPI', 'title': 'Construction 4.0 Strategic Business Research Report 2025 | Market to Reach $42.2 Billion by 2030 - Automation and Robotics in Construction Spur Demand for Smart Site Technologies', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148471/28124/en/Construction-4-0-Strategic-Business-Research-Report-2025-Market-to-Reach-42-2-Billion-by-2030-Automation-and-Robotics-in-Construction-Spur-Demand-for-Smart-Site-Technologies.html', 'published': '2025-09-11T11:44:00Z'}, {'source': 'NewsAPI', 'title': 'How are guest expectations changing and how can hotels prepare for the future?', 'url': 'https://www.hospitalitynet.org/news/4128877.html', 'published': '2025-09-11T11:58:15Z'}, {'source': 'NewsAPI', 'title': 'Marquess School of Global Assets and BLUZOR Exchange Unite: Pioneering a New Era of AI and Blockchain-Driven Wealth', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148470/0/en/Marquess-School-of-Global-Assets-and-BLUZOR-Exchange-Unite-Pioneering-a-New-Era-of-AI-and-Blockchain-Driven-Wealth.html', 'published': '2025-09-11T11:43:00Z'}, {'source': 'NewsAPI', 'title': 'Little-known AI stock rockets past giants Nvidia and Palantir – here’s why it’s hot in 2025', 'url': 'https://economictimes.indiatimes.com/news/international/us/ai-stock-nbis-nebius-group-stock-surges-in-2025-outperforming-nvidia-and-palantir-latest-stock-market-news/articleshow/123830684.cms', 'published': '2025-09-11T12:25:02Z'}, {'source': 'NewsAPI', 'title': \"Elon Musk snatched back world's richest crown from Oracle co-founder Larry Ellison? Check their net worths\", 'url': 'https://economictimes.indiatimes.com/news/international/global-trends/us-news-elon-musk-snatched-back-worlds-richest-crown-from-oracle-co-founder-larry-ellison-check-their-net-worths/articleshow/123827895.cms', 'published': '2025-09-11T09:48:33Z'}, {'source': 'NewsAPI', 'title': 'CCC Intelligent Solutions Holdings Inc. $CCCS Shares Bought by MIRAE ASSET GLOBAL ETFS HOLDINGS Ltd.', 'url': 'https://www.etfdailynews.com/2025/09/11/ccc-intelligent-solutions-holdings-inc-cccs-shares-bought-by-mirae-asset-global-etfs-holdings-ltd/', 'published': '2025-09-11T08:32:18Z'}, {'source': 'NewsAPI', 'title': 'A Landmark Pivotal Study Applying Precision Medicine to Psychiatry with Denovo’s DB104 (liafensine) for Treatment-Resistant Depression Published in JAMA Psychiatry', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148427/0/en/A-Landmark-Pivotal-Study-Applying-Precision-Medicine-to-Psychiatry-with-Denovo-s-DB104-liafensine-for-Treatment-Resistant-Depression-Published-in-JAMA-Psychiatry.html', 'published': '2025-09-11T11:00:00Z'}, {'source': 'NewsAPI', 'title': 'Cosmos Health Expands Presence in the $7 Billion GCC Nutraceuticals Market, Signs Distribution Agreement with Scientific Pharmacy for Sky Premium Life in Oman, Including Initial Purchase Order of 42,000 Units', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148513/0/en/Cosmos-Health-Expands-Presence-in-the-7-Billion-GCC-Nutraceuticals-Market-Signs-Distribution-Agreement-with-Scientific-Pharmacy-for-Sky-Premium-Life-in-Oman-Including-Initial-Purch.html', 'published': '2025-09-11T12:00:00Z'}, {'source': 'NewsAPI', 'title': 'Philips and Masimo announce innovation partnership to advance access to patient monitoring measurement technologies', 'url': 'https://financialpost.com/globe-newswire/philips-and-masimo-announce-innovation-partnership-to-advance-access-to-patient-monitoring-measurement-technologies', 'published': '2025-09-11T12:01:37Z'}, {'source': 'NewsAPI', 'title': 'Foundation for inclusive growth, consumption-led economy laid with GST cut: Hyundai Motor India MD', 'url': 'https://economictimes.indiatimes.com/industry/auto/auto-news/foundation-for-inclusive-growth-consumption-led-economy-laid-with-gst-cut-hyundai-motor-india-md/articleshow/123827602.cms', 'published': '2025-09-11T09:29:51Z'}, {'source': 'NewsAPI', 'title': 'AIIMS launches AI-based app to tackle suicides, improve mental health among students', 'url': 'https://indianexpress.com/article/education/aiims-delhi-launches-ai-based-app-to-tackle-suicides-improve-mental-health-students-neet-10243824/', 'published': '2025-09-11T10:54:26Z'}, {'source': 'NewsAPI', 'title': 'First French AI billionaires emerge after €11.7 billion Mistral funding round', 'url': 'https://economictimes.indiatimes.com/tech/artificial-intelligence/first-french-ai-billionaires-emerge-after-11-7-billion-mistral-funding-round/articleshow/123829197.cms', 'published': '2025-09-11T10:40:53Z'}, {'source': 'NewsAPI', 'title': 'Asian shares are mostly up after US stocks inch to more records as inflation slows', 'url': 'https://abcnews.go.com/Business/wireStory/asian-shares-after-us-stocks-inch-records-inflation-125469112', 'published': '2025-09-11T09:23:21Z'}, {'source': 'NewsAPI', 'title': 'Autonomous Vehicle Sensor Market Size Worth USD 25.71 Billion by 2032, Driven by Rapid Advancements in AI-enabled Sensor Fusion | SNS Insider', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148481/0/en/Autonomous-Vehicle-Sensor-Market-Size-Worth-USD-25-71-Billion-by-2032-Driven-by-Rapid-Advancements-in-AI-enabled-Sensor-Fusion-SNS-Insider.html', 'published': '2025-09-11T12:00:00Z'}, {'source': 'NewsAPI', 'title': 'MIRAE ASSET GLOBAL ETFS HOLDINGS Ltd. Increases Holdings in Informatica Inc. $INFA', 'url': 'https://www.etfdailynews.com/2025/09/11/mirae-asset-global-etfs-holdings-ltd-increases-holdings-in-informatica-inc-infa/', 'published': '2025-09-11T09:05:41Z'}, {'source': 'NewsAPI', 'title': 'C.H. Robinson Introduces Cross-border Freight Consolidation Service', 'url': 'https://financialpost.com/pmn/business-wire-news-releases-pmn/c-h-robinson-introduces-cross-border-freight-consolidation-service', 'published': '2025-09-11T09:08:28Z'}, {'source': 'NewsAPI', 'title': 'Visteon Corporation $VC Shares Acquired by Advisors Asset Management Inc.', 'url': 'https://www.etfdailynews.com/2025/09/11/visteon-corporation-vc-shares-acquired-by-advisors-asset-management-inc/', 'published': '2025-09-11T09:04:59Z'}, {'source': 'NewsAPI', 'title': 'Family offices double down on stocks and dial back on private equity', 'url': 'https://www.cnbc.com/2025/09/11/family-office-investing-portfolio-allocation-goldman-sachs-survey.html', 'published': '2025-09-11T12:03:50Z'}, {'source': 'NewsAPI', 'title': 'Nokia ups the ante in AI-optimised datacentre networking', 'url': 'https://www.computerweekly.com/news/366630503/Nokia-ups-the-ante-in-AI-optimised-datacentre-networking', 'published': '2025-09-11T11:09:00Z'}, {'source': 'NewsAPI', 'title': 'EV Plant Construction Global Strategic Business Analysis Report 2025: Market to Reach $64.4 Billion by 2030 - Growing Investments in Advanced Manufacturing Technology Propel Plant Innovations', 'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148452/28124/en/EV-Plant-Construction-Global-Strategic-Business-Analysis-Report-2025-Market-to-Reach-64-4-Billion-by-2030-Growing-Investments-in-Advanced-Manufacturing-Technology-Propel-Plant-Inno.html', 'published': '2025-09-11T11:22:00Z'}, {'source': 'NewsAPI', 'title': 'From Dhule to your phone: Fake ₹500 notes are going viral online', 'url': 'https://www.livemint.com/industry/fake-500-notes-viral-online-instagram-facebook-reserve-bank-of-india-counterfeit-currency-banking-11757587071723.html', 'published': '2025-09-11T11:30:49Z'}, {'source': 'NewsAPI', 'title': 'Chat Control: EU to decide on requirement for tech firms to scan encrypted messages', 'url': 'https://www.computerweekly.com/news/366630597/Chat-Control-EU-to-decide-on-requirement-for-tech-firms-to-scan-encrypted-messages', 'published': '2025-09-11T12:10:00Z'}, {'source': 'NewsAPI', 'title': 'Alien: Earth – how realistic are the extraterrestrials? Three experts rank them', 'url': 'https://theconversation.com/alien-earth-how-realistic-are-the-extraterrestrials-three-experts-rank-them-263553', 'published': '2025-09-11T10:57:09Z'}, {'source': 'NewsAPI', 'title': 'How to stop AI from straining networks', 'url': 'https://www.computerweekly.com/news/366630272/How-to-stop-AI-from-straining-networks', 'published': '2025-09-11T10:48:00Z'}, {'source': 'NewsAPI', 'title': 'AI’s replacement of humans in HR is emblematic of what could happen across the workplace', 'url': 'https://www.irishtimes.com/business/work/2025/09/11/ais-replacement-of-humans-in-hr-is-emblematic-of-what-could-happen-across-the-workplace/', 'published': '2025-09-11T09:00:00Z'}, {'source': 'NewsAPI', 'title': '384 motorists illegally passed Chesterfield school buses in a single day', 'url': 'https://richmond.com/news/local/education/k-12/article_6da53c17-f566-4f97-9fe8-a2f71740f902.html', 'published': '2025-09-11T09:00:00Z'}, {'source': 'NewsAPI', 'title': 'The quiet war:\\xa0How\\xa0terror\\xa0has\\xa0evolved\\xa0since Sept 11', 'url': 'https://www.foxnews.com/opinion/quiet-war-how-terror-has-evolved-since-sept-11', 'published': '2025-09-11T09:00:00Z'}, {'source': 'NewsAPI', 'title': \"Musk loses crown as the world's richest person to Larry Ellison and then snatches it back\", 'url': 'https://www.wsbtv.com/news/musk-loses-crown/BAKGJRIOYVCBDF7DVKIU6SWGY4/', 'published': '2025-09-11T08:50:56Z'}, {'source': 'NewsAPI', 'title': 'Stacey Abrams Has the Write Stuff', 'url': 'https://www.portlandmercury.com/fall-arts-2025/2025/09/11/48017947/stacey-abrams-has-the-write-stuff', 'published': '2025-09-11T10:34:00Z'}], 'status': 'success', 'metadata': {'total_results': 101, 'articles_returned': 98, 'query': 'artificial intelligence', 'date_from': '2025-09-11T08:32:00'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "UserError",
     "evalue": "Error running tool inspect_state: 'NewsletterAgentState' object has no attribute 'article_summaries'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/_run_impl.py:575\u001b[39m, in \u001b[36mRunImpl.execute_function_tool_calls.<locals>.run_single_tool\u001b[39m\u001b[34m(func_tool, tool_call)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     _, _, result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    576\u001b[39m         hooks.on_tool_start(tool_context, agent, func_tool),\n\u001b[32m    577\u001b[39m         (\n\u001b[32m    578\u001b[39m             agent.hooks.on_tool_start(tool_context, agent, func_tool)\n\u001b[32m    579\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m agent.hooks\n\u001b[32m    580\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m _coro.noop_coroutine()\n\u001b[32m    581\u001b[39m         ),\n\u001b[32m    582\u001b[39m         func_tool.on_invoke_tool(tool_context, tool_call.arguments),\n\u001b[32m    583\u001b[39m     )\n\u001b[32m    585\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    586\u001b[39m         hooks.on_tool_end(tool_context, agent, func_tool, result),\n\u001b[32m    587\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m    591\u001b[39m         ),\n\u001b[32m    592\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mStateInspectionTool._inspect_state\u001b[39m\u001b[34m(self, ctx, args)\u001b[39m\n\u001b[32m    112\u001b[39m     report_lines.extend([\n\u001b[32m    113\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  AI-related: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai_related\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    114\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  With content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwith_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Sources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(a.get(\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39ma\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstate.headline_data))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    118\u001b[39m     ])\n\u001b[32m    120\u001b[39m report_lines.extend([\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    122\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPROCESSING RESULTS:\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Article summaries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43marticle_summaries\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m articles\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Topic clusters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(state.topic_clusters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m topics\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    125\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Newsletter sections: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(state.newsletter_sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sections\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    126\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Final newsletter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mGenerated\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mstate.final_newsletter\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNot created\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    127\u001b[39m ])\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state.topic_clusters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NewsletterAgentState' object has no attribute 'article_summaries'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mUserError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m user_prompt = \u001b[33m\"\u001b[39m\u001b[33mRun all the workflow steps in order and create the newsletter\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m news_agent.run_step(user_prompt)\n\u001b[32m      5\u001b[39m duration = time.time() - start_time\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mNewsletterAgent.run_step\u001b[39m\u001b[34m(self, user_input)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_input: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     97\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run a workflow step with persistent state\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m     99\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    100\u001b[39m         user_input,\n\u001b[32m    101\u001b[39m         session=\u001b[38;5;28mself\u001b[39m.session,\n\u001b[32m    102\u001b[39m         context=\u001b[38;5;28mself\u001b[39m.default_state,  \u001b[38;5;66;03m# Will load from session if exists\u001b[39;00m\n\u001b[32m    103\u001b[39m         max_turns=\u001b[32m50\u001b[39m  \u001b[38;5;66;03m# Increased for complete 9-step workflow\u001b[39;00m\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.final_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/run.py:267\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    268\u001b[39m     starting_agent,\n\u001b[32m    269\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    270\u001b[39m     context=context,\n\u001b[32m    271\u001b[39m     max_turns=max_turns,\n\u001b[32m    272\u001b[39m     hooks=hooks,\n\u001b[32m    273\u001b[39m     run_config=run_config,\n\u001b[32m    274\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    275\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    276\u001b[39m     session=session,\n\u001b[32m    277\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/run.py:504\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    482\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    483\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    501\u001b[39m         ),\n\u001b[32m    502\u001b[39m     )\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    505\u001b[39m         agent=current_agent,\n\u001b[32m    506\u001b[39m         all_tools=all_tools,\n\u001b[32m    507\u001b[39m         original_input=original_input,\n\u001b[32m    508\u001b[39m         generated_items=generated_items,\n\u001b[32m    509\u001b[39m         hooks=hooks,\n\u001b[32m    510\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    511\u001b[39m         run_config=run_config,\n\u001b[32m    512\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    513\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    514\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    515\u001b[39m         conversation_id=conversation_id,\n\u001b[32m    516\u001b[39m     )\n\u001b[32m    517\u001b[39m should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    519\u001b[39m model_responses.append(turn_result.model_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/run.py:1175\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id, conversation_id)\u001b[39m\n\u001b[32m   1157\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m   1159\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1160\u001b[39m     agent,\n\u001b[32m   1161\u001b[39m     system_prompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1172\u001b[39m     prompt_config,\n\u001b[32m   1173\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1176\u001b[39m     agent=agent,\n\u001b[32m   1177\u001b[39m     original_input=original_input,\n\u001b[32m   1178\u001b[39m     pre_step_items=generated_items,\n\u001b[32m   1179\u001b[39m     new_response=new_response,\n\u001b[32m   1180\u001b[39m     output_schema=output_schema,\n\u001b[32m   1181\u001b[39m     all_tools=all_tools,\n\u001b[32m   1182\u001b[39m     handoffs=handoffs,\n\u001b[32m   1183\u001b[39m     hooks=hooks,\n\u001b[32m   1184\u001b[39m     context_wrapper=context_wrapper,\n\u001b[32m   1185\u001b[39m     run_config=run_config,\n\u001b[32m   1186\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1187\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/run.py:1215\u001b[39m, in \u001b[36mAgentRunner._get_single_step_result_from_response\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, pre_step_items, new_response, output_schema, handoffs, hooks, context_wrapper, run_config, tool_use_tracker)\u001b[39m\n\u001b[32m   1205\u001b[39m processed_response = RunImpl.process_model_response(\n\u001b[32m   1206\u001b[39m     agent=agent,\n\u001b[32m   1207\u001b[39m     all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1210\u001b[39m     handoffs=handoffs,\n\u001b[32m   1211\u001b[39m )\n\u001b[32m   1213\u001b[39m tool_use_tracker.add_tool_use(agent, processed_response.tools_used)\n\u001b[32m-> \u001b[39m\u001b[32m1215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m RunImpl.execute_tools_and_side_effects(\n\u001b[32m   1216\u001b[39m     agent=agent,\n\u001b[32m   1217\u001b[39m     original_input=original_input,\n\u001b[32m   1218\u001b[39m     pre_step_items=pre_step_items,\n\u001b[32m   1219\u001b[39m     new_response=new_response,\n\u001b[32m   1220\u001b[39m     processed_response=processed_response,\n\u001b[32m   1221\u001b[39m     output_schema=output_schema,\n\u001b[32m   1222\u001b[39m     hooks=hooks,\n\u001b[32m   1223\u001b[39m     context_wrapper=context_wrapper,\n\u001b[32m   1224\u001b[39m     run_config=run_config,\n\u001b[32m   1225\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/_run_impl.py:253\u001b[39m, in \u001b[36mRunImpl.execute_tools_and_side_effects\u001b[39m\u001b[34m(cls, agent, original_input, pre_step_items, new_response, processed_response, output_schema, hooks, context_wrapper, run_config)\u001b[39m\n\u001b[32m    250\u001b[39m new_step_items.extend(processed_response.new_items)\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# First, lets run the tool calls - function tools and computer actions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m function_results, computer_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    254\u001b[39m     \u001b[38;5;28mcls\u001b[39m.execute_function_tool_calls(\n\u001b[32m    255\u001b[39m         agent=agent,\n\u001b[32m    256\u001b[39m         tool_runs=processed_response.functions,\n\u001b[32m    257\u001b[39m         hooks=hooks,\n\u001b[32m    258\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    259\u001b[39m         config=run_config,\n\u001b[32m    260\u001b[39m     ),\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mcls\u001b[39m.execute_computer_actions(\n\u001b[32m    262\u001b[39m         agent=agent,\n\u001b[32m    263\u001b[39m         actions=processed_response.computer_actions,\n\u001b[32m    264\u001b[39m         hooks=hooks,\n\u001b[32m    265\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    266\u001b[39m         config=run_config,\n\u001b[32m    267\u001b[39m     ),\n\u001b[32m    268\u001b[39m )\n\u001b[32m    269\u001b[39m new_step_items.extend([result.run_item \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m function_results])\n\u001b[32m    270\u001b[39m new_step_items.extend(computer_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/asyncio/tasks.py:279\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         result = coro.throw(exc)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    282\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/_run_impl.py:613\u001b[39m, in \u001b[36mRunImpl.execute_function_tool_calls\u001b[39m\u001b[34m(cls, agent, tool_runs, hooks, context_wrapper, config)\u001b[39m\n\u001b[32m    610\u001b[39m     function_tool = tool_run.function_tool\n\u001b[32m    611\u001b[39m     tasks.append(run_single_tool(function_tool, tool_run.tool_call))\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    616\u001b[39m     FunctionToolResult(\n\u001b[32m    617\u001b[39m         tool=tool_run.function_tool,\n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool_run, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tool_runs, results)\n\u001b[32m    626\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/asyncio/tasks.py:279\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         result = coro.throw(exc)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    282\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/asdk/lib/python3.11/site-packages/agents/_run_impl.py:602\u001b[39m, in \u001b[36mRunImpl.execute_function_tool_calls.<locals>.run_single_tool\u001b[39m\u001b[34m(func_tool, tool_call)\u001b[39m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, AgentsException):\n\u001b[32m    601\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError running tool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_tool.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.trace_include_sensitive_data:\n\u001b[32m    605\u001b[39m     span_fn.span_data.output = result\n",
      "\u001b[31mUserError\u001b[39m: Error running tool inspect_state: 'NewsletterAgentState' object has no attribute 'article_summaries'"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Run all the workflow steps in order and create the newsletter\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = await news_agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock context\n",
    "class MockContext:\n",
    "    def __init__(self):\n",
    "        self.context = news_agent.default_state\n",
    "\n",
    "ctx = MockContext()\n",
    "current_state = ctx.context  # From your previous run, or reload it\n",
    "df = current_state.headline_df\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    current_state = news_agent.session.get_state()\n",
    "except:\n",
    "    current_state = news_agent.default_state\n",
    "\n",
    "print(current_state)\n",
    "print()\n",
    "\n",
    "print(f\"Current Step: {current_state.current_step}/9\")\n",
    "print(f\"Workflow Complete: {current_state.workflow_complete}\")\n",
    "print(f\"Progress: {(current_state.current_step/9)*100:.1f}%\")\n",
    "print(f\"Total articles: {len(current_state.headline_data)}\")\n",
    "\n",
    "if current_state.headline_data:\n",
    "    ai_related = sum(1 for a in current_state.headline_data if a.get('ai_related') is True)\n",
    "    print(f\"AI-related articles: {ai_related}\")\n",
    "    print(f\"Summaries: {len(current_state.article_summaries)}\")\n",
    "    print(f\"Clusters: {len(current_state.topic_clusters)}\")\n",
    "    print(f\"Sections: {len(current_state.newsletter_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review slides\n",
    "\n",
    "# review workflow status, move to a moadule\n",
    "# all prints should be logs\n",
    "# section writing and composition will have the critic /optimizer loop\n",
    "# add batch with async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_news_dataframe():\n",
    "    \"\"\"\n",
    "    Creates an empty DataFrame to support headline/article analysis\n",
    "    - URLs, source tracking and metadata\n",
    "    - Topic classification and clustering\n",
    "    - Content quality ratings and rankings\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Empty DataFrame with predefined column structure\n",
    "    \"\"\"\n",
    "\n",
    "    # column structure\n",
    "    column_dict = {\n",
    "        # Core identifiers and source info\n",
    "        'article_id': 'object',              # Unique identifier for each article\n",
    "        'source':     'object',              # Source category\n",
    "        'headline_title': 'object',          # Article headline/title\n",
    "        'original_url': 'object',            # Initial URL before redirects\n",
    "        'final_url': 'object',               # URL after following redirects\n",
    "        'domain_name': 'category',           # Website domain\n",
    "        'site_name': 'category',             # Human-readable site name\n",
    "        'site_reputation_score': 'float32',  # Reputation/trustworthiness score for the site\n",
    "        'keep_flag': 'boolean',\n",
    "\n",
    "        # File paths and storage\n",
    "        'html_file_path': 'object',          # Path to stored HTML content\n",
    "        'text_file_path': 'object',          # Path to extracted text content\n",
    "\n",
    "        # Time information\n",
    "        'last_updated_timestamp': 'datetime64[ns]',  # When article was last updated\n",
    "        'article_age_days': 'int32',         # Age of article in days\n",
    "        'recency_score': 'float32',          # Calculated recency score (higher = more recent)\n",
    "\n",
    "        # Content analysis\n",
    "        'content_summary': 'object',         # Generated summary of article content\n",
    "        'bullet_points': 'object',           # Key points extracted as bullets\n",
    "        'article_length_chars': 'int32',     # Character count of article content\n",
    "\n",
    "        # Rating flags (LLM-generated probabilities)\n",
    "        'is_high_quality': 'float32',        # LLM probability for low-quality content\n",
    "        'is_off_topic': 'float32',           # LLM probability for off-topic content\n",
    "        'is_low_importance': 'float32',      # 1-LLM probability for high-importance content\n",
    "\n",
    "        # Other ratings\n",
    "        'bradley_terry_score': 'float32',    # Bradley-Terry rating from pairwise article comparisons\n",
    "        'bradley_terry_rank': 'int32',       # Ordinal rank based on Bradley-Terry scores (1 = highest rated)\n",
    "        'adjusted_length_score': 'float32',  # Length-adjusted quality score\n",
    "        'final_composite_rating': 'float32', # Final weighted rating combining multiple factors\n",
    "\n",
    "        # Topic classification\n",
    "        'topic_string': 'object',            # Topic labels as comma-separated string\n",
    "        'topic_list': 'object',              # Topic labels as list/array structure (same topics, different format)\n",
    "\n",
    "        # Organization and clustering (HDBSCAN-based)\n",
    "        'display_order': 'int32',            # Order for display/presentation\n",
    "        'cluster_id': 'int32',               # HDBSCAN cluster identifier (-1 = noise/outlier)\n",
    "        'cluster_label': 'category'          # Human-readable cluster name/description\n",
    "    }\n",
    "\n",
    "    # Create empty DataFrame from column dictionary\n",
    "    df = pd.DataFrame(columns=list(column_dict.keys())).astype(column_dict)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a22a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsletterState:\n",
    "    \"\"\"\n",
    "    Maintains session state for the OpenAI Agents SDK workflow.\n",
    "\n",
    "    Attributes:\n",
    "        headline_df: DataFrame containing headline data for processing\n",
    "        sources_file: Path to YAML file containing source configurations\n",
    "        sources: Dictionary of source configurations loaded from YAML\n",
    "        cluster_topics: List of clean topic names for headline categorization\n",
    "        max_edits: Maximum number of critic optimizer editing iterations allowed\n",
    "        edit_complete: Boolean flag indicating if editing process is finished\n",
    "        n_browsers: Number of concurrent Playwright browser instances for downloads\n",
    "    \"\"\"\n",
    "\n",
    "    status: WorkflowStatus = WorkflowStatus()\n",
    "    headline_df: pd.DataFrame = field(default_factory=create_news_dataframe)\n",
    "    sources_file: str = field(default=\"sources.yaml\")\n",
    "    sources: Dict[str, Any] = field(default_factory=dict)\n",
    "    cluster_topics: List[str] = field(default_factory=list)\n",
    "    max_edits: int = field(default=3)\n",
    "    edit_complete: bool = field(default=False)\n",
    "    n_browsers: int = field(default=8)\n",
    "    verbose: bool = field(default=True)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization validation and setup.\n",
    "\n",
    "        Validates that the configuration makes sense and performs\n",
    "        any necessary initialization steps.\n",
    "        \"\"\"\n",
    "        # Validate max_edits is reasonable\n",
    "        if self.max_edits < 1 or self.max_edits > 10:\n",
    "            raise ValueError(f\"max_edits should be between 1-10, got {self.max_edits}\")\n",
    "\n",
    "        # Validate n_browsers is reasonable\n",
    "        if self.n_browsers < 1 or self.n_browsers > 32:\n",
    "            raise ValueError(f\"n_browsers should be between 1-32, got {self.n_browsers}\")\n",
    "\n",
    "        # Validate sources_file exists and load sources from file automatically\n",
    "        try:\n",
    "            sources_path = Path(self.sources_file)\n",
    "            with open(sources_path, 'r', encoding='utf-8') as file:\n",
    "                self.sources = yaml.safe_load(file) or {}\n",
    "            if self.verbose:\n",
    "                print(f\"Loaded {len(self.sources)} sources from {self.sources_file}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Sources file not found: {self.sources_file}\")\n",
    "        except yaml.YAMLError as e:\n",
    "            raise ValueError(f\"Error parsing YAML file {self.sources_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = NewsletterState()\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, SQLiteSession, function_tool, RunContextWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c475f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsletterAgent(Agent[NewsletterState]):\n",
    "    \"\"\"AI newsletter writing agent with structured workflow\"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str = \"newsletter_agent\"):\n",
    "        self.session = SQLiteSession(session_id, \"newsletter.db\")\n",
    "        self.state = NewsletterState()\n",
    "\n",
    "        super().__init__(\n",
    "            name=\"AINewsletterAgent\",\n",
    "            instructions=\"\"\"\n",
    "            You are an AI newsletter writing agent. Your role is to:\n",
    "            1. Scrape headlines and URLs from various sources\n",
    "            2. Filter the headlines to ones that are about AI\n",
    "            3. Fetch the URLs and save them as plain text\n",
    "            4. Summarize each article to 3 bullet points containing the key facts\n",
    "            5. Extract topics from each article and cluster articles by topic\n",
    "            6. Rate each article according to the provided rubric\n",
    "            7. Identify 6-15 thematic sections + \"Other News\", assign articles to sections and deduplicate\n",
    "            8. Write each section\n",
    "            9. Combine sections and polish\n",
    "\n",
    "            Use the tools available to accomplish these tasks in order.\n",
    "            Always maintain context about workflow progress and data.\n",
    "            Guide users through the workflow steps systematically.\n",
    "            \"\"\",\n",
    "            tools=[\n",
    "                self.step1_scrape_headlines,\n",
    "                self.step2_filter_ai_headlines,\n",
    "                self.step3_fetch_article_texts,\n",
    "                self.step4_summarize_articles,\n",
    "                self.step5_extract_and_cluster_topics,\n",
    "                self.step6_rate_articles,\n",
    "                self.step7_organize_sections,\n",
    "                self.step8_write_sections,\n",
    "                self.step9_finalize_newsletter,\n",
    "                self.get_workflow_status,\n",
    "                self.run_complete_workflow,\n",
    "                self.reset_workflow\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @function_tool\n",
    "    async def step1_scrape_headlines(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        sources: List[str] = None,\n",
    "        max_articles_per_source: int = 50\n",
    "    ) -> str:\n",
    "        \"\"\"Step 1: Scrape headlines and URLs from various sources\"\"\"\n",
    "        if sources is None:\n",
    "            sources = [\"techcrunch\", \"arstechnica\", \"theverge\", \"wired\", \"venturebeat\"]\n",
    "\n",
    "        scraped_data = []\n",
    "\n",
    "        # Mock scraping implementation (replace with real RSS/API scraping)\n",
    "        for source in sources:\n",
    "            for i in range(max_articles_per_source):\n",
    "                article = {\n",
    "                    'title': f\"{source} AI Article {i+1}: Latest developments in machine learning\",\n",
    "                    'url': f\"https://{source}.com/ai-article-{i+1}\",\n",
    "                    'source': source,\n",
    "                    'published_at': (datetime.now() - timedelta(hours=i)).isoformat(),\n",
    "                    'description': f\"AI-related content from {source}\"\n",
    "                }\n",
    "                scraped_data.append(article)\n",
    "\n",
    "        wrapper.context.raw_headlines = scraped_data\n",
    "        wrapper.context.scraped_urls = [article['url'] for article in scraped_data]\n",
    "        wrapper.context.current_step = 1\n",
    "\n",
    "        return f\"✅ Step 1 Complete: Scraped {len(scraped_data)} headlines from {len(sources)} sources\"\n",
    "\n",
    "\n",
    "    @function_tool\n",
    "    async def step2_filter_ai_content(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        ai_keywords: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Step 2: Filter headlines to AI-related content only\"\"\"\n",
    "        if not wrapper.context.raw_headlines:\n",
    "            return \"❌ No headlines to filter. Run step 1 first.\"\n",
    "\n",
    "        if ai_keywords is None:\n",
    "            ai_keywords = [\n",
    "                'ai', 'artificial intelligence', 'machine learning', 'deep learning',\n",
    "                'neural network', 'llm', 'gpt', 'transformer', 'chatbot', 'automation',\n",
    "                'computer vision', 'nlp', 'natural language', 'algorithm', 'model'\n",
    "            ]\n",
    "\n",
    "        ai_articles = []\n",
    "        for article in wrapper.context.raw_headlines:\n",
    "            title_lower = article['title'].lower()\n",
    "            desc_lower = article['description'].lower()\n",
    "\n",
    "            # Check if any AI keywords are present\n",
    "            if any(keyword in title_lower or keyword in desc_lower for keyword in ai_keywords):\n",
    "                ai_articles.append(article)\n",
    "\n",
    "        wrapper.context.ai_headlines = pd.DataFrame(ai_articles)\n",
    "        wrapper.context.current_step = 2\n",
    "\n",
    "        return f\"✅ Step 2 Complete: Filtered to {len(ai_articles)} AI-related headlines from {len(wrapper.context.raw_headlines)} total\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step3_fetch_article_texts(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 3: Fetch full article texts from URLs\"\"\"\n",
    "        if wrapper.context.ai_headlines.empty:\n",
    "            return \"❌ No AI headlines to fetch. Complete steps 1-2 first.\"\n",
    "\n",
    "        # Mock article fetching (replace with actual web scraping)\n",
    "        article_texts = {}\n",
    "\n",
    "        for _, row in wrapper.context.ai_headlines.iterrows():\n",
    "            url = row['url']\n",
    "            # Mock article content\n",
    "            article_texts[url] = f\"\"\"\n",
    "            {row['title']}\n",
    "\n",
    "            This is a mock article about AI developments. In a real implementation,\n",
    "            you would use libraries like requests + BeautifulSoup or newspaper3k\n",
    "            to extract the full article text from the URL.\n",
    "\n",
    "            Key points about this AI story:\n",
    "            - Advancement in machine learning techniques\n",
    "            - Impact on industry applications\n",
    "            - Future implications for AI development\n",
    "\n",
    "            This content would be much longer in practice, containing the full\n",
    "            article text that needs to be summarized and analyzed.\n",
    "            \"\"\"\n",
    "\n",
    "        wrapper.context.article_texts = article_texts\n",
    "        wrapper.context.current_step = 3\n",
    "\n",
    "        return f\"✅ Step 3 Complete: Fetched full text for {len(article_texts)} articles\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step4_summarize_articles(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 4: Summarize each article to 3 key bullet points\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No article texts to summarize. Complete steps 1-3 first.\"\n",
    "\n",
    "        summaries = {}\n",
    "\n",
    "        for url, text in wrapper.context.article_texts.items():\n",
    "            # Mock summarization (replace with actual LLM summarization)\n",
    "            summaries[url] = [\n",
    "                \"• Key development in AI technology or research\",\n",
    "                \"• Practical implications for businesses or developers\",\n",
    "                \"• Future outlook or next steps in this area\"\n",
    "            ]\n",
    "\n",
    "        wrapper.context.article_summaries = summaries\n",
    "        wrapper.context.current_step = 4\n",
    "\n",
    "        return f\"✅ Step 4 Complete: Generated 3-point summaries for {len(summaries)} articles\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step5_extract_and_cluster_topics(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        max_clusters: int = 8\n",
    "    ) -> str:\n",
    "        \"\"\"Step 5: Extract topics and cluster articles\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No articles to analyze. Complete steps 1-4 first.\"\n",
    "\n",
    "        # Extract topics from each article (mock implementation)\n",
    "        article_topics = {}\n",
    "        all_topics = []\n",
    "\n",
    "        for url, text in wrapper.context.article_texts.items():\n",
    "            # Mock topic extraction (replace with NLP)\n",
    "            topics = ['machine learning', 'business applications', 'research', 'ethics']\n",
    "            article_topics[url] = topics\n",
    "            all_topics.extend(topics)\n",
    "\n",
    "        # Cluster articles by common topics\n",
    "        topic_counts = Counter(all_topics)\n",
    "        main_topics = [topic for topic, count in topic_counts.most_common(max_clusters)]\n",
    "\n",
    "        topic_clusters = {}\n",
    "        for topic in main_topics:\n",
    "            topic_clusters[topic] = [\n",
    "                url for url, topics in article_topics.items()\n",
    "                if topic in topics\n",
    "            ]\n",
    "\n",
    "        wrapper.context.article_topics = article_topics\n",
    "        wrapper.context.topic_clusters = topic_clusters\n",
    "        wrapper.context.current_step = 5\n",
    "\n",
    "        return f\"✅ Step 5 Complete: Extracted topics and created {len(topic_clusters)} clusters\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step6_rate_articles(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        custom_rubric: Dict[str, str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Step 6: Rate articles according to rubric\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No articles to rate. Complete previous steps first.\"\n",
    "\n",
    "        if custom_rubric:\n",
    "            wrapper.context.rating_rubric.update(custom_rubric)\n",
    "\n",
    "        # Mock rating (replace with actual evaluation)\n",
    "        ratings = {}\n",
    "        for url in wrapper.context.article_texts.keys():\n",
    "            # Mock scoring based on rubric criteria\n",
    "            relevance_score = 0.8\n",
    "            novelty_score = 0.7\n",
    "            impact_score = 0.9\n",
    "            credibility_score = 0.8\n",
    "\n",
    "            overall_rating = (relevance_score + novelty_score + impact_score + credibility_score) / 4\n",
    "            ratings[url] = overall_rating\n",
    "\n",
    "        wrapper.context.article_ratings = ratings\n",
    "        wrapper.context.current_step = 6\n",
    "\n",
    "        avg_rating = sum(ratings.values()) / len(ratings)\n",
    "        return f\"✅ Step 6 Complete: Rated {len(ratings)} articles. Average rating: {avg_rating:.2f}\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step7_organize_sections(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        target_sections: int = 10\n",
    "    ) -> str:\n",
    "        \"\"\"Step 7: Organize articles into thematic sections\"\"\"\n",
    "        if not wrapper.context.topic_clusters:\n",
    "            return \"❌ No topic clusters available. Complete steps 1-6 first.\"\n",
    "\n",
    "        # Create thematic sections based on clusters and ratings\n",
    "        sections = {}\n",
    "\n",
    "        # Main thematic sections from top clusters\n",
    "        top_clusters = sorted(\n",
    "            wrapper.context.topic_clusters.items(),\n",
    "            key=lambda x: len(x[1]),  # Sort by cluster size\n",
    "            reverse=True\n",
    "        )[:target_sections-1]  # Reserve space for \"Other News\"\n",
    "\n",
    "        for topic, urls in top_clusters:\n",
    "            # Only include high-rated articles\n",
    "            high_rated_urls = [\n",
    "                url for url in urls\n",
    "                if wrapper.context.article_ratings.get(url, 0) >= 0.6\n",
    "            ]\n",
    "            if high_rated_urls:\n",
    "                section_name = topic.title().replace('_', ' ')\n",
    "                sections[section_name] = high_rated_urls\n",
    "\n",
    "        # \"Other News\" section for remaining articles\n",
    "        assigned_urls = set()\n",
    "        for urls in sections.values():\n",
    "            assigned_urls.update(urls)\n",
    "\n",
    "        other_urls = [\n",
    "            url for url in wrapper.context.article_texts.keys()\n",
    "            if url not in assigned_urls and wrapper.context.article_ratings.get(url, 0) >= 0.5\n",
    "        ]\n",
    "\n",
    "        if other_urls:\n",
    "            sections[\"Other News\"] = other_urls\n",
    "\n",
    "        wrapper.context.thematic_sections = sections\n",
    "        wrapper.context.section_names = list(sections.keys())\n",
    "        wrapper.context.current_step = 7\n",
    "\n",
    "        section_summary = \"\\n\".join([\n",
    "            f\"• {name}: {len(urls)} articles\"\n",
    "            for name, urls in sections.items()\n",
    "        ])\n",
    "\n",
    "        return f\"✅ Step 7 Complete: Organized into {len(sections)} sections:\\n{section_summary}\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step8_write_sections(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 8: Write content for each thematic section\"\"\"\n",
    "        if not wrapper.context.thematic_sections:\n",
    "            return \"❌ No sections to write. Complete steps 1-7 first.\"\n",
    "\n",
    "        section_drafts = {}\n",
    "\n",
    "        for section_name, urls in wrapper.context.thematic_sections.items():\n",
    "            # Gather content for this section\n",
    "            section_articles = []\n",
    "\n",
    "            for url in urls:\n",
    "                summary = wrapper.context.article_summaries.get(url, [])\n",
    "                rating = wrapper.context.article_ratings.get(url, 0)\n",
    "\n",
    "                # Get article title from DataFrame\n",
    "                article_row = wrapper.context.ai_headlines[\n",
    "                    wrapper.context.ai_headlines['url'] == url\n",
    "                ]\n",
    "                title = article_row['title'].iloc[0] if not article_row.empty else \"Unknown Title\"\n",
    "\n",
    "                section_articles.append({\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'summary': summary,\n",
    "                    'rating': rating\n",
    "                })\n",
    "\n",
    "            # Write section content (mock implementation)\n",
    "            section_content = f\"## {section_name}\\n\\n\"\n",
    "\n",
    "            for article in sorted(section_articles, key=lambda x: x['rating'], reverse=True):\n",
    "                section_content += f\"**{article['title']}**\\n\"\n",
    "                for bullet in article['summary']:\n",
    "                    section_content += f\"{bullet}\\n\"\n",
    "                section_content += f\"[Read more]({article['url']})\\n\\n\"\n",
    "\n",
    "            section_drafts[section_name] = section_content\n",
    "\n",
    "        wrapper.context.section_drafts = section_drafts\n",
    "        wrapper.context.current_step = 8\n",
    "\n",
    "        return f\"✅ Step 8 Complete: Wrote content for {len(section_drafts)} sections\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step9_finalize_newsletter(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        newsletter_title: str = \"AI Weekly Newsletter\"\n",
    "    ) -> str:\n",
    "        \"\"\"Step 9: Combine sections and polish final newsletter\"\"\"\n",
    "        if not wrapper.context.section_drafts:\n",
    "            return \"❌ No section drafts available. Complete steps 1-8 first.\"\n",
    "\n",
    "        # Combine all sections\n",
    "        newsletter_content = f\"# {newsletter_title}\\n\"\n",
    "        newsletter_content += f\"*Generated on {datetime.now().strftime('%B %d, %Y')}*\\n\\n\"\n",
    "\n",
    "        # Add introduction\n",
    "        total_articles = len(wrapper.context.article_texts)\n",
    "        newsletter_content += f\"This week's AI newsletter covers {total_articles} key developments across {len(wrapper.context.section_drafts)} areas of AI.\\n\\n\"\n",
    "\n",
    "        # Add each section\n",
    "        for section_name in wrapper.context.section_names:\n",
    "            if section_name in wrapper.context.section_drafts:\n",
    "                newsletter_content += wrapper.context.section_drafts[section_name]\n",
    "                newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "        # Add footer\n",
    "        newsletter_content += \"*Thank you for reading! This newsletter was generated using AI curation and analysis.*\"\n",
    "\n",
    "        wrapper.context.final_newsletter = newsletter_content\n",
    "        wrapper.context.workflow_complete = True\n",
    "        wrapper.context.current_step = 9\n",
    "\n",
    "        return f\"✅ Step 9 Complete: Finalized newsletter with {len(wrapper.context.section_drafts)} sections\"\n",
    "\n",
    "    @function_tool\n",
    "    async def get_workflow_status(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Get detailed workflow progress status\"\"\"\n",
    "        state = wrapper.context\n",
    "\n",
    "        status = {\n",
    "            'current_step': state.current_step,\n",
    "            'steps_completed': [\n",
    "                f\"1. Scraping: {len(state.raw_headlines)} headlines\" if state.raw_headlines else \"1. Scraping: Pending\",\n",
    "                f\"2. AI Filtering: {len(state.ai_headlines)} AI articles\" if not state.ai_headlines.empty else \"2. AI Filtering: Pending\",\n",
    "                f\"3. Text Fetching: {len(state.article_texts)} articles\" if state.article_texts else \"3. Text Fetching: Pending\",\n",
    "                f\"4. Summarization: {len(state.article_summaries)} summaries\" if state.article_summaries else \"4. Summarization: Pending\",\n",
    "                f\"5. Topic Clustering: {len(state.topic_clusters)} clusters\" if state.topic_clusters else \"5. Topic Clustering: Pending\",\n",
    "                f\"6. Article Rating: {len(state.article_ratings)} rated\" if state.article_ratings else \"6. Article Rating: Pending\",\n",
    "                f\"7. Section Organization: {len(state.thematic_sections)} sections\" if state.thematic_sections else \"7. Section Organization: Pending\",\n",
    "                f\"8. Section Writing: {len(state.section_drafts)} drafts\" if state.section_drafts else \"8. Section Writing: Pending\",\n",
    "                f\"9. Newsletter Finalization: {'Complete' if state.final_newsletter else 'Pending'}\"\n",
    "            ],\n",
    "            'workflow_complete': state.workflow_complete\n",
    "        }\n",
    "\n",
    "        return f\"Newsletter Workflow Status:\\n\\n\" + \"\\n\".join(status['steps_completed'])\n",
    "\n",
    "    @function_tool\n",
    "    async def run_complete_workflow(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        sources: List[str] = None,\n",
    "        ai_keywords: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Run the complete 9-step workflow automatically\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Execute each step in sequence\n",
    "        result1 = await self.step1_scrape_headlines(wrapper, sources)\n",
    "        results.append(result1)\n",
    "\n",
    "        result2 = await self.step2_filter_ai_content(wrapper, ai_keywords)\n",
    "        results.append(result2)\n",
    "\n",
    "        result3 = await self.step3_fetch_article_texts(wrapper)\n",
    "        results.append(result3)\n",
    "\n",
    "        result4 = await self.step4_summarize_articles(wrapper)\n",
    "        results.append(result4)\n",
    "\n",
    "        result5 = await self.step5_extract_and_cluster_topics(wrapper)\n",
    "        results.append(result5)\n",
    "\n",
    "        result6 = await self.step6_rate_articles(wrapper)\n",
    "        results.append(result6)\n",
    "\n",
    "        result7 = await self.step7_organize_sections(wrapper)\n",
    "        results.append(result7)\n",
    "\n",
    "        result8 = await self.step8_write_sections(wrapper)\n",
    "        results.append(result8)\n",
    "\n",
    "        result9 = await self.step9_finalize_newsletter(wrapper)\n",
    "        results.append(result9)\n",
    "\n",
    "        newsletter_length = len(wrapper.context.final_newsletter)\n",
    "\n",
    "        return \"\\n\".join(results) + f\"\\n\\n🎉 Complete workflow finished! Newsletter ready ({newsletter_length} characters)\"\n",
    "\n",
    "    @function_tool\n",
    "    async def reset_workflow(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Reset workflow to start fresh\"\"\"\n",
    "        wrapper.context.__dict__.update(NewsletterState().__dict__)\n",
    "        return \"🔄 Workflow reset. Ready to start step 1.\"\n",
    "\n",
    "    @function_tool\n",
    "    async def get_newsletter_preview(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        max_chars: int = 500\n",
    "    ) -> str:\n",
    "        \"\"\"Get a preview of the current newsletter\"\"\"\n",
    "        if not wrapper.context.final_newsletter:\n",
    "            return \"Newsletter not ready yet. Complete the full workflow first.\"\n",
    "\n",
    "        preview = wrapper.context.final_newsletter[:max_chars]\n",
    "        if len(wrapper.context.final_newsletter) > max_chars:\n",
    "            preview += \"...\"\n",
    "\n",
    "        return f\"Newsletter Preview:\\n\\n{preview}\"\n",
    "\n",
    "    async def run_step(self, user_input: str) -> str:\n",
    "        \"\"\"Run a workflow step with persistent state\"\"\"\n",
    "        result = await Runner.run(\n",
    "            self,\n",
    "            user_input,\n",
    "            session=self.session,\n",
    "            context=self.state\n",
    "        )\n",
    "        return result.final_output\n",
    "\n",
    "    def save_newsletter(self, filepath: str = None):\n",
    "        \"\"\"Save the final newsletter to file\"\"\"\n",
    "        if not self.state.final_newsletter:\n",
    "            print(\"No newsletter to save. Complete workflow first.\")\n",
    "            return\n",
    "\n",
    "        if filepath is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"ai_newsletter_{timestamp}.md\"\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(self.state.final_newsletter)\n",
    "\n",
    "        print(f\"Newsletter saved to {filepath}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"http://localhost:8787/v1\",\n",
    "  api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "  default_headers={\"x-portkey-provider\": \"openai\"}\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from portkey_ai import Portkey\n",
    "\n",
    "client = Portkey(\n",
    "    provider=\"openai\",\n",
    "    Authorization=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Example: Send a chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61793fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State of the LangGraph agent.\n",
    "    Each node in the graph is a function that takes the current state and returns the updated state.\n",
    "    \"\"\"\n",
    "\n",
    "    # the current working set of headlines (pandas dataframe not supported)\n",
    "    AIdf: list[dict]\n",
    "    # ignore stories before this date for deduplication (force reprocess since)\n",
    "    model_low: str     # cheap fast model like gpt-4o-mini or flash\n",
    "    model_medium: str  # medium model like gpt-4o or gemini-1.5-pro\n",
    "    model_high: str    # slow expensive thinking model like o3-mini\n",
    "    sources: dict  # sources to scrap\n",
    "    sources_reverse: dict[str, str]  # map file names to sources\n",
    "\n",
    "state = AgentState()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES_FILE = \"sources.yaml\"\n",
    "\n",
    "def initialize(state, sources_file=SOURCES_FILE) -> Dict[str, Any]:\n",
    "    \"\"\"Read and parse the sources.yaml file.\"\"\"\n",
    "    try:\n",
    "        with open(sources_file, 'r', encoding='utf-8') as file:\n",
    "            state[\"sources\"] =  yaml.safe_load(file)\n",
    "        state[\"sources_reverse\"] = {v[\"title\"]+\".html\":k for k,v in state[\"sources\"].items()}\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Sources file '{self.sources_file}' not found\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Error parsing YAML file: {e}\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4112b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = initialize(state)\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdk",
   "language": "python",
   "name": "asdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
