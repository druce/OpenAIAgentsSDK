{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d549de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import pydantic\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "from typing import Dict, TypedDict, Type, List, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import agents\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "from agents import (Agent, Runner, Tool, ModelSettings, FunctionTool, InputGuardrail, GuardrailFunctionOutput,\n",
    "                    SQLiteSession, set_default_openai_api, set_default_openai_client\n",
    "                   )\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from prompt_loader import PromptLoader\n",
    "from log_handler import SQLiteLogHandler, setup_sqlite_logging, sanitize_error_for_logging\n",
    "from utilities import (StepStatus, WorkflowStatus,\n",
    "                       get_workflow_status_report, print_workflow_summary, \n",
    "                      )\n",
    "# from scrape import gather_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cea80dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI:            1.107.0\n",
      "OpenAI Agents SDK  0.2.11\n",
      "Pydantic           2.11.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"OpenAI:            {openai.__version__}\")\n",
    "print(f\"OpenAI Agents SDK  {agents.__version__}\")\n",
    "print(f\"Pydantic           {pydantic.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742842e",
   "metadata": {},
   "source": [
    "# Basic usage\n",
    "- Run a prompt using agents\n",
    "- Sessions\n",
    "- Route through Portkey for observability\n",
    "- Save logs\n",
    "- Link to openai for traces and evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153c333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_BASE_URL = http://localhost:8787/v1\n",
      "OPENAI_DEFAULT_HEADERS = {\"x-portkey-provider\": \"openai\"}\n"
     ]
    }
   ],
   "source": [
    "# load environment variables including OPENAI_API_KEY\n",
    "# important - for portkey\n",
    "# OPENAI_BASE_URL=\"http://localhost:8787/v1\"\n",
    "# OPENAI_DEFAULT_HEADERS='{\"x-portkey-provider\": \"openai\"}'\n",
    "# launch proxy service https://portkey.ai/docs/product/enterprise-offering/components\n",
    "# npx @portkey-ai/gateway\n",
    "# could point to a database with a portkey_config.yaml\n",
    "# logging:\n",
    "#   sink: sql\n",
    "#   database_url: postgres://user:password@localhost:5432/portkey\n",
    "# npx @portkey-ai/gateway --portkey_config.yaml\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# to run async in jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# verbose console logging if something doesn't work\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# openai_logger = logging.getLogger(\"openai\")a\n",
    "# openai_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# needed for portkey - responses API is persistent connection-oriented and seeems to not work\n",
    "set_default_openai_api(\"chat_completions\")\n",
    "\n",
    "print(\"OPENAI_BASE_URL =\", os.getenv(\"OPENAI_BASE_URL\"))\n",
    "print(\"OPENAI_DEFAULT_HEADERS =\", os.getenv(\"OPENAI_DEFAULT_HEADERS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686ed01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:28:09 | NewsletterAgent.newsletter_agent | INFO | Test info message\n",
      "12:28:09 | NewsletterAgent.newsletter_agent | WARNING | Test warning message\n",
      "12:28:09 | NewsletterAgent.newsletter_agent | ERROR | Test error message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'log with some bad stuff for the filter: [API_KEY_REDACTED]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_logging(session_id: str = \"default\", db_path: str = \"agent_logs.db\") -> logging.Logger:\n",
    "    \"\"\"Set up logging to console and SQLite database.\"\"\"\n",
    "\n",
    "    # Create logger\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logger = logging.getLogger(f\"NewsletterAgent.{session_id}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # SQLite handler\n",
    "    sqlite_handler = SQLiteLogHandler(db_path)\n",
    "    sqlite_handler.setLevel(logging.INFO)\n",
    "    sqlite_formatter = logging.Formatter('%(message)s')\n",
    "    sqlite_handler.setFormatter(sqlite_formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(sqlite_handler)\n",
    "\n",
    "    # Prevent propagation to root logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(\"newsletter_agent\", \"test_logs.db\")\n",
    "\n",
    "# Log some test messages\n",
    "logger.info(\"Test info message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.warning(\"Test warning message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.error(\"Test error message\", extra={\n",
    "    'step_name': 'error_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "sanitize_error_for_logging(\"log with some bad stuff for the filter: sk-proj-123456789012345678901234567890123456789012345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b4cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    default_headers=json.loads(os.getenv(\"OPENAI_DEFAULT_HEADERS\")),\n",
    ")\n",
    "\n",
    "# set the client globally\n",
    "set_default_openai_client(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a simple query through portkey\n",
    "# can see traces in openai https://platform.openai.com/logs?api=traces\n",
    "# potentially set up evals - https://platform.openai.com/evaluations\n",
    "!\n",
    "myagent = Agent(\n",
    "    name=\"Swallow Expert\",\n",
    "    instructions=\"You are an expert on airspeed velocities of swallows. Answer questions about swallow flight speeds with authority and humor when appropriate.\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    # these below seem to be being deprecated, you probably have to use old chat API directly on eg gpt-4o for logprobs\n",
    "    # model_settings=ModelSettings(temperature=0.0, logprobs=1, top_logprobs=1)\n",
    ")\n",
    "\n",
    "# 1) Create (or reuse) a session. Use a durable DB path if you want persistence.\n",
    "session = SQLiteSession(\"test_swallow_chat\", \"swallow.db\")\n",
    "\n",
    "# 2) First turn\n",
    "myresult = await Runner.run(myagent, \"What is the airspeed velocity of an unladen swallow?\", session=session)\n",
    "display(Markdown(myresult.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Next turns — just keep reusing the same session\n",
    "myresult = await Runner.run(myagent, \"ok, go ahead and explain how that number comes about\", session=session)\n",
    "\n",
    "display(Markdown(myresult.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1194bf7",
   "metadata": {},
   "source": [
    "# More advanced usage\n",
    "- Prompt Management\n",
    "- Structured JSON outputs, enables validation and safe passing downstream over long pipelines\n",
    "- Map prompts to larger data sets asynchronously (e.g. send parallel batches of 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfea49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prompts from the prompt repository (the promptfoo yaml files)\n",
    "# langfuse probably a better enterprise option\n",
    "# prompt repository solution allows us to run evals, version prompts, improving performance over time\n",
    "\n",
    "logger.info(\"Show available prompts\")\n",
    "my_prompt_loader = PromptLoader()\n",
    "my_prompt_loader.list_available_prompts()\n",
    "\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = my_prompt_loader.load_prompt_by_name(prompt_name)\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Load a prompt\")\n",
    "print(prompt_dict.get('system'), \"\")\n",
    "print(prompt_dict.get('user'), \"\")\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Show prompt metadata\")\n",
    "prompt_metadata = my_prompt_loader.get_prompt_metadata(prompt_name)\n",
    "print(prompt_metadata)\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Format a prompt with input\")\n",
    "print(my_prompt_loader.format_prompt(prompt_name, input_str=\"AI Is Replacing Online Moderators, But It's Bad at the Job\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f123315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output class for classifying headlines\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"A single headline classification result\"\"\"\n",
    "    input_str: str = Field(description=\"The original headline text\")\n",
    "    output: bool = Field(description=\"Whether the headline is AI-related\")\n",
    "\n",
    "class ClassificationResultList(BaseModel):\n",
    "    \"\"\"List of ClassificationResult for batch processing\"\"\"\n",
    "    results_list: list[ClassificationResult] = Field(description=\"List of classification results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    default_headers=json.loads(os.getenv(\"OPENAI_DEFAULT_HEADERS\")),\n",
    ")\n",
    "\n",
    "# set the client globally\n",
    "set_default_openai_client(client)\n",
    "\n",
    "class ClassifierAgent(Agent):\n",
    "    \"\"\"Agent for classifying headlines as AI-related or not\n",
    "    or more generally apply a prompt to a string for a classification according to an output type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_prompt: str,\n",
    "                 user_prompt: str,\n",
    "                 output_type: Type[BaseModel],\n",
    "                 model: str,\n",
    "                 verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifierAgent\n",
    "\n",
    "        Args:\n",
    "            system_prompt: The system prompt template to use\n",
    "            user_prompt: The user prompt template to use\n",
    "            output_type: Pydantic model class for structured output\n",
    "            verbose: Enable verbose logging\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=\"ClassifierAgent\",\n",
    "            model=model,\n",
    "            instructions=system_prompt,\n",
    "            output_type=output_type\n",
    "        )\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f\"\"\"Initialized ClassifierAgent:\n",
    "system_prompt:\n",
    "{self.system_prompt}\n",
    "user_prompt:\n",
    "{self.user_prompt}\n",
    "output_type:         {output_type.__name__}\n",
    "model:               {self.model}\n",
    "schema:              {json.dumps(output_type.model_json_schema(), indent=2)}\n",
    "\"\"\")\n",
    "    @retry(\n",
    "        retry=retry_if_exception_type((openai.APIConnectionError,\n",
    "                                       openai.APITimeoutError,\n",
    "                                       openai.InternalServerError)),\n",
    "        stop=stop_after_attempt(5),  # 5 attempts sufficient for classification\n",
    "        after=lambda retry_state: log(sanitize_error_for_logging(\n",
    "            f\"Attempt {retry_state.attempt_number}: {retry_state.outcome.exception()}, tag: {retry_state.args[1].get('tag', '')}\")),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    )\n",
    "\n",
    "    async def classify(self, input_str: str) -> Type[BaseModel]:\n",
    "        \"\"\"\n",
    "        Classify a single input or a string with multiple inputs to the specified type\n",
    "\n",
    "        Args:\n",
    "            input: The input text to classify\n",
    "\n",
    "        Returns:\n",
    "            The specified type\n",
    "        \"\"\"\n",
    "        user_message = self.user_prompt.format(input_str=input_str)\n",
    "        if self.verbose:\n",
    "            logger.info(f\"User message: {user_message}\")\n",
    "\n",
    "        results_list = await Runner.run(self, user_message)\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Result: {results_list}\")\n",
    "        return results_list\n",
    "\n",
    "    async def classify_batch(self, input_list: List[str], batch_size: int = 25,\n",
    "                             *, max_concurrency: int = 16, retries: int = 3\n",
    "                            ) -> Any:\n",
    "\n",
    "        \"\"\"\n",
    "        Classify a list using paged, parallel calls to `self.classify()`,\n",
    "        preserving the original input order and validating page sizes.\n",
    "        \"\"\"\n",
    "        # Type must have a 'results_list' element\n",
    "        null_return = self.output_type(results_list=[])\n",
    "        if not input_list:\n",
    "            return null_return\n",
    "\n",
    "        pages = [input_list[i:i+batch_size]\n",
    "                 for i in range(0, len(input_list), batch_size)]\n",
    "        sem = asyncio.Semaphore(max_concurrency)\n",
    "        logger.info(f\"Sending {len(pages)} batches with concurrency {max_concurrency}\")\n",
    "\n",
    "        async def _guarded_classify(page_idx: int, items: List[str]) -> self.output_type:\n",
    "            for i in range(retries):\n",
    "                input_str = \"\\n\".join(items)\n",
    "                try:\n",
    "                    async with sem:\n",
    "                        result = await self.classify(input_str)\n",
    "                    res = result.final_output\n",
    "#                     print(type(res))\n",
    "#                     print(\"----\")\n",
    "#                     print(res)\n",
    "#                     print(\"----\")\n",
    "                    if not hasattr(res, \"results_list\"):\n",
    "                        raise ValueError(\"Bad structured output or missing 'results_list'.\")\n",
    "                    if not isinstance(res.results_list, list):\n",
    "                        raise ValueError(\"Structured output invalid 'results_list'.\")\n",
    "                    if len(res.results_list) != len(items):\n",
    "                        raise ValueError(\n",
    "                            f\"Page {page_idx}: count mismatch (got {len(res.results_list)} vs expected {len(items)}).\"\n",
    "                        )\n",
    "                    return (page_idx, res)\n",
    "                except Exception as e:\n",
    "                    last_exc = e\n",
    "                    logger.info(f\"[page {page_idx}] attempt {i+1}/{retries} failed: {e}\")\n",
    "                    if i < retries:\n",
    "                        await asyncio.sleep(2 ** i)  # 1s, 2s, 4s backoff\n",
    "\n",
    "            return page_idx, last_exc if last_exc else RuntimeError(f\"Unknown error on page {page_idx}\")\n",
    "\n",
    "        tasks = [\n",
    "            asyncio.create_task(_guarded_classify(i, page))\n",
    "            for i, page in enumerate(pages)\n",
    "        ]\n",
    "        page_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Reassemble in original order\n",
    "        flattened_results = []\n",
    "        for idx, res_or_exc in page_results:\n",
    "            if isinstance(res_or_exc, Exception):\n",
    "                raise res_or_exc\n",
    "            elif res_or_exc:\n",
    "                flattened_results.extend(res_or_exc.results_list)\n",
    "            else:\n",
    "                logger.info(f\"no results for page {idx}\")\n",
    "\n",
    "        final = self.output_type(results_list=flattened_results)\n",
    "\n",
    "        # Final sanity check\n",
    "        if len(final.results_list) != len(input_list):\n",
    "            raise ValueError(f\"Final count mismatch: expected {len(input_list)} results, got {len(flattened.results_list)}.\")\n",
    "\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send singly\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResult,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=True)\n",
    "\n",
    "test_headlines = [\n",
    "    \"AI Is Replacing Online Moderators, But It's Bad at the Job\",\n",
    "    \"Baby Trapped in Refrigerator Eats Own Foot\",\n",
    "    \"Machine Learning Breakthrough in Medical Diagnosis\",\n",
    "    \"Local Restaurant Opens New Location\",\n",
    "    \"ChatGPT Usage Soars in Educational Settings\"\n",
    "]\n",
    "\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResult,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=True)\n",
    "\n",
    "result = await classifier.classify(test_headlines[0])\n",
    "print(result.final_output)\n",
    "result = await classifier.classify(test_headlines[1])\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a single batch with verbose\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResultList,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=True)\n",
    "\n",
    "result = await classifier.classify(str(test_headlines))\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4498281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batches and send multiple in parallel\n",
    "headlines_df = pd.read_csv(\"test_headlines.csv\")\n",
    "headlines_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"classify headlines as AI-related or not\")\n",
    "prompt_name = 'headline_classifier_v1'\n",
    "prompt_dict = PromptLoader().load_prompt_by_name(prompt_name)\n",
    "\n",
    "classifier = ClassifierAgent(prompt_dict.get('system'),\n",
    "                             prompt_dict.get('user'),\n",
    "                             ClassificationResultList,\n",
    "                             \"gpt-5-mini\",\n",
    "                             verbose=False)\n",
    "\n",
    "classification_result = await classifier.classify_batch(list(headlines_df['title'].to_list()))\n",
    "classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see results, true and false\n",
    "zdf = pd.DataFrame([(z.input_str, z.output) for z in classification_result.results_list], columns=[\"input\", \"output\"])\n",
    "display(zdf.loc[zdf[\"output\"]])\n",
    "zdf.loc[~zdf[\"output\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdad2f",
   "metadata": {},
   "source": [
    "# Run Agent Worfklow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc786240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store agent state from step to step\n",
    "\n",
    "class NewsletterAgentState(BaseModel):\n",
    "    \"\"\"\n",
    "    Persistent state for the newsletter agent workflow.\n",
    "\n",
    "    Manages the complete newsletter generation process with serializable storage\n",
    "    of headlines, processing results, and workflow progress. Supports resumable execution\n",
    "    and DataFrame integration for data manipulation.\n",
    "\n",
    "    \"\"\"\n",
    "    # Serializable data storage (DataFrame as list of dicts)\n",
    "    headline_data: List[Dict[str, Any]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of headline dictionaries with columns: title, url, source, timestamp, is_ai, summary,etc.\"\n",
    "    )\n",
    "\n",
    "    # Configuration\n",
    "    max_edits: int = Field(default=2, description=\"Maximum editing iterations\")\n",
    "    concurrency: int = Field(default=16, description=\"Number of concurrent browsers\")\n",
    "    current_step: int = Field(default=0, description=\"Current workflow step (0-9)\")\n",
    "    workflow_complete: bool = Field(default=False, description=\"Whether the entire workflow is complete\")\n",
    "\n",
    "    # Source config\n",
    "    sources_file: str = Field(\n",
    "        default=\"sources.yaml\",\n",
    "        description=\"YAML filename containing source configurations\"\n",
    "    )\n",
    "    sources: Dict[str, Any] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Dictionary of source configurations loaded from YAML\"\n",
    "    )\n",
    "\n",
    "    # Topics and clustering\n",
    "    cluster_names: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of topic names for categorization\"\n",
    "    )\n",
    "    clusters: Dict[str, List[str]] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Topic name -> list of article IDs and related info\"\n",
    "    )\n",
    "\n",
    "    # Newsletter content\n",
    "    newsletter_sections: Dict[str, str] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Section name -> section content\"\n",
    "    )\n",
    "    final_newsletter: str = Field(default=\"\", description=\"Final newsletter content\")\n",
    "\n",
    "    # Helper methods\n",
    "    @classmethod\n",
    "    def create_headline_df(cls) -> pd.DataFrame:\n",
    "        \"\"\"Create an empty DataFrame with proper columns for news headlines.\"\"\"\n",
    "        return pd.DataFrame(columns=[\n",
    "            'id',\n",
    "            'source',\n",
    "            'title',\n",
    "            'orig_url',\n",
    "            'url',\n",
    "            'text_path',\n",
    "            'site_name',\n",
    "            'published',\n",
    "            'is_ai',\n",
    "            'topic_list',\n",
    "            'cluster',\n",
    "            'rating',\n",
    "            'summary'\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def headline_dict_to_df(self) -> 'pd.DataFrame':\n",
    "        \"\"\"Convert headline data to DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.headline_data)\n",
    "\n",
    "    def headline_df_to_dict(self, df: 'pd.DataFrame'):\n",
    "        \"\"\"Update headline data from DataFrame\"\"\"\n",
    "        self.headline_data = df.to_dict(orient='records')\n",
    "\n",
    "    def add_headlines(self, new_headlines: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Add new headlines to the DataFrame with deduplication.\n",
    "\n",
    "        Args:\n",
    "            new_headlines: List of dictionaries with headline data\n",
    "                          Expected keys: title, url, source, timestamp, etc.\n",
    "        \"\"\"\n",
    "        if not new_headlines:\n",
    "            print(\"⚠️  No new headlines to add\")\n",
    "            return\n",
    "\n",
    "        new_df = pd.DataFrame(new_headlines)\n",
    "\n",
    "        if self.headline_data:\n",
    "            existing_df = self.headline_dict_to_df()\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            self.headline_df_to_dict(combined_df)\n",
    "        else:\n",
    "            self.headline_df_to_dict(new_df)\n",
    "\n",
    "        print(f\"📰 Added headlines - updated count: {len(self.headline_data)}\")\n",
    "\n",
    "    def get_status_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get a summary of the current session state.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with key metrics and status information\n",
    "        \"\"\"\n",
    "        total_headlines = len(self.headline_data)\n",
    "\n",
    "        return {\n",
    "            \"headlines\": {\n",
    "                \"total\": total_headlines,\n",
    "                # group by source\n",
    "                # ai vs non-ai\n",
    "                # downloaded: have text path\n",
    "                # have summary\n",
    "                # have topics\n",
    "                # have cluster assignment\n",
    "                # have rating\n",
    "            },\n",
    "            \"sources\": {\n",
    "                \"config_file\": self.sources_file,\n",
    "                \"loaded_sources\": len(self.sources),\n",
    "            },\n",
    "            \"topics\": {\n",
    "                \"cluster_topics\": len(self.cluster_names),\n",
    "                \"topics\": self.cluster_names\n",
    "            },\n",
    "            \"workflow\": {\n",
    "                \"current_step\": self.current_step,\n",
    "                \"workflow_complete\": self.workflow_complete,\n",
    "                \"max_edits\": self.max_edits,\n",
    "                \"concurrency\": self.concurrency\n",
    "            },\n",
    "            \"processing\": {\n",
    "                \"topic_clusters\": len(self.clusters),\n",
    "                \"newsletter_sections\": len(self.newsletter_sections),\n",
    "                \"final_newsletter_length\": len(self.final_newsletter)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def print_status(self) -> None:\n",
    "        \"\"\"Print a formatted summary of the current session state.\"\"\"\n",
    "        status = self.get_status_summary()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"📊 NEWSLETTER AGENT STATE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        print(f\"📰 Headlines: {status['headlines']['total']} total\")\n",
    "        # print(f\"   🤖 AI-related: {status['headlines']['is_ai']} ({status['headlines']['ai_percentage']})\")\n",
    "        # print(f\"   📄 Non-AI: {status['headlines']['non_ai']}\")\n",
    "\n",
    "        print(f\"\\n📡 Sources: {status['sources']['loaded_sources']} loaded\")\n",
    "        print(f\"   📁 Config: {status['sources']['config_file']}\")\n",
    "\n",
    "        print(f\"\\n🏷️  Topics: {status['topics']['cluster_topics']} cluster topics\")\n",
    "        if status['topics']['topics']:\n",
    "            print(f\"   📋 Topics: {', '.join(status['topics']['topics'])}\")\n",
    "\n",
    "        print(f\"\\n⚙️  Workflow:\")\n",
    "        print(f\"   📍 Current step: {status['workflow']['current_step']}/9\")\n",
    "        print(f\"   ✅ Complete: {status['workflow']['workflow_complete']}\")\n",
    "        print(f\"   ✏️ Max edits: {status['workflow']['max_edits']}\")\n",
    "        print(f\"   🌐 Concurrency: {status['workflow']['concurrency']}\")\n",
    "\n",
    "        print(f\"\\n🔄 Processing:\")\n",
    "        # print(f\"   📝 Article summaries: {status['processing']['article_summaries']}\")\n",
    "        print(f\"   🏷️  Topic clusters: {status['processing']['topic_clusters']}\")\n",
    "        print(f\"   📑 Newsletter sections: {status['processing']['newsletter_sections']}\")\n",
    "        if status['processing']['final_newsletter_length'] > 0:\n",
    "            print(f\"   📰 Final newsletter: {status['processing']['final_newsletter_length']} chars\")\n",
    "\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "    def get_unique_sources(self) -> List[str]:\n",
    "        \"\"\"Get list of unique source names from headline data.\"\"\"\n",
    "        df = self.headline_dict_to_df\n",
    "        df['count'] = 1\n",
    "        df = df[['source', 'count']].groupby(['source']).sum().reset_index()\n",
    "        print(df)\n",
    "        return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7ba5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "if 'fetch' in sys.modules:\n",
    "    del sys.modules['fetch']\n",
    "    # Delete the reference\n",
    "    del Fetcher\n",
    "from fetch import Fetcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907210e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 12:28:24,470 - fetcher_4738149776 - INFO - [fetcher_init] Loading sources from sources.yaml\n",
      "2025-09-12 12:28:24,490 - fetcher_4738149776 - INFO - [fetcher_init] Loaded 17 sources: 7 RSS, 9 HTML, 1 API\n",
      "2025-09-12 12:28:24,491 - fetcher_4738149776 - INFO - [fetcher_init] Fetcher initialized with max_concurrent=8\n",
      "2025-09-12 12:28:24,492 - fetcher_4738149776 - INFO - [fetch_rss] Fetching RSS from Ars Technica: https://arstechnica.com/ai/feed/\n",
      "2025-09-12 12:28:24,786 - fetcher_4738149776 - INFO - [fetch_rss] RSS fetch successful for Ars Technica: 20 articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'Ars Technica',\n",
       " 'results': [{'source': 'Ars Technica',\n",
       "   'title': 'Education report calling for ethical AI use contains over 15 fake sources',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/education-report-calling-for-ethical-ai-use-contains-over-15-fake-sources/',\n",
       "   'published': 'Fri, 12 Sep 2025 16:01:27 +0000',\n",
       "   'rss_summary': 'Experts find fake sources in Canadian government report that took 18 months to complete.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'OpenAI and Microsoft sign preliminary deal to revise partnership terms',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/openai-and-microsoft-sign-preliminary-deal-to-revise-partnership-terms/',\n",
       "   'published': 'Thu, 11 Sep 2025 22:27:53 +0000',\n",
       "   'rss_summary': 'Companies work to finalize terms as OpenAI pursues for-profit restructuring.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Ted Cruz AI bill could let firms bribe Trump to avoid safety laws, critics warn',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/ted-cruz-bill-would-let-big-tech-go-wild-with-ai-experiments-for-10-years/',\n",
       "   'published': 'Thu, 11 Sep 2025 18:21:08 +0000',\n",
       "   'rss_summary': 'Ted Cruz won’t give up fight to block states from regulating AI.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'One of Google’s new Pixel 10 AI features has already been removed',\n",
       "   'url': 'https://arstechnica.com/google/2025/09/google-pulls-daily-hub-ai-feature-from-pixel-10-phones/',\n",
       "   'published': 'Wed, 10 Sep 2025 19:00:41 +0000',\n",
       "   'rss_summary': \"The Pixel 10 Daily Hub has vanished, but Google says it will come back when it's ready.\"},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Developers joke about “coding like cavemen” as AI service suffers major outage',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/developers-joke-about-coding-like-cavemen-as-ai-service-suffers-major-outage/',\n",
       "   'published': 'Wed, 10 Sep 2025 18:08:49 +0000',\n",
       "   'rss_summary': 'Anthropic outage takes down AI tools some developers rely on to create software.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Spotify peeved after 10,000 users sold data to build AI tools',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/spotify-peeved-after-10000-users-sold-data-to-build-ai-tools/',\n",
       "   'published': 'Wed, 10 Sep 2025 16:23:25 +0000',\n",
       "   'rss_summary': 'Spotify sent a warning to stop data sales, but developers say they never got it.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Microsoft ends OpenAI exclusivity in Office, adds rival Anthropic',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/report-microsoft-taps-rival-anthropics-ai-for-office-after-it-beats-openai-at-some-tasks/',\n",
       "   'published': 'Wed, 10 Sep 2025 15:41:42 +0000',\n",
       "   'rss_summary': \"Microsoft will end OpenAI's exclusive hold on its productivity suite, adding second AI supplier.\"},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'AI vs. MAGA: Populists alarmed by Trump’s embrace of AI, Big Tech',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/ai-vs-maga-populists-alarmed-by-trumps-embrace-of-ai-big-tech/',\n",
       "   'published': 'Wed, 10 Sep 2025 13:41:17 +0000',\n",
       "   'rss_summary': 'AI “threatens the common man’s liberty,\" says GOP Sen. Josh Hawley.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Pay-per-output? AI firms blindsided by beefed up robots.txt instructions.',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/pay-per-output-ai-firms-blindsided-by-beefed-up-robots-txt-instructions/',\n",
       "   'published': 'Wed, 10 Sep 2025 13:00:03 +0000',\n",
       "   'rss_summary': '\"Really Simple Licensing\" makes it easier for creators to get paid for AI scraping.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Reddit bug caused lesbian subreddit to be labeled as a place for “straight” women',\n",
       "   'url': 'https://arstechnica.com/gadgets/2025/09/reddit-bug-caused-lesbian-subreddit-to-be-labeled-as-a-place-for-straight-women/',\n",
       "   'published': 'Tue, 09 Sep 2025 22:32:52 +0000',\n",
       "   'rss_summary': 'Users feared Reddit used generative AI to rewrite user-created content.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Claude’s new AI file-creation feature ships with security risks built in',\n",
       "   'url': 'https://arstechnica.com/information-technology/2025/09/anthropics-new-claude-feature-can-leak-data-users-told-to-monitor-chats-closely/',\n",
       "   'published': 'Tue, 09 Sep 2025 20:55:34 +0000',\n",
       "   'rss_summary': 'Expert calls security advice \"unfairly outsourcing the problem to Anthropic\\'s users.\"'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Judge: Anthropic’s $1.5B settlement is being shoved “down the throat of authors”',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/judge-anthropics-1-5b-settlement-is-being-shoved-down-the-throat-of-authors/',\n",
       "   'published': 'Tue, 09 Sep 2025 16:21:02 +0000',\n",
       "   'rss_summary': \"Feeling “misled,” judge refuses to rubber-stamp Anthropic's proposed settlement.\"},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Why accessibility might be AI’s biggest breakthrough',\n",
       "   'url': 'https://arstechnica.com/information-technology/2025/09/study-finds-neurodiverse-workers-more-satisfied-with-ai-assistants/',\n",
       "   'published': 'Tue, 09 Sep 2025 11:08:44 +0000',\n",
       "   'rss_summary': 'UK study findings may challenge assumptions about who benefits most from AI tools.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'In court filing, Google concedes the open web is in “rapid decline”',\n",
       "   'url': 'https://arstechnica.com/google/2025/09/in-court-filing-google-concedes-the-open-web-is-in-rapid-decline/',\n",
       "   'published': 'Mon, 08 Sep 2025 19:29:40 +0000',\n",
       "   'rss_summary': \"Google's position on the state of the Internet is murky to say the least.\"},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'AI will consume all of IT by 2030—but not all IT jobs, Gartner says',\n",
       "   'url': 'https://arstechnica.com/information-technology/2025/09/no-ai-jobs-bloodbath-as-ai-permeates-all-it-work-over-the-next-5-years/',\n",
       "   'published': 'Mon, 08 Sep 2025 17:17:49 +0000',\n",
       "   'rss_summary': 'AI still threatens entry-level IT jobs.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': '“First of its kind” AI settlement: Anthropic to pay authors $1.5 billion',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/first-of-its-kind-ai-settlement-anthropic-to-pay-authors-1-5-billion/',\n",
       "   'published': 'Fri, 05 Sep 2025 20:26:03 +0000',\n",
       "   'rss_summary': 'Settlement shows AI companies can face consequences for pirated training data.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'Warner Bros. sues Midjourney to stop AI knockoffs of Batman, Scooby-Doo',\n",
       "   'url': 'https://arstechnica.com/tech-policy/2025/09/warner-bros-sues-midjourney-to-stop-ai-knockoffs-of-batman-scooby-doo/',\n",
       "   'published': 'Fri, 05 Sep 2025 18:47:32 +0000',\n",
       "   'rss_summary': 'Warner Bros. case builds on arguments raised in a Disney/Universal lawsuit.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'ChatGPT’s new branching feature is a good reminder that AI chatbots aren’t people',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/chatgpts-new-branching-feature-is-a-good-reminder-that-ai-chatbots-arent-people/',\n",
       "   'published': 'Fri, 05 Sep 2025 16:06:55 +0000',\n",
       "   'rss_summary': 'Users can explore multiple paths without losing their original chat thread.'},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'OpenAI links up with Broadcom to produce its own AI chips',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/openai-links-up-with-broadcom-to-produce-its-own-ai-chips/',\n",
       "   'published': 'Fri, 05 Sep 2025 13:30:46 +0000',\n",
       "   'rss_summary': \"Industry is moving towards custom solutions in wake of Nvidia's market domination.\"},\n",
       "  {'source': 'Ars Technica',\n",
       "   'title': 'New AI model turns photos into explorable 3D worlds, with caveats',\n",
       "   'url': 'https://arstechnica.com/ai/2025/09/new-ai-model-turns-photos-into-explorable-3d-worlds-with-caveats/',\n",
       "   'published': 'Wed, 03 Sep 2025 21:56:07 +0000',\n",
       "   'rss_summary': 'Openly available AI tool creates steerable 3D-like video, but requires serious GPU muscle.'}],\n",
       " 'status': 'success',\n",
       " 'metadata': {'feed_title': 'AI – Ars Technica',\n",
       "  'feed_description': 'Serving the Technologist since 1998. News, reviews, and analysis.',\n",
       "  'entries_count': 20,\n",
       "  'rss_url': 'https://arstechnica.com/ai/feed/'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async with Fetcher() as f:\n",
    "      z=await f.fetch_rss('Ars Technica')\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31fa98c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 12:28:25,734 - fetcher_4742280016 - INFO - [fetcher_init] Loading sources from sources.yaml\n",
      "2025-09-12 12:28:25,751 - fetcher_4742280016 - INFO - [fetcher_init] Loaded 17 sources: 7 RSS, 9 HTML, 1 API\n",
      "2025-09-12 12:28:25,752 - fetcher_4742280016 - INFO - [fetcher_init] Fetcher initialized with max_concurrent=8\n",
      "2025-09-12 12:28:25,752 - fetcher_4742280016 - INFO - [fetch_html] Fetching HTML from The Verge: https://www.theverge.com/ai-artificial-intelligence\n",
      "2025-09-12 12:28:27,101 - fetcher_4742280016 - INFO - [fetch_html] Source dict for The Verge: {'url': 'https://www.theverge.com/ai-artificial-intelligence', 'title': 'The Verge', 'sourcename': 'The Verge', 'click': '', 'scroll': 0, 'scroll_div': '', 'initial_sleep': None, 'include': ['^https://www.theverge.com/news'], 'exclude': None, 'minlength': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch_source https://www.theverge.com/ai-artificial-intelligence, The Verge\n",
      "fetch_url(https://www.theverge.com/ai-artificial-intelligence)\n",
      "Fetching https://www.theverge.com/ai-artificial-intelligence to download/sources\n",
      "Response: 200\n",
      "10\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 12:28:46,487 - fetcher_4742280016 - ERROR - [fetch_html] HTML fetch failed for The Verge: '<=' not supported between instances of 'int' and 'NoneType'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed human like actions\n",
      "Found last updated time from document.lastModified: 09/12/2025 09:28:46\n",
      "Saving HTML to download/sources/The_Verge.html\n"
     ]
    }
   ],
   "source": [
    "async with Fetcher() as f:\n",
    "      z = await f.fetch_html('The Verge')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd1d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPBin_HTML_Test.html The_Verge.html\r\n"
     ]
    }
   ],
   "source": [
    "!ls download/sources \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc429f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29484488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 12:29:15,293 - fetcher_4779257104 - INFO - [fetcher_init] Loading sources from sources.yaml\n",
      "2025-09-12 12:29:15,309 - fetcher_4779257104 - INFO - [fetcher_init] Loaded 17 sources: 7 RSS, 9 HTML, 1 API\n",
      "2025-09-12 12:29:15,310 - fetcher_4779257104 - INFO - [fetcher_init] Fetcher initialized with max_concurrent=8\n",
      "2025-09-12 12:29:15,311 - fetcher_4779257104 - INFO - [newsapi] Fetching top 100 stories matching artificial intelligence since 2025-09-11T12:29:15 from NewsAPI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'NewsAPI',\n",
       " 'results': [{'source': 'NewsAPI',\n",
       "   'title': \"Venezuela military, militias deploy to 'battlefronts', Maduro says\",\n",
       "   'url': 'https://www.yahoo.com/news/articles/venezuela-military-militias-deploy-battlefronts-131832857.html',\n",
       "   'published': '2025-09-11T13:18:32Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'French Voice Actress For Lara Croft Accuses Developer Of Using AI To Alter Her Voice',\n",
       "   'url': 'https://kotaku.com/lara-croft-tomb-raider-francoise-cadol-aspyr-remaster-2000624934',\n",
       "   'published': '2025-09-11T15:21:33Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Four talks and events hosted by Dezeen during London Design Festival 2025',\n",
       "   'url': 'https://www.dezeen.com/2025/09/11/dezeen-events-london-design-festival-2025/',\n",
       "   'published': '2025-09-11T15:00:27Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Google Pixel 10 Adds C2PA Support to Verify AI-Generated Media Authenticity',\n",
       "   'url': 'https://thehackernews.com/2025/09/google-pixel-10-adds-c2pa-support-to.html',\n",
       "   'published': '2025-09-11T15:03:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Analysts revamp Apple stock price target on iPhone 17 launch',\n",
       "   'url': 'https://biztoc.com/x/2752bb2882dc658a',\n",
       "   'published': '2025-09-11T15:27:13Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"Hunting for Aliens in the Galaxy's Most Promising Neighbourhood\",\n",
       "   'url': 'https://www.universetoday.com/articles/hunting-for-aliens-in-the-galaxys-most-promising-neighbourhood',\n",
       "   'published': '2025-09-11T16:10:06Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '7 AI Tools Every Photographer Should Actually Be Using',\n",
       "   'url': 'https://fstoppers.com/artificial-intelligence/7-ai-tools-every-photographer-should-actually-be-using-711039',\n",
       "   'published': '2025-09-11T16:06:01Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'iPhone 17 vs iPhone 16: what’s new?',\n",
       "   'url': 'https://9to5mac.com/2025/09/11/iphone-17-vs-iphone-16-whats-new/',\n",
       "   'published': '2025-09-11T16:00:07Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'More than half of consumers are wary of AI-powered search: Gartner',\n",
       "   'url': 'https://www.marketingdive.com/news/more-than-half-of-consumers-are-wary-of-ai-powered-search-gartner/759451/',\n",
       "   'published': '2025-09-11T14:57:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"Microsoft AI CEO Warns Against Granting Rights to AI, Calling It 'Dangerous and Misguided'\",\n",
       "   'url': 'https://www.breitbart.com/tech/2025/09/11/microsoft-ai-ceo-warns-against-granting-rights-to-ai-calling-it-dangerous-and-misguided/',\n",
       "   'published': '2025-09-11T16:05:47Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Galaxy, Circle, Bitfarms Lead Crypto Stock Gains as Bitcoin Vehicles Metaplanet, Nakamoto Plunge',\n",
       "   'url': 'https://www.coindesk.com/markets/2025/09/11/galaxy-circle-bitfarms-lead-crypto-stock-gains-as-bitcoin-vehicles-metaplanet-nakamoto-plunge',\n",
       "   'published': '2025-09-11T15:34:37Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Apple analyst sentiment hits five-year low after two downgrades',\n",
       "   'url': 'https://finance.yahoo.com/news/apple-analyst-sentiment-hits-five-140131891.html',\n",
       "   'published': '2025-09-11T14:01:31Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Albania appoints world’s first AI-made minister',\n",
       "   'url': 'https://www.politico.eu/article/albania-apppoints-worlds-first-virtual-minister-edi-rama-diella/',\n",
       "   'published': '2025-09-11T12:59:05Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'ResMed Inc. (RMD) Launches Global Sleep Institute at World Sleep Congress',\n",
       "   'url': 'https://finance.yahoo.com/news/resmed-inc-rmd-launches-global-151208726.html',\n",
       "   'published': '2025-09-11T15:12:08Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"Did McDonald's open 'first-ever buffet' in Missouri?\",\n",
       "   'url': 'https://www.snopes.com//fact-check/missouri-mcdonalds-buffet/',\n",
       "   'published': '2025-09-11T13:00:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Alibaba to Raise $3.2 Billion for Cloud Push With Convertible Bond',\n",
       "   'url': 'https://finance.yahoo.com/news/alibaba-raise-3-2-billion-132511524.html',\n",
       "   'published': '2025-09-11T13:25:11Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"Nvidia Rises on Oracle's AI Spending Outlook\",\n",
       "   'url': 'https://finance.yahoo.com/news/nvidia-rises-oracles-ai-spending-123002086.html',\n",
       "   'published': '2025-09-11T12:30:02Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Does AI Defeat The Purpose Of a Humanities Education?',\n",
       "   'url': 'https://www.thecrimson.com/article/2025/9/9/chiocco-farrell-harvard-ai/',\n",
       "   'published': '2025-09-11T16:15:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Warp Embeds AI Agents into a CLI to Provide Better Feedback Loop',\n",
       "   'url': 'https://devops.com/?p=178975',\n",
       "   'published': '2025-09-11T13:51:39Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Micron Technology Stock Jumps on Price Target Increase From Citi',\n",
       "   'url': 'https://www.investopedia.com/micron-technology-stock-jumps-on-price-target-increase-from-citi-11807966',\n",
       "   'published': '2025-09-11T14:35:43Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Promoting young people’s agency in the age of AI',\n",
       "   'url': 'https://www.raspberrypi.org/blog/promoting-young-peoples-agency-in-the-age-of-ai/',\n",
       "   'published': '2025-09-11T13:44:22Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '3 Ways AI Can Help You Build Wealth at Every Income Level',\n",
       "   'url': 'https://finance.yahoo.com/news/3-ways-ai-help-build-140034365.html',\n",
       "   'published': '2025-09-11T14:00:34Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'BIBI’s ‘Scott And Zelda’ Enters Spotify, Shazam Charts With Viral AI Cover Trend Led By BTS’ V',\n",
       "   'url': 'https://www.forbes.com/sites/jeffbenjamin/2025/09/11/bibis-scott-and-zelda-enters-spotify-shazam-charts-with-viral-ai-cover-trend-led-by-bts-v/',\n",
       "   'published': '2025-09-11T16:15:44Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Trump’s Economics: The Long Run Effects Matter Most',\n",
       "   'url': 'https://www.forbes.com/sites/harryholzer/2025/09/11/trumps-economics-the-long-run-effects-matter-most/',\n",
       "   'published': '2025-09-11T14:29:23Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Meet the protesters staging a hunger strike to prevent AI ‘catastrophe’',\n",
       "   'url': 'https://www.dazeddigital.com/life-culture/article/68614/1/meet-protesters-hunger-strike-prevent-ai-catastrophe-google-deepmind-london?utm_source=Link&utm_medium=Link&utm_campaign=RSSFeed&utm_term=meet-the-protesters-staging-a-hunger-strike-to-prevent-ai-catastrophe',\n",
       "   'published': '2025-09-11T14:21:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Smart toothbrush with charging travel case - Oral-B iO9 review',\n",
       "   'url': 'https://www.notebookcheck.net/Smart-toothbrush-with-charging-travel-case-Oral-B-iO9-review.1111868.0.html',\n",
       "   'published': '2025-09-11T12:38:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"Here's how much per month it costs to own a home in the U.S.\",\n",
       "   'url': 'https://www.cbsnews.com/news/homeownership-costs-monthly/',\n",
       "   'published': '2025-09-11T16:27:21Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'New study reveals space travel speeds up aging in human stem cells',\n",
       "   'url': 'https://techpinions.com/new-study-reveals-space-travel-speeds-up-aging-in-human-stem-cells/',\n",
       "   'published': '2025-09-11T13:01:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Emerging markets to lead 22% global egg growth by 2035 – Rabobank',\n",
       "   'url': 'https://www.just-food.com/news/emerging-markets-to-lead-22-global-egg-growth-by-2035-rabobank/',\n",
       "   'published': '2025-09-11T13:52:32Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Wearable Devices Stock Explodes As Financing Round And Gesture Control Patent Fuel Hype',\n",
       "   'url': 'https://finance.yahoo.com/news/wearable-devices-stock-explodes-financing-145055931.html',\n",
       "   'published': '2025-09-11T14:50:55Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Is AI a threat to our current encryption standards?',\n",
       "   'url': 'https://www.techradar.com/pro/is-ai-a-threat-to-our-current-encryption-standards',\n",
       "   'published': '2025-09-11T14:17:15Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '‘Quiet Layoffs’: 6 Red Flags That Your Career Is On The Chopping Block',\n",
       "   'url': 'https://www.forbes.com/sites/bryanrobinson/2025/09/11/quiet-layoffs-6-red-flags-that-your-career-is-on-the-chopping-block/',\n",
       "   'published': '2025-09-11T12:44:16Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Users With EU Apple Accounts to Miss Out on AirPods Live Translation Feature',\n",
       "   'url': 'https://www.mactrast.com/2025/09/users-with-eu-apple-accounts-to-miss-out-on-airpods-live-translation-feature/',\n",
       "   'published': '2025-09-11T12:56:29Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Globalization Is Dead, but Planetarization Has Yet to Be Born',\n",
       "   'url': 'https://www.project-syndicate.org/commentary/globalization-dead-and-military-spending-far-outstripping-climate-investment-by-bertrand-badre-2025-09',\n",
       "   'published': '2025-09-11T13:51:11Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'AI wants to help you plan your next trip. Can it save you time and money?',\n",
       "   'url': 'https://www.cbsnews.com/news/ai-travel-planning-tools/',\n",
       "   'published': '2025-09-11T12:55:31Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Expert shares how to cope with sudden job loss before retirement',\n",
       "   'url': 'https://finance.yahoo.com/news/expert-shares-how-to-cope-with-sudden-job-loss-before-retirement-150041440.html',\n",
       "   'published': '2025-09-11T15:00:41Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': ' Atlético de Madrid Women’s starting XI for the Champions League',\n",
       "   'url': 'https://onefootball.com/en/news/atletico-de-madrid-womens-starting-xi-for-the-champions-league-41648028',\n",
       "   'published': '2025-09-11T16:23:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Box debuts AI agents for almost every aspect of content management',\n",
       "   'url': 'https://siliconangle.com/2025/09/11/box-debuts-ai-agents-almost-every-aspect-content-management/',\n",
       "   'published': '2025-09-11T13:00:48Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Breaking news: Kirian Rodríguez given the all-clear by doctors',\n",
       "   'url': 'https://onefootball.com/en/news/breaking-news-kirian-rodriguez-given-the-all-clear-by-doctors-41646981',\n",
       "   'published': '2025-09-11T13:17:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Alibaba Leads Chinese Tech Funding Spree With $3.2 Billion Deal',\n",
       "   'url': 'https://finance.yahoo.com/news/alibaba-leads-chinese-tech-funding-035606623.html',\n",
       "   'published': '2025-09-11T13:24:13Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Oracle’s blockbuster earnings swell Larry Ellison’s fortune by $100 billion in 30 minutes to put him neck and neck with Elon Musk',\n",
       "   'url': 'https://fortune.com/2025/09/11/oracle-earnings-blockbuster-larry-ellison-elon-musk-worlds-richest-man-billionaires/',\n",
       "   'published': '2025-09-11T14:31:02Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': ' Official: LaLiga announces dates and kick-off times for matchday 9',\n",
       "   'url': 'https://onefootball.com/en/news/official-laliga-announces-dates-and-kick-off-times-for-matchday-9-41647673',\n",
       "   'published': '2025-09-11T15:19:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'EA Sports reveals top Bundesliga players in FC 26 ',\n",
       "   'url': 'https://onefootball.com/en/news/ea-sports-reveals-top-bundesliga-players-in-fc-26-41647671',\n",
       "   'published': '2025-09-11T15:19:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': ' Breaking news: Metropolitano to host 2027 Champions League final',\n",
       "   'url': 'https://onefootball.com/en/news/breaking-news-metropolitano-to-host-2027-champions-league-final-41647743',\n",
       "   'published': '2025-09-11T15:31:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '✋ Postponed: UEFA yet to rule on Villarreal-Barcelona in Miami',\n",
       "   'url': 'https://onefootball.com/en/news/postponed-uefa-yet-to-rule-on-villarreal-barcelona-in-miami-41647833',\n",
       "   'published': '2025-09-11T15:46:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': ' Why has Vardy signed for Cremonese?',\n",
       "   'url': 'https://onefootball.com/en/news/why-has-vardy-signed-for-cremonese-41647153',\n",
       "   'published': '2025-09-11T13:46:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Commanders vs. Packers props, SportsLine Machine Learning Model AI picks: Jordan Love Over 223.5 passing yards',\n",
       "   'url': 'https://www.cbssports.com/nfl/news/commanders-vs-packers-props-sportsline-machine-learning-model-ai-picks-jordan-love-over-223-5-passing-yards/',\n",
       "   'published': '2025-09-11T14:15:07Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'hepai 1.2.6',\n",
       "   'url': 'https://pypi.org/project/hepai/1.2.6/',\n",
       "   'published': '2025-09-11T15:09:59Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Applied Digital (APLD) Jumps 11% as AI Wave Resumes',\n",
       "   'url': 'https://finance.yahoo.com/news/applied-digital-apld-jumps-11-131440966.html',\n",
       "   'published': '2025-09-11T13:14:40Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Bloom Energy (BE) Soars to New High, Rides AI Rocket',\n",
       "   'url': 'https://finance.yahoo.com/news/bloom-energy-soars-high-rides-131447020.html',\n",
       "   'published': '2025-09-11T13:14:47Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Cipher Mining (CIFR) Hits Fresh High on AI Frenzy',\n",
       "   'url': 'https://finance.yahoo.com/news/cipher-mining-cifr-hits-fresh-131422130.html',\n",
       "   'published': '2025-09-11T13:14:22Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Credo Technology (CRDO) Jumps to New High on New Next-Gen AI Signal Processor',\n",
       "   'url': 'https://finance.yahoo.com/news/credo-technology-crdo-jumps-high-131411316.html',\n",
       "   'published': '2025-09-11T13:14:11Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'ai-ebash 0.1.44',\n",
       "   'url': 'https://pypi.org/project/ai-ebash/0.1.44/',\n",
       "   'published': '2025-09-11T13:02:24Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'ai-ebash 0.1.39',\n",
       "   'url': 'https://pypi.org/project/ai-ebash/0.1.39/',\n",
       "   'published': '2025-09-11T12:33:12Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'ai-ebash 0.1.42',\n",
       "   'url': 'https://pypi.org/project/ai-ebash/0.1.42/',\n",
       "   'published': '2025-09-11T12:48:55Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'ai-ebash 0.1.51',\n",
       "   'url': 'https://pypi.org/project/ai-ebash/0.1.51/',\n",
       "   'published': '2025-09-11T13:41:19Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Alibaba, Baidu said to be using in-house AI chips',\n",
       "   'url': 'https://biztoc.com/x/f764f727da4aace2',\n",
       "   'published': '2025-09-11T16:11:10Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'War of Words',\n",
       "   'url': 'https://wng.org/articles/war-of-words-1757602833',\n",
       "   'published': '2025-09-11T15:05:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'AI is changing the way leaders lead and companies create',\n",
       "   'url': 'https://www.atlassian.com/blog/teamwork/ai-insights-september-2025',\n",
       "   'published': '2025-09-11T15:27:36Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Home World Cup 2026 in sight: Reyna explains his move to Gladbach',\n",
       "   'url': 'https://onefootball.com/en/news/home-world-cup-2026-in-sight-reyna-explains-his-move-to-gladbach-41646942',\n",
       "   'published': '2025-09-11T13:12:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': ' Breaking: Thiago Alcántara returns to Barcelona with Flick',\n",
       "   'url': 'https://onefootball.com/en/news/breaking-thiago-alcantara-returns-to-barcelona-with-flick-41647444',\n",
       "   'published': '2025-09-11T14:39:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '6 Instances Karine Jean-Pierre Insisted Biden Was Sharp. Will She Tell Congress the Same?',\n",
       "   'url': 'https://www.dailysignal.com/2025/09/11/6-instances-karine-jean-pierre-insisted-biden-was-sharp-will-she-tell-congress-same/',\n",
       "   'published': '2025-09-11T15:36:37Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'When AI chatbots leak and how it happens',\n",
       "   'url': 'https://www.malwarebytes.com/blog/news/2025/09/when-ai-chatbots-leak-and-how-it-happens',\n",
       "   'published': '2025-09-11T12:46:12Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'What the Google Search Case Means for Antitrust Enforcement | Opinion',\n",
       "   'url': 'https://www.newsweek.com/what-google-search-case-means-antitrust-enforcement-opinion-2126426',\n",
       "   'published': '2025-09-11T14:29:26Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Over 1 in 5 Americans Following AI Medical Advice Later Proven Wrong—Survey',\n",
       "   'url': 'https://www.newsweek.com/over-one-five-americans-following-ai-medical-advice-later-proven-wrong-survey-2128344',\n",
       "   'published': '2025-09-11T15:41:59Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Samsung Galaxy Z Roll Mini Leaks: From Tablet to Phone in an Instant',\n",
       "   'url': 'https://www.geeky-gadgets.com/samsung-galaxy-z-roll-mini/',\n",
       "   'published': '2025-09-11T14:00:43Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '3 Ways AI Can Help You Build Wealth at Every Income Level',\n",
       "   'url': 'https://biztoc.com/x/406b98c265ce2cad',\n",
       "   'published': '2025-09-11T14:30:46Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Micron Technology Stock Jumps on Price Target Increase From Citi',\n",
       "   'url': 'https://biztoc.com/x/691b6892761feccc',\n",
       "   'published': '2025-09-11T14:53:31Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Alibaba Leads Chinese Tech Funding Spree With $3.2 Billion Deal',\n",
       "   'url': 'https://biztoc.com/x/c619c182873e8a3d',\n",
       "   'published': '2025-09-11T13:57:05Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '(GIST OF YOJANA) Empowering the Farmers',\n",
       "   'url': 'https://iasexamportal.com/the-gist/yojana-empowering-the-farmers',\n",
       "   'published': '2025-09-11T15:06:36Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Endpoint cybersecurity firm Koi raises $48M to stop attacks against enterprise networks',\n",
       "   'url': 'https://siliconangle.com/2025/09/11/endpoint-cybersecurity-firm-koi-raises-48m-stop-attacks-enterprise-networks/',\n",
       "   'published': '2025-09-11T15:45:06Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': '2025 NAFB Convention Registration in Progress',\n",
       "   'url': 'https://agwired.com/2025/09/11/2025-nafb-convention-registration-in-progress/',\n",
       "   'published': '2025-09-11T14:00:15Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Ant International and Alipay Aim to Safeguard Digital Wallets',\n",
       "   'url': 'http://www.pymnts.com/digital-payments/2025/ant-international-and-alipay-aim-to-safeguard-digital-wallets/',\n",
       "   'published': '2025-09-11T15:28:03Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Job Scams Surge 1,000% As Americans Struggle to Find Work',\n",
       "   'url': 'https://www.newsweek.com/job-scams-surge-1000-employment-struggles-2128153',\n",
       "   'published': '2025-09-11T15:07:36Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'The rationalist murders',\n",
       "   'url': 'https://newhumanist.org.uk/articles/6458/the-rationalist-murders#utm_source=rss',\n",
       "   'published': '2025-09-11T16:03:46Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'EmpowerFresh and Lowe’s Markets Expand Partnership to Elevate Produce Departments',\n",
       "   'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148649/0/en/EmpowerFresh-and-Lowe-s-Markets-Expand-Partnership-to-Elevate-Produce-Departments.html',\n",
       "   'published': '2025-09-11T14:00:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'US stock market rally to new highs: Dow hits record, S&P 500 and Nasdaq reach all-time highs as Nvidia and Micron jump ahead of bigger-than-expected Fed rate cut after jobless claims rise',\n",
       "   'url': 'https://economictimes.indiatimes.com/news/international/us/us-stock-market-rally-to-new-highs-dow-hits-record-sp-500-and-nasdaq-reach-all-time-highs-as-nvidia-and-micron-jump-ahead-of-bigger-than-expected-fed-rate-cut-after-jobless-claims-rise/articleshow/123833651.cms',\n",
       "   'published': '2025-09-11T15:41:34Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'The D Brief: Political violence in the USA; House’s NDAA; Intel centers may close; Old ICBMs may operate longer; And a bit more.',\n",
       "   'url': 'https://www.defenseone.com/threats/2025/09/the-d-brief-september-11-2025/408043/',\n",
       "   'published': '2025-09-11T15:01:42Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"AI Regulation Moratorium Idea Isn't Dead as Ted Cruz Pushes Sandbox Act\",\n",
       "   'url': 'https://www.cnet.com/tech/services-and-software/ai-regulation-moratorium-idea-isnt-dead-as-ted-cruz-pushes-sandbox-act/',\n",
       "   'published': '2025-09-11T15:28:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Machine embroidery encodes skin-like tension lines in textiles, enabling mass-customizable wearables',\n",
       "   'url': 'https://techxplore.com/news/2025-09-machine-embroidery-encodes-skin-tension.html',\n",
       "   'published': '2025-09-11T14:20:04Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'The pragmatic generation: How will Gen Z transform the global workplace?',\n",
       "   'url': 'https://fortune.com/2025/09/11/gen-z-the-pragmatic-generation-transfor-global-workplace-ey/',\n",
       "   'published': '2025-09-11T13:30:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Juve eye Lookman, Inter-Lucca twist, Napoli renewals: today’s news ',\n",
       "   'url': 'https://onefootball.com/en/news/juve-eye-lookman-inter-lucca-twist-napoli-renewals-todays-news-41646759',\n",
       "   'published': '2025-09-11T12:37:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Actress claims Tomb Raider IV–VI Remastered replaced French Lara Croft dialogue with AI voice-overs',\n",
       "   'url': 'https://www.notebookcheck.net/Actress-claims-Tomb-Raider-IV-VI-Remastered-replaced-French-Lara-Croft-dialogue-with-AI-voice-overs.1112395.0.html',\n",
       "   'published': '2025-09-11T15:48:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Automotive Aftermarket Service Software Market Forecast 2025-2030 - Open API Platforms Enabling Seamless E-Commerce Integration for Automotive Aftermarket Parts Retailers',\n",
       "   'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148720/28124/en/Automotive-Aftermarket-Service-Software-Market-Forecast-2025-2030-Open-API-Platforms-Enabling-Seamless-E-Commerce-Integration-for-Automotive-Aftermarket-Parts-Retailers.html',\n",
       "   'published': '2025-09-11T14:50:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Affiliate Marketing Solutions Industry Analysis Report 2025: Features Strategic Profiles of Awin, CJ Affiliate, Rakuten Advertising, Impact, Amazon and Other Key Players',\n",
       "   'url': 'https://www.globenewswire.com/news-release/2025/09/11/3148735/28124/en/Affiliate-Marketing-Solutions-Industry-Analysis-Report-2025-Features-Strategic-Profiles-of-Awin-CJ-Affiliate-Rakuten-Advertising-Impact-Amazon-and-Other-Key-Players.html',\n",
       "   'published': '2025-09-11T15:17:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Meesho’s annual festive sale to begin on September 19',\n",
       "   'url': 'https://economictimes.indiatimes.com/tech/technology/meeshos-annual-festive-sale-to-begin-on-september-19/articleshow/123831708.cms',\n",
       "   'published': '2025-09-11T12:40:41Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'GOP Sen. Marsha Blackburn dares Mark Zuckerberg to answer claims Meta ‘willfully’ pushed VR product harmful to kids',\n",
       "   'url': 'https://nypost.com/2025/09/11/us-news/gop-sen-marsha-blackburn-dares-mark-zuckerberg-to-answer-claims-meta-willfully-pushed-vr-product-harmful-to-kids/',\n",
       "   'published': '2025-09-11T15:29:39Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"Anna (Row) Auble's Life Memories - An AI-Assisted Memoir\",\n",
       "   'url': 'https://www.blogger.com/comment/fullpage/post/26204193/5943143610300778530',\n",
       "   'published': '2025-09-11T15:29:00Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'AI Goes Mainstream as Nearly Half of Retail Brands Now Use It Weekly',\n",
       "   'url': 'https://www.newsweek.com/nw-ai/ai-goes-mainstream-nearly-half-retail-brands-now-use-it-weekly-2126535',\n",
       "   'published': '2025-09-11T13:00:01Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Who is Larry Ellison, the Oracle founder who (briefly) became the world’s richest person?',\n",
       "   'url': 'https://financialpost.com/news/larry-ellison-oracle-founder-worlds-richest-person',\n",
       "   'published': '2025-09-11T13:04:46Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Alphabet, Meta, OpenAI, xAI and Snap face FTC probe over AI chatbot safety for kids',\n",
       "   'url': 'https://www.cnbc.com/2025/09/11/alphabet-meta-openai-x-ai-chatbot-ftc.html',\n",
       "   'published': '2025-09-11T15:32:02Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'NATO’s first battle with Russian drones inside its own borders puts alliance on defensive',\n",
       "   'url': 'https://abcnews.go.com/International/wireStory/natos-battle-russian-drones-inside-borders-puts-alliance-125479516',\n",
       "   'published': '2025-09-11T15:04:53Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Maharashtra govt, Lodha Developers ink pact for Green Integrated Data Centre Park',\n",
       "   'url': 'https://economictimes.indiatimes.com/industry/services/property-/-cstruction/maharashtra-govt-lodha-developers-ink-pact-for-green-integrated-data-centre-park/articleshow/123834550.cms',\n",
       "   'published': '2025-09-11T16:04:06Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Google, Meta, OpenAI face FTC inquiry on chatbot impact on kids',\n",
       "   'url': 'https://economictimes.indiatimes.com/tech/technology/google-meta-openai-face-ftc-inquiry-on-chatbot-impact-on-kids/articleshow/123834241.cms',\n",
       "   'published': '2025-09-11T15:41:14Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': \"China's AI2 Robotics hopes to go public within two years\",\n",
       "   'url': 'https://economictimes.indiatimes.com/tech/technology/chinas-ai2-robotics-hopes-to-go-public-within-two-years/articleshow/123832596.cms',\n",
       "   'published': '2025-09-11T13:35:56Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Albania appoints AI bot as minister to tackle corruption',\n",
       "   'url': 'https://economictimes.indiatimes.com/tech/artificial-intelligence/albania-appoints-ai-bot-as-minister-to-tackle-corruption/articleshow/123833556.cms',\n",
       "   'published': '2025-09-11T14:44:52Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Former OpenAI employee Suchir Balaji was murdered, alleges Elon Musk',\n",
       "   'url': 'https://economictimes.indiatimes.com/tech/artificial-intelligence/former-openai-employee-suchir-balaji-was-murdered-alleges-elon-musk/articleshow/123834134.cms',\n",
       "   'published': '2025-09-11T15:36:34Z'},\n",
       "  {'source': 'NewsAPI',\n",
       "   'title': 'Lutnick Signals Optimism on Trade Talks With Taiwan, Switzerland',\n",
       "   'url': 'https://financialpost.com/pmn/business-pmn/lutnick-signals-optimism-on-trade-talks-with-taiwan-switzerland',\n",
       "   'published': '2025-09-11T15:22:30Z'}],\n",
       " 'status': 'success',\n",
       " 'metadata': {'total_results': 144,\n",
       "  'articles_returned': 98,\n",
       "  'query': 'artificial intelligence',\n",
       "  'date_from': '2025-09-11T12:29:15'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async with Fetcher() as f:\n",
    "     z = await f.fetch_api('NewsAPI')\n",
    "z \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.sources.get('Ars Technica')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741be85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "\n",
    "class WorkflowStatusTool:\n",
    "    \"\"\"Tool to check current workflow status\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, logger: logging.Logger):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _check_workflow_status(self, ctx, args: str) -> str:\n",
    "        \"\"\"Get current workflow status report based on persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting check_workflow_status\")\n",
    "\n",
    "        try:\n",
    "            # Access the persistent state\n",
    "            state: NewsletterAgentState = ctx.context\n",
    "\n",
    "            # Create a status report based on persistent state\n",
    "            step_names = [\n",
    "                \"step_01_gather_urls\", \"step_02_filter_urls\", \"step_03_download_articles\",\n",
    "                \"step_04_extract_summaries\", \"step_05_cluster_by_topic\", \"step_06_rate_articles\",\n",
    "                \"step_07_select_sections\", \"step_08_draft_sections\", \"step_09_finalize_newsletter\"\n",
    "            ]\n",
    "\n",
    "            lines = [\n",
    "                \"WORKFLOW STATUS (FROM PERSISTENT STATE)\",\n",
    "                f\"Current Step: {state.current_step}/9\",\n",
    "                f\"Workflow Complete: {state.workflow_complete}\",\n",
    "                f\"Progress: {(state.current_step/9)*100:.1f}%\",\n",
    "                \"\",\n",
    "                \"Step Details:\"\n",
    "            ]\n",
    "\n",
    "            for i, step_name in enumerate(step_names, 1):\n",
    "                if i <= state.current_step:\n",
    "                    status = \"✅ completed\"\n",
    "                elif i == state.current_step + 1:\n",
    "                    status = \"➡️ next to execute\"\n",
    "                else:\n",
    "                    status = \"⭕ not started\"\n",
    "\n",
    "                formatted_name = step_name.replace('step_', 'Step ').replace('_', ' ').title()\n",
    "                formatted_name = formatted_name.replace('0', '').replace('  ', ' ')  # Clean up numbering\n",
    "                lines.append(f\"  {formatted_name}: {status}\")\n",
    "\n",
    "            if state.headline_data:\n",
    "                lines.extend([\n",
    "                    \"\",\n",
    "                    \"Data Summary:\",\n",
    "                    f\"  Total articles: {len(state.headline_data)}\",\n",
    "                    f\"  AI-related: {sum(1 for a in state.headline_data if a.get('ai_related') is True)}\",\n",
    "                    f\"  Summaries: {len(state.article_summaries)}\",\n",
    "                    f\"  Clusters: {len(state.topic_clusters)}\",\n",
    "                    f\"  Sections: {len(state.newsletter_sections)}\",\n",
    "                ])\n",
    "\n",
    "                result = \"\\n\".join(lines)\n",
    "                if self.logger:\n",
    "                    self.logger.info(\"Completed check_workflow_status\")\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"check_workflow_status failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"check_workflow_status\",\n",
    "            description=\"Check the current status of the newsletter workflow and see which steps are completed, in progress, or pending\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._check_workflow_status\n",
    "        )\n",
    "\n",
    "\n",
    "class StateInspectionTool:\n",
    "    \"\"\"Tool to inspect detailed persistent state data\"\"\"\n",
    "\n",
    "    def __init__(self, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _inspect_state(self, ctx, args: str) -> str:\n",
    "        \"\"\"Inspect detailed state data for debugging and monitoring\"\"\"\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Create detailed state report\n",
    "        report_lines = [\n",
    "            \"DETAILED STATE INSPECTION\",\n",
    "            \"=\" * 50,\n",
    "            f\"Current Step: {state.current_step}/9\",\n",
    "            f\"Workflow Complete: {state.workflow_complete}\",\n",
    "            f\"Sources File: {state.sources_file}\",\n",
    "            \"\",\n",
    "            \"HEADLINE DATA:\",\n",
    "            f\"  Total articles: {len(state.headline_data)}\",\n",
    "        ]\n",
    "\n",
    "        if state.headline_data:\n",
    "            ai_related = sum(1 for a in state.headline_data if a.get('ai_related') is True)\n",
    "            with_content = sum(1 for a in state.headline_data if a.get('content'))\n",
    "            with_ratings = sum(1 for a in state.headline_data if a.get('quality_rating'))\n",
    "            with_clusters = sum(1 for a in state.headline_data if a.get('cluster_topic'))\n",
    "\n",
    "            report_lines.extend([\n",
    "                f\"  AI-related: {ai_related}\",\n",
    "                f\"  With content: {with_content}\",\n",
    "                f\"  With ratings: {with_ratings}\",\n",
    "                f\"  With clusters: {with_clusters}\",\n",
    "                f\"  Sources: {len(set(a.get('source', 'Unknown') for a in state.headline_data))}\",\n",
    "            ])\n",
    "\n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"PROCESSING RESULTS:\",\n",
    "            f\"  Article summaries: {len(state.article_summaries)} articles\",\n",
    "            f\"  Topic clusters: {len(state.topic_clusters)} topics\",\n",
    "            f\"  Newsletter sections: {len(state.newsletter_sections)} sections\",\n",
    "            f\"  Final newsletter: {'Generated' if state.final_newsletter else 'Not created'}\",\n",
    "        ])\n",
    "\n",
    "        if state.topic_clusters:\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"TOPIC CLUSTERS:\",\n",
    "            ])\n",
    "            for topic, urls in state.topic_clusters.items():\n",
    "                report_lines.append(f\"  {topic}: {len(urls)} articles\")\n",
    "\n",
    "        if state.newsletter_sections:\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"NEWSLETTER SECTIONS:\",\n",
    "            ])\n",
    "            for section_name, section_data in state.newsletter_sections.items():\n",
    "                status = section_data.get('section_status', 'unknown')\n",
    "                word_count = section_data.get('word_count', 0)\n",
    "                article_count = section_data.get('article_count', 0)\n",
    "                report_lines.append(f\"  {section_name}: {status}, {article_count} articles, {word_count} words\")\n",
    "\n",
    "        if state.final_newsletter:\n",
    "            newsletter_words = len(state.final_newsletter.split())\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"FINAL NEWSLETTER:\",\n",
    "                f\"  Length: {newsletter_words} words\",\n",
    "                f\"  Preview: {state.final_newsletter[:200]}...\" if len(state.final_newsletter) > 200 else f\"  Content: {state.final_newsletter}\",\n",
    "            ])\n",
    "\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"inspect_state\",\n",
    "            description=\"Inspect detailed persistent state data including article counts, processing results, and content status. Useful for debugging and monitoring workflow progress.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._inspect_state\n",
    "        )\n",
    "\n",
    "\n",
    "class GatherUrlsTool:\n",
    "    \"\"\"Tool for Step 1: Gather URLs from various news sources\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status  # Keep for UI progress tracking\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _gather_urls(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 1: Gather URLs using persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting Step 1: Gather URLs\")\n",
    "\n",
    "        step_name = \"step_01_gather_urls\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 1:\n",
    "            total_articles = len(state.headline_data)\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Step 1 already completed with {total_articles} articles\")\n",
    "            return f\"Step 1 already completed! Found {total_articles} articles in persistent state.\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Use real RSS fetching from sources.yaml\n",
    "            fetcher = Fetcher()\n",
    "#             rss_dict = await Fetcher.fetch_rss()\n",
    "#             html_dict = await Fetcher.fetch_html()\n",
    "#             api_dict = await Fetcher.fetch_api()\n",
    "            sources_results = await fetcher.gather_all()\n",
    "            print(sources_results)\n",
    "\n",
    "            # Process results and store in persistent state\n",
    "            all_articles = []\n",
    "            successful_sources = []\n",
    "            failed_sources = []\n",
    "\n",
    "            for result in sources_results:\n",
    "                if result['status'] == 'success' and result['results']:\n",
    "                    # Add source info to each article\n",
    "                    for article in result['results']:\n",
    "                        article['source_key'] = result['source_key']\n",
    "                        article['ai_related'] = None  # To be determined in step 2\n",
    "                        all_articles.append(article)\n",
    "                    successful_sources.append(result['source_key'])\n",
    "                elif result['status'] == 'not_implemented':\n",
    "                    # Skip HTML/API sources for now\n",
    "                    continue\n",
    "                else:\n",
    "                    failed_sources.append(result['source_key'])\n",
    "\n",
    "            # Store results in persistent state\n",
    "            state.headline_data = all_articles\n",
    "            state.current_step = 1\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 1: Gathered {len(all_articles)} URLs from {len(successful_sources)} RSS sources\")\n",
    "                if failed_sources:\n",
    "                    print(f\"⚠️  Failed sources: {', '.join(failed_sources)}\")\n",
    "\n",
    "            status_msg = f\"✅ Step 1 completed successfully! Gathered {len(all_articles)} articles from {len(successful_sources)} sources (RSS only).\"\n",
    "            if failed_sources:\n",
    "                status_msg += f\" {len(failed_sources)} sources failed or not implemented.\"\n",
    "\n",
    "            status_msg += f\"\\n\\n📊 Articles stored in persistent state: {len(state.headline_data)}\"\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Completed Step 1: Gathered {len(all_articles)} articles\")\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Step 1 failed: {str(e)}\")\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 1 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"gather_urls\",\n",
    "            description=\"Execute Step 1: Gather URLs and headlines from various news sources. Only use this tool if Step 1 is not already completed.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._gather_urls\n",
    "        )\n",
    "\n",
    "\n",
    "class FilterUrlsTool:\n",
    "    \"\"\"Tool for Step 2: Filter URLs to AI-related content\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _filter_urls(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 2: Filter URLs using persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting Step 2: Filter URLs\")\n",
    "\n",
    "        step_name = \"step_02_filter_urls\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 2:\n",
    "            ai_related_count = sum(1 for article in state.headline_data if article.get('ai_related') is True)\n",
    "            total_count = len(state.headline_data)\n",
    "            return f\"Step 2 already completed! Filtered {total_count} articles, {ai_related_count} identified as AI-related.\"\n",
    "\n",
    "        # Check if step 1 is completed\n",
    "        if state.current_step < 1 or not state.headline_data:\n",
    "            return f\"❌ Cannot execute Step 2: Step 1 (Gather URLs) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Read headlines from persistent state\n",
    "            total_articles = len(state.headline_data)\n",
    "\n",
    "            # Mock AI classification - in a real implementation, this would use an AI model\n",
    "            # to analyze titles and descriptions for AI relevance\n",
    "            ai_related_count = 0\n",
    "            for i, article in enumerate(state.headline_data):\n",
    "                # Simple keyword-based mock classification\n",
    "                title_lower = article.get('title', '').lower()\n",
    "                description_lower = article.get('description', '').lower()\n",
    "\n",
    "                ai_keywords = [\n",
    "                    'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',\n",
    "                    'neural network', 'llm', 'large language model', 'gpt', 'claude',\n",
    "                    'openai', 'anthropic', 'chatbot', 'automation', 'algorithm',\n",
    "                    'computer vision', 'natural language', 'nlp', 'robotics'\n",
    "                ]\n",
    "\n",
    "                is_ai_related = any(keyword in title_lower or keyword in description_lower\n",
    "                                  for keyword in ai_keywords)\n",
    "\n",
    "                # Update article with AI classification\n",
    "                state.headline_data[i]['ai_related'] = is_ai_related\n",
    "                if is_ai_related:\n",
    "                    ai_related_count += 1\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 2\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            filter_accuracy = ai_related_count / total_articles if total_articles > 0 else 0\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 2: Filtered to {ai_related_count} AI-related headlines from {total_articles} total\")\n",
    "\n",
    "            status_msg = f\"✅ Step 2 completed successfully! Filtered {total_articles} headlines to {ai_related_count} AI-related articles (accuracy: {filter_accuracy:.1%}).\"\n",
    "            status_msg += f\"\\n\\n📊 Results stored in persistent state. Current step: {state.current_step}\"\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Completed Step 2: Filtered to {ai_related_count} AI-related articles\")\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Step 2 failed: {str(e)}\")\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 2 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"filter_urls\",\n",
    "            description=\"Execute Step 2: Filter URLs to AI-related content only. Requires Step 1 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._filter_urls\n",
    "        )\n",
    "\n",
    "\n",
    "class DownloadArticlesTool:\n",
    "    \"\"\"Tool for Step 3: Download article content\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _download_articles(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 3: Download Articles using persistent state\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(\"Starting Step 3: Download Articles\")\n",
    "\n",
    "        step_name = \"step_03_download_articles\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 3:\n",
    "            ai_articles = [article for article in state.headline_data if article.get('ai_related') is True]\n",
    "            downloaded_count = sum(1 for article in ai_articles if article.get('content'))\n",
    "            return f\"Step 3 already completed! Downloaded content for {downloaded_count} AI-related articles.\"\n",
    "\n",
    "        # Check if step 2 is completed\n",
    "        if state.current_step < 2:\n",
    "            return f\"❌ Cannot execute Step 3: Step 2 (Filter URLs) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get AI-related articles from persistent state\n",
    "            ai_articles = [article for article in state.headline_data if article.get('ai_related') is True]\n",
    "\n",
    "            if not ai_articles:\n",
    "                return f\"❌ No AI-related articles found to download. Please run step 2 first.\"\n",
    "\n",
    "            # Mock content download - in a real implementation, this would fetch actual article content\n",
    "            successful_downloads = 0\n",
    "            total_length = 0\n",
    "\n",
    "            for article in state.headline_data:\n",
    "                if article.get('ai_related') is True:\n",
    "                    # Simulate downloading article content\n",
    "                    # In reality, this would use web scraping or API calls\n",
    "                    mock_content = f\"Mock article content for: {article.get('title', 'Unknown title')}\\n\\n\"\n",
    "                    mock_content += f\"This is placeholder content that would normally be extracted from {article.get('url', 'unknown URL')}.\\n\"\n",
    "                    mock_content += f\"The article covers topics related to AI and technology as indicated by the title and description.\\n\"\n",
    "                    mock_content += f\"Source: {article.get('source', 'Unknown source')}\\n\"\n",
    "\n",
    "                    # Add content to the article data\n",
    "                    article['content'] = mock_content\n",
    "                    article['download_timestamp'] = datetime.now().isoformat()\n",
    "                    article['content_length'] = len(mock_content)\n",
    "\n",
    "                    successful_downloads += 1\n",
    "                    total_length += len(mock_content)\n",
    "\n",
    "            # Calculate stats\n",
    "            download_success_rate = successful_downloads / len(ai_articles) if ai_articles else 0\n",
    "            avg_article_length = total_length / successful_downloads if successful_downloads > 0 else 0\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 3\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 3: Downloaded {successful_downloads} AI-related articles\")\n",
    "\n",
    "            status_msg = f\"✅ Step 3 completed successfully! Downloaded {successful_downloads} AI-related articles with {download_success_rate:.0%} success rate.\"\n",
    "            status_msg += f\"\\n📊 Average article length: {avg_article_length:.0f} characters\"\n",
    "            status_msg += f\"\\n🔗 Content stored in persistent state. Current step: {state.current_step}\"\n",
    "            if self.logger:\n",
    "                self.logger.info(f\"Completed Step 3: Downloaded {successful_downloads} articles\")\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Step 3 failed: {str(e)}\")\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 3 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"download_articles\",\n",
    "            description=\"Execute Step 3: Download full article content from filtered URLs. Requires Step 2 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._download_articles\n",
    "        )\n",
    "\n",
    "\n",
    "class ExtractSummariesTool:\n",
    "    \"\"\"Tool for Step 4: Extract article summaries\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _extract_summaries(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 4: Extract Summaries using persistent state\"\"\"\n",
    "        step_name = \"step_04_extract_summaries\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 4:\n",
    "            summary_count = len([url for url in state.article_summaries.keys() if state.article_summaries[url]])\n",
    "            return f\"Step 4 already completed! Generated summaries for {summary_count} articles.\"\n",
    "\n",
    "        # Check if step 3 is completed\n",
    "        if state.current_step < 3:\n",
    "            return f\"❌ Cannot execute Step 4: Step 3 (Download Articles) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get articles with content from persistent state\n",
    "            articles_with_content = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and article.get('content')\n",
    "            ]\n",
    "\n",
    "            if not articles_with_content:\n",
    "                return f\"❌ No downloaded AI-related articles found to summarize. Please run step 3 first.\"\n",
    "\n",
    "            # Clear existing summaries if rerunning\n",
    "            state.article_summaries = {}\n",
    "\n",
    "            # Generate summaries for each article\n",
    "            articles_summarized = 0\n",
    "            total_bullets = 0\n",
    "\n",
    "            for article in articles_with_content:\n",
    "                url = article.get('url', f\"article_{articles_summarized}\")\n",
    "                title = article.get('title', 'Unknown title')\n",
    "                content = article.get('content', '')\n",
    "\n",
    "                # Mock summary generation - in a real implementation, this would use an AI model\n",
    "                # to create bullet point summaries from the full article content\n",
    "                mock_summary = [\n",
    "                    f\"Key insight from '{title[:50]}...' - Main technological development discussed\",\n",
    "                    f\"Business implications or market impact highlighted in the article\",\n",
    "                    f\"Future outlook or expert predictions mentioned in the content\"\n",
    "                ]\n",
    "\n",
    "                # Store summary in persistent state\n",
    "                state.article_summaries[url] = mock_summary\n",
    "                articles_summarized += 1\n",
    "                total_bullets += len(mock_summary)\n",
    "\n",
    "                # Add summary reference to article data as well\n",
    "                article['summary_bullets'] = len(mock_summary)\n",
    "                article['summary_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "            # Calculate stats\n",
    "            avg_bullets_per_article = total_bullets / articles_summarized if articles_summarized > 0 else 0\n",
    "            summary_quality_score = 0.89  # Mock quality score\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 4\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 4: Created summaries for {articles_summarized} articles\")\n",
    "\n",
    "            status_msg = f\"✅ Step 4 completed successfully! Generated {avg_bullets_per_article:.1f}-bullet summaries for {articles_summarized} articles.\"\n",
    "            status_msg += f\"\\n📝 Quality score: {summary_quality_score:.1%}\"\n",
    "            status_msg += f\"\\n💾 Summaries stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 4 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"extract_summaries\",\n",
    "            description=\"Execute Step 4: Create bullet point summaries of each downloaded article. Requires Step 3 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._extract_summaries\n",
    "        )\n",
    "\n",
    "\n",
    "class ClusterByTopicTool:\n",
    "    \"\"\"Tool for Step 5: Cluster articles by topic\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _cluster_by_topic(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 5: Cluster By Topic using persistent state\"\"\"\n",
    "        step_name = \"step_05_cluster_by_topic\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 5:\n",
    "            cluster_count = len(state.topic_clusters)\n",
    "            total_articles = sum(len(articles) for articles in state.topic_clusters.values())\n",
    "            return f\"Step 5 already completed! Created {cluster_count} topic clusters with {total_articles} articles.\"\n",
    "\n",
    "        # Check if step 4 is completed\n",
    "        if state.current_step < 4:\n",
    "            return f\"❌ Cannot execute Step 5: Step 4 (Extract Summaries) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get articles with summaries from persistent state\n",
    "            articles_with_summaries = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and\n",
    "                article.get('url') in state.article_summaries\n",
    "            ]\n",
    "\n",
    "            if not articles_with_summaries:\n",
    "                return f\"❌ No summarized articles found to cluster. Please run step 4 first.\"\n",
    "\n",
    "            # Clear existing clusters if rerunning\n",
    "            state.topic_clusters = {}\n",
    "\n",
    "            # Mock clustering logic - in a real implementation, this would use NLP/ML\n",
    "            # to group articles by semantic similarity of their titles and summaries\n",
    "            predefined_topics = [\n",
    "                \"LLM Advances\", \"AI Safety & Ethics\", \"Business AI Applications\",\n",
    "                \"Research Breakthroughs\", \"Industry News\", \"Other AI Topics\"\n",
    "            ]\n",
    "\n",
    "            # Initialize empty clusters\n",
    "            for topic in predefined_topics:\n",
    "                state.topic_clusters[topic] = []\n",
    "\n",
    "            # Simple keyword-based clustering\n",
    "            topic_keywords = {\n",
    "                \"LLM Advances\": [\"llm\", \"large language model\", \"gpt\", \"claude\", \"language model\", \"chatbot\", \"chat\"],\n",
    "                \"AI Safety & Ethics\": [\"safety\", \"ethics\", \"bias\", \"fairness\", \"responsible\", \"trust\", \"alignment\"],\n",
    "                \"Business AI Applications\": [\"business\", \"enterprise\", \"productivity\", \"automation\", \"workflow\", \"commercial\"],\n",
    "                \"Research Breakthroughs\": [\"research\", \"breakthrough\", \"paper\", \"study\", \"academic\", \"university\", \"science\"],\n",
    "                \"Industry News\": [\"company\", \"startup\", \"funding\", \"acquisition\", \"partnership\", \"launch\", \"release\"],\n",
    "                \"Other AI Topics\": []  # Catch-all\n",
    "            }\n",
    "\n",
    "            for article in articles_with_summaries:\n",
    "                url = article.get('url', '')\n",
    "                title_lower = article.get('title', '').lower()\n",
    "                description_lower = article.get('description', '').lower()\n",
    "\n",
    "                # Find best matching topic\n",
    "                best_topic = \"Other AI Topics\"  # Default\n",
    "                max_matches = 0\n",
    "\n",
    "                for topic, keywords in topic_keywords.items():\n",
    "                    if topic == \"Other AI Topics\":\n",
    "                        continue\n",
    "\n",
    "                    matches = sum(1 for keyword in keywords\n",
    "                                if keyword in title_lower or keyword in description_lower)\n",
    "\n",
    "                    if matches > max_matches:\n",
    "                        max_matches = matches\n",
    "                        best_topic = topic\n",
    "\n",
    "                # Add article URL to the appropriate cluster\n",
    "                state.topic_clusters[best_topic].append(url)\n",
    "\n",
    "                # Also update the article with cluster info\n",
    "                article['cluster_topic'] = best_topic\n",
    "                article['cluster_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "            # Remove empty clusters\n",
    "            state.topic_clusters = {\n",
    "                topic: articles for topic, articles in state.topic_clusters.items()\n",
    "                if articles\n",
    "            }\n",
    "\n",
    "            # Calculate stats\n",
    "            total_clusters = len(state.topic_clusters)\n",
    "            total_articles = sum(len(articles) for articles in state.topic_clusters.values())\n",
    "            cluster_coherence_score = 0.84  # Mock coherence score\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 5\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 5: Created {total_clusters} topic clusters\")\n",
    "\n",
    "            status_msg = f\"✅ Step 5 completed successfully! Organized {total_articles} articles into {total_clusters} topic clusters.\"\n",
    "            status_msg += f\"\\n📊 Cluster coherence score: {cluster_coherence_score:.1%}\"\n",
    "            status_msg += f\"\\n🏷️ Topics: {', '.join(state.topic_clusters.keys())}\"\n",
    "            status_msg += f\"\\n💾 Clusters stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 5 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"cluster_by_topic\",\n",
    "            description=\"Execute Step 5: Group articles by thematic topics using clustering. Requires Step 4 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._cluster_by_topic\n",
    "        )\n",
    "\n",
    "\n",
    "class RateArticlesTool:\n",
    "    \"\"\"Tool for Step 6: Rate article quality and importance\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _rate_articles(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 6: Rate Articles using persistent state\"\"\"\n",
    "        step_name = \"step_06_rate_articles\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 6:\n",
    "            rated_articles = [article for article in state.headline_data if article.get('quality_rating')]\n",
    "            avg_rating = sum(article.get('quality_rating', 0) for article in rated_articles) / len(rated_articles) if rated_articles else 0\n",
    "            return f\"Step 6 already completed! Rated {len(rated_articles)} articles with average rating {avg_rating:.1f}/10.\"\n",
    "\n",
    "        # Check if step 5 is completed\n",
    "        if state.current_step < 5:\n",
    "            return f\"❌ Cannot execute Step 6: Step 5 (Cluster By Topic) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get clustered articles from persistent state\n",
    "            clustered_articles = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and article.get('cluster_topic')\n",
    "            ]\n",
    "\n",
    "            if not clustered_articles:\n",
    "                return f\"❌ No clustered articles found to rate. Please run step 5 first.\"\n",
    "\n",
    "            # Rate each article based on mock criteria\n",
    "            articles_rated = 0\n",
    "            total_rating = 0\n",
    "            high_quality_count = 0\n",
    "\n",
    "            for article in clustered_articles:\n",
    "                # Mock rating logic - in reality, this would use AI to evaluate:\n",
    "                # - Content quality, originality, depth\n",
    "                # - Source credibility\n",
    "                # - Relevance to AI community\n",
    "                # - Timeliness and newsworthiness\n",
    "\n",
    "                title_length = len(article.get('title', ''))\n",
    "                has_description = bool(article.get('description', ''))\n",
    "                source_quality = 8 if article.get('source') in ['Techmeme', 'Ars Technica', 'The Verge'] else 6\n",
    "                cluster_bonus = 2 if article.get('cluster_topic') != 'Other AI Topics' else 0\n",
    "\n",
    "                # Calculate mock quality rating (1-10)\n",
    "                base_rating = 5\n",
    "                if title_length > 50: base_rating += 1\n",
    "                if has_description: base_rating += 1\n",
    "                rating = min(10, base_rating + (source_quality - 6) + cluster_bonus)\n",
    "\n",
    "                # Add some randomness to make it more realistic\n",
    "                import random\n",
    "                rating = max(1, min(10, rating + random.uniform(-1, 1)))\n",
    "\n",
    "                # Store rating in article data\n",
    "                article['quality_rating'] = round(rating, 1)\n",
    "                article['rating_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "                articles_rated += 1\n",
    "                total_rating += rating\n",
    "                if rating >= 7.0:\n",
    "                    high_quality_count += 1\n",
    "\n",
    "            # Calculate stats\n",
    "            avg_rating = total_rating / articles_rated if articles_rated > 0 else 0\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 6\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 6: Rated {articles_rated} articles\")\n",
    "\n",
    "            status_msg = f\"✅ Step 6 completed successfully! Rated {articles_rated} articles with average rating {avg_rating:.1f}/10.\"\n",
    "            status_msg += f\"\\n⭐ High quality articles (≥7.0): {high_quality_count}\"\n",
    "            status_msg += f\"\\n💾 Ratings stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 6 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"rate_articles\",\n",
    "            description=\"Execute Step 6: Evaluate article quality and importance with ratings. Requires Step 5 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._rate_articles\n",
    "        )\n",
    "\n",
    "\n",
    "class SelectSectionsTool:\n",
    "    \"\"\"Tool for Step 7: Select newsletter sections\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _select_sections(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 7: Select Sections using persistent state\"\"\"\n",
    "        step_name = \"step_07_select_sections\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 7:\n",
    "            section_count = len(state.newsletter_sections)\n",
    "            return f\"Step 7 already completed! Created {section_count} newsletter sections.\"\n",
    "\n",
    "        # Check if step 6 is completed\n",
    "        if state.current_step < 6:\n",
    "            return f\"❌ Cannot execute Step 7: Step 6 (Rate Articles) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get rated articles from persistent state\n",
    "            rated_articles = [\n",
    "                article for article in state.headline_data\n",
    "                if article.get('ai_related') is True and article.get('quality_rating')\n",
    "            ]\n",
    "\n",
    "            if not rated_articles:\n",
    "                return f\"❌ No rated articles found to organize into sections. Please run step 6 first.\"\n",
    "\n",
    "            # Clear existing sections if rerunning\n",
    "            state.newsletter_sections = {}\n",
    "\n",
    "            # Create newsletter sections based on topic clusters and ratings\n",
    "            # Use existing topic clusters but prioritize high-quality articles\n",
    "            high_quality_articles = [a for a in rated_articles if a.get('quality_rating', 0) >= 7.0]\n",
    "            medium_quality_articles = [a for a in rated_articles if 5.0 <= a.get('quality_rating', 0) < 7.0]\n",
    "\n",
    "            # Group articles by cluster topic and select best ones for each section\n",
    "            cluster_sections = {}\n",
    "            for article in high_quality_articles + medium_quality_articles:\n",
    "                cluster = article.get('cluster_topic', 'Other AI Topics')\n",
    "                if cluster not in cluster_sections:\n",
    "                    cluster_sections[cluster] = []\n",
    "                cluster_sections[cluster].append(article)\n",
    "\n",
    "            # Create newsletter sections with article assignments\n",
    "            articles_assigned = 0\n",
    "            for cluster, articles in cluster_sections.items():\n",
    "                if not articles:\n",
    "                    continue\n",
    "\n",
    "                # Sort articles by rating (highest first) and take top articles\n",
    "                sorted_articles = sorted(articles, key=lambda x: x.get('quality_rating', 0), reverse=True)\n",
    "                top_articles = sorted_articles[:5]  # Max 5 articles per section\n",
    "\n",
    "                # Create section outline (will be filled in step 8)\n",
    "                section_content = {\n",
    "                    'title': cluster,\n",
    "                    'article_count': len(top_articles),\n",
    "                    'articles': [{\n",
    "                        'url': article.get('url'),\n",
    "                        'title': article.get('title'),\n",
    "                        'rating': article.get('quality_rating'),\n",
    "                        'source': article.get('source')\n",
    "                    } for article in top_articles],\n",
    "                    'section_status': 'selected',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "                state.newsletter_sections[cluster] = section_content\n",
    "                articles_assigned += len(top_articles)\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 7\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 7: Created {len(state.newsletter_sections)} newsletter sections\")\n",
    "\n",
    "            status_msg = f\"✅ Step 7 completed successfully! Organized content into {len(state.newsletter_sections)} sections with {articles_assigned} articles assigned.\"\n",
    "            status_msg += f\"\\n📑 Sections: {', '.join(state.newsletter_sections.keys())}\"\n",
    "            status_msg += f\"\\n💾 Section plan stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 7 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"select_sections\",\n",
    "            description=\"Execute Step 7: Organize articles into newsletter sections based on topics and ratings. Requires Step 6 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._select_sections\n",
    "        )\n",
    "\n",
    "\n",
    "class DraftSectionsTool:\n",
    "    \"\"\"Tool for Step 8: Draft section content\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _draft_sections(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 8: Draft Sections using persistent state\"\"\"\n",
    "        step_name = \"step_08_draft_sections\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 8:\n",
    "            drafted_sections = [s for s in state.newsletter_sections.values() if s.get('content')]\n",
    "            total_words = sum(len(s.get('content', '').split()) for s in drafted_sections)\n",
    "            return f\"Step 8 already completed! Drafted {len(drafted_sections)} sections with {total_words} total words.\"\n",
    "\n",
    "        # Check if step 7 is completed\n",
    "        if state.current_step < 7:\n",
    "            return f\"❌ Cannot execute Step 8: Step 7 (Select Sections) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get section plans from persistent state\n",
    "            if not state.newsletter_sections:\n",
    "                return f\"❌ No newsletter sections found to draft. Please run step 7 first.\"\n",
    "\n",
    "            # Draft content for each section\n",
    "            sections_drafted = 0\n",
    "            total_words = 0\n",
    "\n",
    "            for section_name, section_data in state.newsletter_sections.items():\n",
    "                if section_data.get('section_status') != 'selected':\n",
    "                    continue\n",
    "\n",
    "                articles = section_data.get('articles', [])\n",
    "                if not articles:\n",
    "                    continue\n",
    "\n",
    "                # Mock section content generation - in reality, this would use AI\n",
    "                # to create engaging newsletter content from article summaries\n",
    "                section_content = f\"## {section_name}\\n\\n\"\n",
    "\n",
    "                # Add intro paragraph\n",
    "                intro_templates = {\n",
    "                    'LLM Advances': \"The latest developments in large language models continue to push the boundaries of what's possible in AI.\",\n",
    "                    'AI Safety & Ethics': \"Important discussions around responsible AI development and deployment are shaping the future of the field.\",\n",
    "                    'Business AI Applications': \"Companies are finding innovative ways to integrate AI into their products and workflows.\",\n",
    "                    'Research Breakthroughs': \"Academic researchers are making significant strides in advancing our understanding of artificial intelligence.\",\n",
    "                    'Industry News': \"The AI industry continues to evolve with new partnerships, funding rounds, and product launches.\"\n",
    "                }\n",
    "\n",
    "                intro = intro_templates.get(section_name, f\"Here are the latest updates in {section_name.lower()}.\")\n",
    "                section_content += f\"{intro}\\n\\n\"\n",
    "\n",
    "                # Add article summaries\n",
    "                for i, article in enumerate(articles[:3]):  # Top 3 articles per section\n",
    "                    article_url = article.get('url', '')\n",
    "                    article_title = article.get('title', 'Unknown Title')\n",
    "                    article_source = article.get('source', 'Unknown Source')\n",
    "\n",
    "                    # Get the actual summary from state if available\n",
    "                    summary_bullets = state.article_summaries.get(article_url, [\n",
    "                        f\"Key insights from this {section_name.lower()} article\",\n",
    "                        f\"Important implications for the AI community\",\n",
    "                        f\"Notable developments worth following\"\n",
    "                    ])\n",
    "\n",
    "                    section_content += f\"### {article_title}\\n\"\n",
    "                    section_content += f\"*Source: {article_source}*\\n\\n\"\n",
    "\n",
    "                    for bullet in summary_bullets:\n",
    "                        section_content += f\"- {bullet}\\n\"\n",
    "\n",
    "                    section_content += f\"\\n[Read more]({article_url})\\n\\n\"\n",
    "\n",
    "                # Store the drafted content\n",
    "                state.newsletter_sections[section_name]['content'] = section_content\n",
    "                state.newsletter_sections[section_name]['section_status'] = 'drafted'\n",
    "                state.newsletter_sections[section_name]['draft_timestamp'] = datetime.now().isoformat()\n",
    "                state.newsletter_sections[section_name]['word_count'] = len(section_content.split())\n",
    "\n",
    "                sections_drafted += 1\n",
    "                total_words += len(section_content.split())\n",
    "\n",
    "            # Calculate average words per section\n",
    "            avg_words_per_section = total_words / sections_drafted if sections_drafted > 0 else 0\n",
    "\n",
    "            # Update persistent state\n",
    "            state.current_step = 8\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 8: Drafted {sections_drafted} sections\")\n",
    "\n",
    "            status_msg = f\"✅ Step 8 completed successfully! Drafted {sections_drafted} sections with {total_words} total words.\"\n",
    "            status_msg += f\"\\n📝 Average words per section: {avg_words_per_section:.0f}\"\n",
    "            status_msg += f\"\\n💾 Section content stored in persistent state. Current step: {state.current_step}\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 8 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"draft_sections\",\n",
    "            description=\"Execute Step 8: Write engaging content for each newsletter section. Requires Step 7 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._draft_sections\n",
    "        )\n",
    "\n",
    "\n",
    "class FinalizeNewsletterTool:\n",
    "    \"\"\"Tool for Step 9: Finalize complete newsletter\"\"\"\n",
    "\n",
    "    def __init__(self, workflow_status: WorkflowStatus, verbose: bool = False, logger: logging.Logger = None):\n",
    "        self.workflow_status = workflow_status\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "\n",
    "    async def _finalize_newsletter(self, ctx, args: str) -> str:\n",
    "        \"\"\"Execute Step 9: Finalize Newsletter using persistent state\"\"\"\n",
    "        step_name = \"step_09_finalize_newsletter\"\n",
    "\n",
    "        # Access the persistent state\n",
    "        state: NewsletterAgentState = ctx.context\n",
    "\n",
    "        # Check if step already completed via persistent state\n",
    "        if state.current_step >= 9:\n",
    "            newsletter_length = len(state.final_newsletter.split()) if state.final_newsletter else 0\n",
    "            sections_count = len([s for s in state.newsletter_sections.values() if s.get('content')])\n",
    "            return f\"Step 9 already completed! Newsletter finalized with {sections_count} sections and {newsletter_length} words.\"\n",
    "\n",
    "        # Check if step 8 is completed\n",
    "        if state.current_step < 8:\n",
    "            return f\"❌ Cannot execute Step 9: Step 8 (Draft Sections) must be completed first. Current step: {state.current_step}\"\n",
    "\n",
    "        try:\n",
    "            # Update workflow status for UI tracking\n",
    "            self.workflow_status.start_step(step_name)\n",
    "\n",
    "            # Get drafted sections from persistent state\n",
    "            drafted_sections = {\n",
    "                name: data for name, data in state.newsletter_sections.items()\n",
    "                if data.get('section_status') == 'drafted' and data.get('content')\n",
    "            }\n",
    "\n",
    "            if not drafted_sections:\n",
    "                return f\"❌ No drafted sections found to finalize. Please run step 8 first.\"\n",
    "\n",
    "            # Create the final newsletter by combining all sections\n",
    "            today = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "            newsletter_content = f\"# AI News Digest - {today}\\n\\n\"\n",
    "            newsletter_content += f\"*Curated insights from the latest in artificial intelligence*\\n\\n\"\n",
    "            newsletter_content += f\"---\\n\\n\"\n",
    "\n",
    "            # Add table of contents\n",
    "            newsletter_content += \"## Table of Contents\\n\\n\"\n",
    "            for i, section_name in enumerate(drafted_sections.keys(), 1):\n",
    "                newsletter_content += f\"{i}. [{section_name}](#{section_name.lower().replace(' ', '-').replace('&', 'and')})\\n\"\n",
    "            newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "            # Add each section content\n",
    "            for section_name, section_data in drafted_sections.items():\n",
    "                newsletter_content += section_data.get('content', '')\n",
    "                newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "            # Add footer\n",
    "            newsletter_content += \"## About This Newsletter\\n\\n\"\n",
    "            newsletter_content += \"This AI News Digest was automatically curated using our intelligent newsletter agent. \"\n",
    "            newsletter_content += f\"We analyzed {len(state.headline_data)} articles from {len(set(a.get('source', '') for a in state.headline_data))} sources \"\n",
    "            newsletter_content += f\"to bring you the most relevant AI developments.\\n\\n\"\n",
    "            newsletter_content += f\"*Generated on {today}*\\n\"\n",
    "\n",
    "            # Store the final newsletter\n",
    "            state.final_newsletter = newsletter_content\n",
    "\n",
    "            # Calculate final stats\n",
    "            newsletter_length = len(newsletter_content.split())\n",
    "            sections_included = len(drafted_sections)\n",
    "\n",
    "            # Mock quality score based on content metrics\n",
    "            base_quality = 7.0\n",
    "            if sections_included >= 4: base_quality += 0.5\n",
    "            if newsletter_length >= 2000: base_quality += 0.5\n",
    "            if newsletter_length >= 3000: base_quality += 0.5\n",
    "            final_quality_score = min(10.0, base_quality)\n",
    "\n",
    "            # Mark workflow as complete\n",
    "            state.current_step = 9\n",
    "            state.workflow_complete = True\n",
    "\n",
    "            # Also update workflow status for UI\n",
    "            self.workflow_status.complete_step(step_name)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Completed Step 9: Finalized newsletter ({newsletter_length} words)\")\n",
    "\n",
    "            status_msg = f\"🎉 Step 9 completed successfully! Newsletter finalized with {sections_included} sections and {newsletter_length} words.\"\n",
    "            status_msg += f\"\\n⭐ Quality score: {final_quality_score:.1f}/10\"\n",
    "            status_msg += f\"\\n📰 Complete newsletter stored in persistent state\"\n",
    "            status_msg += f\"\\n✅ Workflow complete! All 9 steps finished successfully.\"\n",
    "            return status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            self.workflow_status.error_step(step_name, str(e))\n",
    "            return f\"❌ Step 9 failed: {str(e)}\"\n",
    "\n",
    "    def create_tool(self) -> FunctionTool:\n",
    "        \"\"\"Create a FunctionTool instance following OpenAI Agents SDK conventions\"\"\"\n",
    "        return FunctionTool(\n",
    "            name=\"finalize_newsletter\",\n",
    "            description=\"Execute Step 9: Combine all sections into the final newsletter with formatting and polish. Requires Step 8 to be completed first.\",\n",
    "            params_json_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            },\n",
    "            on_invoke_tool=self._finalize_newsletter\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDB = 'newsagent_logs.db'\n",
    "\n",
    "class NewsletterAgent(Agent[NewsletterAgentState]):\n",
    "    \"\"\"Newsletter agent with persistent state and workflow tools\"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str = \"newsletter_agent\", verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the NewsletterAgent with persistent state\n",
    "\n",
    "        Args:\n",
    "            session_id: Unique identifier for the session (for persistence)\n",
    "            verbose: Enable verbose logging\n",
    "        \"\"\"\n",
    "        # Initialize session for persistence\n",
    "        self.session = SQLiteSession(session_id, \"newsletter_agent.db\")\n",
    "        self.workflow_status = WorkflowStatus()  # Keep for progress tracking UI\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize logger\n",
    "        self.logger = setup_logging(session_id, LOGDB)\n",
    "\n",
    "        # System prompt that guides tool selection based on workflow status\n",
    "        system_prompt = \"\"\"\n",
    "You are an AI newsletter writing agent that executes a 9-step workflow process using tools with persistent state.\n",
    "\n",
    "WORKFLOW OVERVIEW:\n",
    "1. Step 1: Gather URLs - Collect headlines and URLs from various sources\n",
    "2. Step 2: Filter URLs - Filter headlines to AI-related content only\n",
    "3. Step 3: Download Articles - Fetch full article content from URLs\n",
    "4. Step 4: Extract Summaries - Create bullet point summaries of each article\n",
    "5. Step 5: Cluster By Topic - Group articles by thematic topics\n",
    "6. Step 6: Rate Articles - Evaluate article quality and importance\n",
    "7. Step 7: Select Sections - Organize articles into newsletter sections\n",
    "8. Step 8: Draft Sections - Write content for each section\n",
    "9. Step 9: Finalize Newsletter - Combine sections into final newsletter\n",
    "\n",
    "WORKFLOW RESUME LOGIC:\n",
    "- You maintain persistent state between runs and can resume from any step\n",
    "- ALWAYS start by checking workflow status to understand current progress\n",
    "- If current_step >= 1, you can resume from any completed step forward\n",
    "- Steps are idempotent - if a step is already completed, tools will return cached results\n",
    "- When resuming, automatically continue from the next incomplete step\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- ALWAYS start by checking the current workflow status using check_workflow_status\n",
    "- Use inspect_state tool to examine detailed state data when debugging\n",
    "- Execute workflow steps in the correct order using the appropriate tools\n",
    "- Each step has prerequisites - only execute a step if the previous step is completed\n",
    "- If a user asks to \"run all steps\" or \"create the newsletter\", execute all remaining steps in sequence\n",
    "- If a user asks for a specific step, execute only that step (if prerequisites are met)\n",
    "- If a user asks to \"resume\" or \"continue\", start from the next incomplete step\n",
    "- Always check status between steps to ensure proper sequencing\n",
    "- Your state persists between sessions - you can resume work from where you left off\n",
    "\n",
    "TOOL SELECTION STRATEGY:\n",
    "1. First, always use check_workflow_status to understand current state and progress\n",
    "2. If resuming, identify the next step that needs to be executed\n",
    "3. Use the appropriate tool for that step\n",
    "4. After each step, check status again to confirm progress\n",
    "5. Continue until workflow is complete or user request is fulfilled\n",
    "\n",
    "RESUME EXAMPLES:\n",
    "- If current_step=3, next step is step 4 (Extract Summaries)\n",
    "- If current_step=7, next step is step 8 (Draft Sections)\n",
    "- If current_step=9, workflow is complete - no further steps needed\n",
    "\n",
    "Remember: Your state is persistent. You can safely resume from any point. Never skip steps or execute them out of order.\n",
    "\"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            name=\"NewsletterAgent\",\n",
    "            instructions=system_prompt,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            tools=[\n",
    "                WorkflowStatusTool(self.workflow_status, self.logger).create_tool(),\n",
    "                StateInspectionTool(self.verbose, self.logger).create_tool(),\n",
    "                GatherUrlsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                FilterUrlsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                DownloadArticlesTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                ExtractSummariesTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                ClusterByTopicTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                RateArticlesTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                SelectSectionsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                DraftSectionsTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "                FinalizeNewsletterTool(self.workflow_status, self.verbose, self.logger).create_tool(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Initialize default state\n",
    "        self.default_state = NewsletterAgentState()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Initialized NewsletterAgent with persistent state and 9-step workflow\")\n",
    "            print(f\"Session ID: {session_id}\")\n",
    "\n",
    "    async def run_step(self, user_input: str) -> str:\n",
    "        \"\"\"Run a workflow step with persistent state\"\"\"\n",
    "        result = await Runner.run(\n",
    "            self,\n",
    "            user_input,\n",
    "            session=self.session,\n",
    "            context=self.default_state,  # Will load from session if exists\n",
    "            max_turns=50  # Increased for complete 9-step workflow\n",
    "        )\n",
    "        return result.final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_agent = NewsletterAgent(session_id=f\"newsletter_{random.randint(10000000, 99999999)}\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ca518",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Run all the workflow steps in order and create the newsletter\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = await news_agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock context\n",
    "class MockContext:\n",
    "    def __init__(self):\n",
    "        self.context = news_agent.default_state\n",
    "\n",
    "ctx = MockContext()\n",
    "current_state = ctx.context  # From your previous run, or reload it\n",
    "df = current_state.headline_df\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    current_state = news_agent.session.get_state()\n",
    "except:\n",
    "    current_state = news_agent.default_state\n",
    "\n",
    "print(current_state)\n",
    "print()\n",
    "\n",
    "print(f\"Current Step: {current_state.current_step}/9\")\n",
    "print(f\"Workflow Complete: {current_state.workflow_complete}\")\n",
    "print(f\"Progress: {(current_state.current_step/9)*100:.1f}%\")\n",
    "print(f\"Total articles: {len(current_state.headline_data)}\")\n",
    "\n",
    "if current_state.headline_data:\n",
    "    ai_related = sum(1 for a in current_state.headline_data if a.get('ai_related') is True)\n",
    "    print(f\"AI-related articles: {ai_related}\")\n",
    "    print(f\"Summaries: {len(current_state.article_summaries)}\")\n",
    "    print(f\"Clusters: {len(current_state.topic_clusters)}\")\n",
    "    print(f\"Sections: {len(current_state.newsletter_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review slides\n",
    "\n",
    "# review workflow status, move to a moadule\n",
    "# all prints should be logs\n",
    "# section writing and composition will have the critic /optimizer loop\n",
    "# add batch with async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_news_dataframe():\n",
    "    \"\"\"\n",
    "    Creates an empty DataFrame to support headline/article analysis\n",
    "    - URLs, source tracking and metadata\n",
    "    - Topic classification and clustering\n",
    "    - Content quality ratings and rankings\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Empty DataFrame with predefined column structure\n",
    "    \"\"\"\n",
    "\n",
    "    # column structure\n",
    "    column_dict = {\n",
    "        # Core identifiers and source info\n",
    "        'article_id': 'object',              # Unique identifier for each article\n",
    "        'source':     'object',              # Source category\n",
    "        'headline_title': 'object',          # Article headline/title\n",
    "        'original_url': 'object',            # Initial URL before redirects\n",
    "        'final_url': 'object',               # URL after following redirects\n",
    "        'domain_name': 'category',           # Website domain\n",
    "        'site_name': 'category',             # Human-readable site name\n",
    "        'site_reputation_score': 'float32',  # Reputation/trustworthiness score for the site\n",
    "        'keep_flag': 'boolean',\n",
    "\n",
    "        # File paths and storage\n",
    "        'html_file_path': 'object',          # Path to stored HTML content\n",
    "        'text_file_path': 'object',          # Path to extracted text content\n",
    "\n",
    "        # Time information\n",
    "        'last_updated_timestamp': 'datetime64[ns]',  # When article was last updated\n",
    "        'article_age_days': 'int32',         # Age of article in days\n",
    "        'recency_score': 'float32',          # Calculated recency score (higher = more recent)\n",
    "\n",
    "        # Content analysis\n",
    "        'content_summary': 'object',         # Generated summary of article content\n",
    "        'bullet_points': 'object',           # Key points extracted as bullets\n",
    "        'article_length_chars': 'int32',     # Character count of article content\n",
    "\n",
    "        # Rating flags (LLM-generated probabilities)\n",
    "        'is_high_quality': 'float32',        # LLM probability for low-quality content\n",
    "        'is_off_topic': 'float32',           # LLM probability for off-topic content\n",
    "        'is_low_importance': 'float32',      # 1-LLM probability for high-importance content\n",
    "\n",
    "        # Other ratings\n",
    "        'bradley_terry_score': 'float32',    # Bradley-Terry rating from pairwise article comparisons\n",
    "        'bradley_terry_rank': 'int32',       # Ordinal rank based on Bradley-Terry scores (1 = highest rated)\n",
    "        'adjusted_length_score': 'float32',  # Length-adjusted quality score\n",
    "        'final_composite_rating': 'float32', # Final weighted rating combining multiple factors\n",
    "\n",
    "        # Topic classification\n",
    "        'topic_string': 'object',            # Topic labels as comma-separated string\n",
    "        'topic_list': 'object',              # Topic labels as list/array structure (same topics, different format)\n",
    "\n",
    "        # Organization and clustering (HDBSCAN-based)\n",
    "        'display_order': 'int32',            # Order for display/presentation\n",
    "        'cluster_id': 'int32',               # HDBSCAN cluster identifier (-1 = noise/outlier)\n",
    "        'cluster_label': 'category'          # Human-readable cluster name/description\n",
    "    }\n",
    "\n",
    "    # Create empty DataFrame from column dictionary\n",
    "    df = pd.DataFrame(columns=list(column_dict.keys())).astype(column_dict)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a22a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsletterState:\n",
    "    \"\"\"\n",
    "    Maintains session state for the OpenAI Agents SDK workflow.\n",
    "\n",
    "    Attributes:\n",
    "        headline_df: DataFrame containing headline data for processing\n",
    "        sources_file: Path to YAML file containing source configurations\n",
    "        sources: Dictionary of source configurations loaded from YAML\n",
    "        cluster_topics: List of clean topic names for headline categorization\n",
    "        max_edits: Maximum number of critic optimizer editing iterations allowed\n",
    "        edit_complete: Boolean flag indicating if editing process is finished\n",
    "        n_browsers: Number of concurrent Playwright browser instances for downloads\n",
    "    \"\"\"\n",
    "\n",
    "    status: WorkflowStatus = WorkflowStatus()\n",
    "    headline_df: pd.DataFrame = field(default_factory=create_news_dataframe)\n",
    "    sources_file: str = field(default=\"sources.yaml\")\n",
    "    sources: Dict[str, Any] = field(default_factory=dict)\n",
    "    cluster_topics: List[str] = field(default_factory=list)\n",
    "    max_edits: int = field(default=3)\n",
    "    edit_complete: bool = field(default=False)\n",
    "    n_browsers: int = field(default=8)\n",
    "    verbose: bool = field(default=True)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization validation and setup.\n",
    "\n",
    "        Validates that the configuration makes sense and performs\n",
    "        any necessary initialization steps.\n",
    "        \"\"\"\n",
    "        # Validate max_edits is reasonable\n",
    "        if self.max_edits < 1 or self.max_edits > 10:\n",
    "            raise ValueError(f\"max_edits should be between 1-10, got {self.max_edits}\")\n",
    "\n",
    "        # Validate n_browsers is reasonable\n",
    "        if self.n_browsers < 1 or self.n_browsers > 32:\n",
    "            raise ValueError(f\"n_browsers should be between 1-32, got {self.n_browsers}\")\n",
    "\n",
    "        # Validate sources_file exists and load sources from file automatically\n",
    "        try:\n",
    "            sources_path = Path(self.sources_file)\n",
    "            with open(sources_path, 'r', encoding='utf-8') as file:\n",
    "                self.sources = yaml.safe_load(file) or {}\n",
    "            if self.verbose:\n",
    "                print(f\"Loaded {len(self.sources)} sources from {self.sources_file}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Sources file not found: {self.sources_file}\")\n",
    "        except yaml.YAMLError as e:\n",
    "            raise ValueError(f\"Error parsing YAML file {self.sources_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = NewsletterState()\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, SQLiteSession, function_tool, RunContextWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c475f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsletterAgent(Agent[NewsletterState]):\n",
    "    \"\"\"AI newsletter writing agent with structured workflow\"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str = \"newsletter_agent\"):\n",
    "        self.session = SQLiteSession(session_id, \"newsletter.db\")\n",
    "        self.state = NewsletterState()\n",
    "\n",
    "        super().__init__(\n",
    "            name=\"AINewsletterAgent\",\n",
    "            instructions=\"\"\"\n",
    "            You are an AI newsletter writing agent. Your role is to:\n",
    "            1. Scrape headlines and URLs from various sources\n",
    "            2. Filter the headlines to ones that are about AI\n",
    "            3. Fetch the URLs and save them as plain text\n",
    "            4. Summarize each article to 3 bullet points containing the key facts\n",
    "            5. Extract topics from each article and cluster articles by topic\n",
    "            6. Rate each article according to the provided rubric\n",
    "            7. Identify 6-15 thematic sections + \"Other News\", assign articles to sections and deduplicate\n",
    "            8. Write each section\n",
    "            9. Combine sections and polish\n",
    "\n",
    "            Use the tools available to accomplish these tasks in order.\n",
    "            Always maintain context about workflow progress and data.\n",
    "            Guide users through the workflow steps systematically.\n",
    "            \"\"\",\n",
    "            tools=[\n",
    "                self.step1_scrape_headlines,\n",
    "                self.step2_filter_ai_headlines,\n",
    "                self.step3_fetch_article_texts,\n",
    "                self.step4_summarize_articles,\n",
    "                self.step5_extract_and_cluster_topics,\n",
    "                self.step6_rate_articles,\n",
    "                self.step7_organize_sections,\n",
    "                self.step8_write_sections,\n",
    "                self.step9_finalize_newsletter,\n",
    "                self.get_workflow_status,\n",
    "                self.run_complete_workflow,\n",
    "                self.reset_workflow\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @function_tool\n",
    "    async def step1_scrape_headlines(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        sources: List[str] = None,\n",
    "        max_articles_per_source: int = 50\n",
    "    ) -> str:\n",
    "        \"\"\"Step 1: Scrape headlines and URLs from various sources\"\"\"\n",
    "        if sources is None:\n",
    "            sources = [\"techcrunch\", \"arstechnica\", \"theverge\", \"wired\", \"venturebeat\"]\n",
    "\n",
    "        scraped_data = []\n",
    "\n",
    "        # Mock scraping implementation (replace with real RSS/API scraping)\n",
    "        for source in sources:\n",
    "            for i in range(max_articles_per_source):\n",
    "                article = {\n",
    "                    'title': f\"{source} AI Article {i+1}: Latest developments in machine learning\",\n",
    "                    'url': f\"https://{source}.com/ai-article-{i+1}\",\n",
    "                    'source': source,\n",
    "                    'published_at': (datetime.now() - timedelta(hours=i)).isoformat(),\n",
    "                    'description': f\"AI-related content from {source}\"\n",
    "                }\n",
    "                scraped_data.append(article)\n",
    "\n",
    "        wrapper.context.raw_headlines = scraped_data\n",
    "        wrapper.context.scraped_urls = [article['url'] for article in scraped_data]\n",
    "        wrapper.context.current_step = 1\n",
    "\n",
    "        return f\"✅ Step 1 Complete: Scraped {len(scraped_data)} headlines from {len(sources)} sources\"\n",
    "\n",
    "\n",
    "    @function_tool\n",
    "    async def step2_filter_ai_content(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        ai_keywords: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Step 2: Filter headlines to AI-related content only\"\"\"\n",
    "        if not wrapper.context.raw_headlines:\n",
    "            return \"❌ No headlines to filter. Run step 1 first.\"\n",
    "\n",
    "        if ai_keywords is None:\n",
    "            ai_keywords = [\n",
    "                'ai', 'artificial intelligence', 'machine learning', 'deep learning',\n",
    "                'neural network', 'llm', 'gpt', 'transformer', 'chatbot', 'automation',\n",
    "                'computer vision', 'nlp', 'natural language', 'algorithm', 'model'\n",
    "            ]\n",
    "\n",
    "        ai_articles = []\n",
    "        for article in wrapper.context.raw_headlines:\n",
    "            title_lower = article['title'].lower()\n",
    "            desc_lower = article['description'].lower()\n",
    "\n",
    "            # Check if any AI keywords are present\n",
    "            if any(keyword in title_lower or keyword in desc_lower for keyword in ai_keywords):\n",
    "                ai_articles.append(article)\n",
    "\n",
    "        wrapper.context.ai_headlines = pd.DataFrame(ai_articles)\n",
    "        wrapper.context.current_step = 2\n",
    "\n",
    "        return f\"✅ Step 2 Complete: Filtered to {len(ai_articles)} AI-related headlines from {len(wrapper.context.raw_headlines)} total\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step3_fetch_article_texts(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 3: Fetch full article texts from URLs\"\"\"\n",
    "        if wrapper.context.ai_headlines.empty:\n",
    "            return \"❌ No AI headlines to fetch. Complete steps 1-2 first.\"\n",
    "\n",
    "        # Mock article fetching (replace with actual web scraping)\n",
    "        article_texts = {}\n",
    "\n",
    "        for _, row in wrapper.context.ai_headlines.iterrows():\n",
    "            url = row['url']\n",
    "            # Mock article content\n",
    "            article_texts[url] = f\"\"\"\n",
    "            {row['title']}\n",
    "\n",
    "            This is a mock article about AI developments. In a real implementation,\n",
    "            you would use libraries like requests + BeautifulSoup or newspaper3k\n",
    "            to extract the full article text from the URL.\n",
    "\n",
    "            Key points about this AI story:\n",
    "            - Advancement in machine learning techniques\n",
    "            - Impact on industry applications\n",
    "            - Future implications for AI development\n",
    "\n",
    "            This content would be much longer in practice, containing the full\n",
    "            article text that needs to be summarized and analyzed.\n",
    "            \"\"\"\n",
    "\n",
    "        wrapper.context.article_texts = article_texts\n",
    "        wrapper.context.current_step = 3\n",
    "\n",
    "        return f\"✅ Step 3 Complete: Fetched full text for {len(article_texts)} articles\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step4_summarize_articles(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 4: Summarize each article to 3 key bullet points\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No article texts to summarize. Complete steps 1-3 first.\"\n",
    "\n",
    "        summaries = {}\n",
    "\n",
    "        for url, text in wrapper.context.article_texts.items():\n",
    "            # Mock summarization (replace with actual LLM summarization)\n",
    "            summaries[url] = [\n",
    "                \"• Key development in AI technology or research\",\n",
    "                \"• Practical implications for businesses or developers\",\n",
    "                \"• Future outlook or next steps in this area\"\n",
    "            ]\n",
    "\n",
    "        wrapper.context.article_summaries = summaries\n",
    "        wrapper.context.current_step = 4\n",
    "\n",
    "        return f\"✅ Step 4 Complete: Generated 3-point summaries for {len(summaries)} articles\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step5_extract_and_cluster_topics(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        max_clusters: int = 8\n",
    "    ) -> str:\n",
    "        \"\"\"Step 5: Extract topics and cluster articles\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No articles to analyze. Complete steps 1-4 first.\"\n",
    "\n",
    "        # Extract topics from each article (mock implementation)\n",
    "        article_topics = {}\n",
    "        all_topics = []\n",
    "\n",
    "        for url, text in wrapper.context.article_texts.items():\n",
    "            # Mock topic extraction (replace with NLP)\n",
    "            topics = ['machine learning', 'business applications', 'research', 'ethics']\n",
    "            article_topics[url] = topics\n",
    "            all_topics.extend(topics)\n",
    "\n",
    "        # Cluster articles by common topics\n",
    "        topic_counts = Counter(all_topics)\n",
    "        main_topics = [topic for topic, count in topic_counts.most_common(max_clusters)]\n",
    "\n",
    "        topic_clusters = {}\n",
    "        for topic in main_topics:\n",
    "            topic_clusters[topic] = [\n",
    "                url for url, topics in article_topics.items()\n",
    "                if topic in topics\n",
    "            ]\n",
    "\n",
    "        wrapper.context.article_topics = article_topics\n",
    "        wrapper.context.topic_clusters = topic_clusters\n",
    "        wrapper.context.current_step = 5\n",
    "\n",
    "        return f\"✅ Step 5 Complete: Extracted topics and created {len(topic_clusters)} clusters\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step6_rate_articles(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        custom_rubric: Dict[str, str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Step 6: Rate articles according to rubric\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No articles to rate. Complete previous steps first.\"\n",
    "\n",
    "        if custom_rubric:\n",
    "            wrapper.context.rating_rubric.update(custom_rubric)\n",
    "\n",
    "        # Mock rating (replace with actual evaluation)\n",
    "        ratings = {}\n",
    "        for url in wrapper.context.article_texts.keys():\n",
    "            # Mock scoring based on rubric criteria\n",
    "            relevance_score = 0.8\n",
    "            novelty_score = 0.7\n",
    "            impact_score = 0.9\n",
    "            credibility_score = 0.8\n",
    "\n",
    "            overall_rating = (relevance_score + novelty_score + impact_score + credibility_score) / 4\n",
    "            ratings[url] = overall_rating\n",
    "\n",
    "        wrapper.context.article_ratings = ratings\n",
    "        wrapper.context.current_step = 6\n",
    "\n",
    "        avg_rating = sum(ratings.values()) / len(ratings)\n",
    "        return f\"✅ Step 6 Complete: Rated {len(ratings)} articles. Average rating: {avg_rating:.2f}\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step7_organize_sections(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        target_sections: int = 10\n",
    "    ) -> str:\n",
    "        \"\"\"Step 7: Organize articles into thematic sections\"\"\"\n",
    "        if not wrapper.context.topic_clusters:\n",
    "            return \"❌ No topic clusters available. Complete steps 1-6 first.\"\n",
    "\n",
    "        # Create thematic sections based on clusters and ratings\n",
    "        sections = {}\n",
    "\n",
    "        # Main thematic sections from top clusters\n",
    "        top_clusters = sorted(\n",
    "            wrapper.context.topic_clusters.items(),\n",
    "            key=lambda x: len(x[1]),  # Sort by cluster size\n",
    "            reverse=True\n",
    "        )[:target_sections-1]  # Reserve space for \"Other News\"\n",
    "\n",
    "        for topic, urls in top_clusters:\n",
    "            # Only include high-rated articles\n",
    "            high_rated_urls = [\n",
    "                url for url in urls\n",
    "                if wrapper.context.article_ratings.get(url, 0) >= 0.6\n",
    "            ]\n",
    "            if high_rated_urls:\n",
    "                section_name = topic.title().replace('_', ' ')\n",
    "                sections[section_name] = high_rated_urls\n",
    "\n",
    "        # \"Other News\" section for remaining articles\n",
    "        assigned_urls = set()\n",
    "        for urls in sections.values():\n",
    "            assigned_urls.update(urls)\n",
    "\n",
    "        other_urls = [\n",
    "            url for url in wrapper.context.article_texts.keys()\n",
    "            if url not in assigned_urls and wrapper.context.article_ratings.get(url, 0) >= 0.5\n",
    "        ]\n",
    "\n",
    "        if other_urls:\n",
    "            sections[\"Other News\"] = other_urls\n",
    "\n",
    "        wrapper.context.thematic_sections = sections\n",
    "        wrapper.context.section_names = list(sections.keys())\n",
    "        wrapper.context.current_step = 7\n",
    "\n",
    "        section_summary = \"\\n\".join([\n",
    "            f\"• {name}: {len(urls)} articles\"\n",
    "            for name, urls in sections.items()\n",
    "        ])\n",
    "\n",
    "        return f\"✅ Step 7 Complete: Organized into {len(sections)} sections:\\n{section_summary}\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step8_write_sections(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 8: Write content for each thematic section\"\"\"\n",
    "        if not wrapper.context.thematic_sections:\n",
    "            return \"❌ No sections to write. Complete steps 1-7 first.\"\n",
    "\n",
    "        section_drafts = {}\n",
    "\n",
    "        for section_name, urls in wrapper.context.thematic_sections.items():\n",
    "            # Gather content for this section\n",
    "            section_articles = []\n",
    "\n",
    "            for url in urls:\n",
    "                summary = wrapper.context.article_summaries.get(url, [])\n",
    "                rating = wrapper.context.article_ratings.get(url, 0)\n",
    "\n",
    "                # Get article title from DataFrame\n",
    "                article_row = wrapper.context.ai_headlines[\n",
    "                    wrapper.context.ai_headlines['url'] == url\n",
    "                ]\n",
    "                title = article_row['title'].iloc[0] if not article_row.empty else \"Unknown Title\"\n",
    "\n",
    "                section_articles.append({\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'summary': summary,\n",
    "                    'rating': rating\n",
    "                })\n",
    "\n",
    "            # Write section content (mock implementation)\n",
    "            section_content = f\"## {section_name}\\n\\n\"\n",
    "\n",
    "            for article in sorted(section_articles, key=lambda x: x['rating'], reverse=True):\n",
    "                section_content += f\"**{article['title']}**\\n\"\n",
    "                for bullet in article['summary']:\n",
    "                    section_content += f\"{bullet}\\n\"\n",
    "                section_content += f\"[Read more]({article['url']})\\n\\n\"\n",
    "\n",
    "            section_drafts[section_name] = section_content\n",
    "\n",
    "        wrapper.context.section_drafts = section_drafts\n",
    "        wrapper.context.current_step = 8\n",
    "\n",
    "        return f\"✅ Step 8 Complete: Wrote content for {len(section_drafts)} sections\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step9_finalize_newsletter(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        newsletter_title: str = \"AI Weekly Newsletter\"\n",
    "    ) -> str:\n",
    "        \"\"\"Step 9: Combine sections and polish final newsletter\"\"\"\n",
    "        if not wrapper.context.section_drafts:\n",
    "            return \"❌ No section drafts available. Complete steps 1-8 first.\"\n",
    "\n",
    "        # Combine all sections\n",
    "        newsletter_content = f\"# {newsletter_title}\\n\"\n",
    "        newsletter_content += f\"*Generated on {datetime.now().strftime('%B %d, %Y')}*\\n\\n\"\n",
    "\n",
    "        # Add introduction\n",
    "        total_articles = len(wrapper.context.article_texts)\n",
    "        newsletter_content += f\"This week's AI newsletter covers {total_articles} key developments across {len(wrapper.context.section_drafts)} areas of AI.\\n\\n\"\n",
    "\n",
    "        # Add each section\n",
    "        for section_name in wrapper.context.section_names:\n",
    "            if section_name in wrapper.context.section_drafts:\n",
    "                newsletter_content += wrapper.context.section_drafts[section_name]\n",
    "                newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "        # Add footer\n",
    "        newsletter_content += \"*Thank you for reading! This newsletter was generated using AI curation and analysis.*\"\n",
    "\n",
    "        wrapper.context.final_newsletter = newsletter_content\n",
    "        wrapper.context.workflow_complete = True\n",
    "        wrapper.context.current_step = 9\n",
    "\n",
    "        return f\"✅ Step 9 Complete: Finalized newsletter with {len(wrapper.context.section_drafts)} sections\"\n",
    "\n",
    "    @function_tool\n",
    "    async def get_workflow_status(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Get detailed workflow progress status\"\"\"\n",
    "        state = wrapper.context\n",
    "\n",
    "        status = {\n",
    "            'current_step': state.current_step,\n",
    "            'steps_completed': [\n",
    "                f\"1. Scraping: {len(state.raw_headlines)} headlines\" if state.raw_headlines else \"1. Scraping: Pending\",\n",
    "                f\"2. AI Filtering: {len(state.ai_headlines)} AI articles\" if not state.ai_headlines.empty else \"2. AI Filtering: Pending\",\n",
    "                f\"3. Text Fetching: {len(state.article_texts)} articles\" if state.article_texts else \"3. Text Fetching: Pending\",\n",
    "                f\"4. Summarization: {len(state.article_summaries)} summaries\" if state.article_summaries else \"4. Summarization: Pending\",\n",
    "                f\"5. Topic Clustering: {len(state.topic_clusters)} clusters\" if state.topic_clusters else \"5. Topic Clustering: Pending\",\n",
    "                f\"6. Article Rating: {len(state.article_ratings)} rated\" if state.article_ratings else \"6. Article Rating: Pending\",\n",
    "                f\"7. Section Organization: {len(state.thematic_sections)} sections\" if state.thematic_sections else \"7. Section Organization: Pending\",\n",
    "                f\"8. Section Writing: {len(state.section_drafts)} drafts\" if state.section_drafts else \"8. Section Writing: Pending\",\n",
    "                f\"9. Newsletter Finalization: {'Complete' if state.final_newsletter else 'Pending'}\"\n",
    "            ],\n",
    "            'workflow_complete': state.workflow_complete\n",
    "        }\n",
    "\n",
    "        return f\"Newsletter Workflow Status:\\n\\n\" + \"\\n\".join(status['steps_completed'])\n",
    "\n",
    "    @function_tool\n",
    "    async def run_complete_workflow(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        sources: List[str] = None,\n",
    "        ai_keywords: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Run the complete 9-step workflow automatically\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Execute each step in sequence\n",
    "        result1 = await self.step1_scrape_headlines(wrapper, sources)\n",
    "        results.append(result1)\n",
    "\n",
    "        result2 = await self.step2_filter_ai_content(wrapper, ai_keywords)\n",
    "        results.append(result2)\n",
    "\n",
    "        result3 = await self.step3_fetch_article_texts(wrapper)\n",
    "        results.append(result3)\n",
    "\n",
    "        result4 = await self.step4_summarize_articles(wrapper)\n",
    "        results.append(result4)\n",
    "\n",
    "        result5 = await self.step5_extract_and_cluster_topics(wrapper)\n",
    "        results.append(result5)\n",
    "\n",
    "        result6 = await self.step6_rate_articles(wrapper)\n",
    "        results.append(result6)\n",
    "\n",
    "        result7 = await self.step7_organize_sections(wrapper)\n",
    "        results.append(result7)\n",
    "\n",
    "        result8 = await self.step8_write_sections(wrapper)\n",
    "        results.append(result8)\n",
    "\n",
    "        result9 = await self.step9_finalize_newsletter(wrapper)\n",
    "        results.append(result9)\n",
    "\n",
    "        newsletter_length = len(wrapper.context.final_newsletter)\n",
    "\n",
    "        return \"\\n\".join(results) + f\"\\n\\n🎉 Complete workflow finished! Newsletter ready ({newsletter_length} characters)\"\n",
    "\n",
    "    @function_tool\n",
    "    async def reset_workflow(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Reset workflow to start fresh\"\"\"\n",
    "        wrapper.context.__dict__.update(NewsletterState().__dict__)\n",
    "        return \"🔄 Workflow reset. Ready to start step 1.\"\n",
    "\n",
    "    @function_tool\n",
    "    async def get_newsletter_preview(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        max_chars: int = 500\n",
    "    ) -> str:\n",
    "        \"\"\"Get a preview of the current newsletter\"\"\"\n",
    "        if not wrapper.context.final_newsletter:\n",
    "            return \"Newsletter not ready yet. Complete the full workflow first.\"\n",
    "\n",
    "        preview = wrapper.context.final_newsletter[:max_chars]\n",
    "        if len(wrapper.context.final_newsletter) > max_chars:\n",
    "            preview += \"...\"\n",
    "\n",
    "        return f\"Newsletter Preview:\\n\\n{preview}\"\n",
    "\n",
    "    async def run_step(self, user_input: str) -> str:\n",
    "        \"\"\"Run a workflow step with persistent state\"\"\"\n",
    "        result = await Runner.run(\n",
    "            self,\n",
    "            user_input,\n",
    "            session=self.session,\n",
    "            context=self.state\n",
    "        )\n",
    "        return result.final_output\n",
    "\n",
    "    def save_newsletter(self, filepath: str = None):\n",
    "        \"\"\"Save the final newsletter to file\"\"\"\n",
    "        if not self.state.final_newsletter:\n",
    "            print(\"No newsletter to save. Complete workflow first.\")\n",
    "            return\n",
    "\n",
    "        if filepath is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"ai_newsletter_{timestamp}.md\"\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(self.state.final_newsletter)\n",
    "\n",
    "        print(f\"Newsletter saved to {filepath}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"http://localhost:8787/v1\",\n",
    "  api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "  default_headers={\"x-portkey-provider\": \"openai\"}\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from portkey_ai import Portkey\n",
    "\n",
    "client = Portkey(\n",
    "    provider=\"openai\",\n",
    "    Authorization=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Example: Send a chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61793fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State of the LangGraph agent.\n",
    "    Each node in the graph is a function that takes the current state and returns the updated state.\n",
    "    \"\"\"\n",
    "\n",
    "    # the current working set of headlines (pandas dataframe not supported)\n",
    "    AIdf: list[dict]\n",
    "    # ignore stories before this date for deduplication (force reprocess since)\n",
    "    model_low: str     # cheap fast model like gpt-4o-mini or flash\n",
    "    model_medium: str  # medium model like gpt-4o or gemini-1.5-pro\n",
    "    model_high: str    # slow expensive thinking model like o3-mini\n",
    "    sources: dict  # sources to scrap\n",
    "    sources_reverse: dict[str, str]  # map file names to sources\n",
    "\n",
    "state = AgentState()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES_FILE = \"sources.yaml\"\n",
    "\n",
    "def initialize(state, sources_file=SOURCES_FILE) -> Dict[str, Any]:\n",
    "    \"\"\"Read and parse the sources.yaml file.\"\"\"\n",
    "    try:\n",
    "        with open(sources_file, 'r', encoding='utf-8') as file:\n",
    "            state[\"sources\"] =  yaml.safe_load(file)\n",
    "        state[\"sources_reverse\"] = {v[\"title\"]+\".html\":k for k,v in state[\"sources\"].items()}\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Sources file '{self.sources_file}' not found\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Error parsing YAML file: {e}\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4112b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = initialize(state)\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdk",
   "language": "python",
   "name": "asdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
