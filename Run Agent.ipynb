{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71d27e",
   "metadata": {},
   "source": [
    "# Test OpenAI Agents SDK\n",
    "- Implement a workflow to write a daily AI newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d549de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import pydantic\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "from typing import Dict, TypedDict, Type, List, Optional, Any, Iterable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import hdbscan\n",
    "\n",
    "import openai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import agents\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "from agents import (Agent, Runner, Tool, OpenAIResponsesModel, \n",
    "                    ModelSettings, FunctionTool, InputGuardrail, GuardrailFunctionOutput,\n",
    "                    SQLiteSession, set_default_openai_api, set_default_openai_client\n",
    "                   )\n",
    "\n",
    "\n",
    "import tenacity\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from log_handler import SQLiteLogHandler, setup_sqlite_logging, sanitize_error_for_logging\n",
    "from config import LOGDB\n",
    "from llm import LLMagent, LangfuseClient  # methods to apply prompts async to large batches\n",
    "from db import Url \n",
    "\n",
    "from fetch import Fetcher # fetch news urls\n",
    "from newsletter_state import NewsletterAgentState, StepStatus\n",
    "from news_agent import NewsletterAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cea80dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI:            1.109.0\n",
      "OpenAI Agents SDK  0.3.1\n",
      "Pydantic           2.11.9\n"
     ]
    }
   ],
   "source": [
    "print(f\"OpenAI:            {openai.__version__}\")\n",
    "print(f\"OpenAI Agents SDK  {agents.__version__}\")\n",
    "print(f\"Pydantic           {pydantic.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153c333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "# to run async in jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# verbose OpenAI console logging if something doesn't work\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# openai_logger = logging.getLogger(\"openai\")\n",
    "# openai_logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686ed01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:52:57 | NewsletterAgent.newsletter_agent | INFO | Test info message\n",
      "11:52:57 | NewsletterAgent.newsletter_agent | WARNING | Test warning message\n",
      "11:52:57 | NewsletterAgent.newsletter_agent | ERROR | Test error message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'log with some bad stuff for the filter: [API_KEY_REDACTED]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modules create a default logger, or we can pass this logger\n",
    "\n",
    "def setup_logging(session_id: str = \"default\", db_path: str = \"agent_logs.db\") -> logging.Logger:\n",
    "    \"\"\"Set up logging to console and SQLite database.\"\"\"\n",
    "\n",
    "    # Create logger\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logger = logging.getLogger(f\"NewsletterAgent.{session_id}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # SQLite handler\n",
    "    sqlite_handler = SQLiteLogHandler(db_path)\n",
    "    sqlite_handler.setLevel(logging.INFO)\n",
    "    sqlite_formatter = logging.Formatter('%(message)s')\n",
    "    sqlite_handler.setFormatter(sqlite_formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(sqlite_handler)\n",
    "\n",
    "    # Prevent propagation to root logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(\"newsletter_agent\", \"test_logs.db\")\n",
    "\n",
    "# Log some test messages\n",
    "logger.info(\"Test info message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.warning(\"Test warning message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.error(\"Test error message\", extra={\n",
    "    'step_name': 'error_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "sanitize_error_for_logging(\"log with some bad stuff for the filter: sk-proj-123456789012345678901234567890123456789012345678\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdad2f",
   "metadata": {},
   "source": [
    "# Run Agent Worfklow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699cb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating NewsletterAgent...\n",
      "session_id is not defined\n",
      "test_newsletter_20250925115257412043\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Creating NewsletterAgent...\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
    "\n",
    "# Set up OpenAI client for the agents SDK\n",
    "set_default_openai_client(AsyncOpenAI(api_key=api_key))\n",
    "\n",
    "# set up state\n",
    "# session_id = 'test_newsletter_20250923174350688839'\n",
    "# step_name = 'step_05_cluster_by_topic'\n",
    "# del session_id\n",
    "\n",
    "do_download=False\n",
    "process_since='2025-09-24 18:00:00'\n",
    "\n",
    "# Create agent with persistent state\n",
    "if 'session_id' in vars():\n",
    "    # load state from db for session_id and state\n",
    "    print(\"session_id is defined\")\n",
    "    print(session_id)\n",
    "    state = NewsletterAgentState(session_id=session_id, \n",
    "                                 db_path=\"newsletter_agent.db\", \n",
    "                                 do_download=do_download,\n",
    "                                 process_since=process_since)\n",
    "    state = state.load_from_db(step_name)\n",
    "    agent = NewsletterAgent(session_id=session_id, state=state, verbose=True, timeout=30)    \n",
    "else:\n",
    "    # create new session\n",
    "    print(\"session_id is not defined\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")    \n",
    "    session_id = f\"test_newsletter_{timestamp}\"\n",
    "    print(session_id)\n",
    "    state = NewsletterAgentState(session_id=session_id, \n",
    "                                 db_path=\"newsletter_agent.db\",\n",
    "                                 do_download=do_download,\n",
    "                                 process_since=process_since\n",
    "                                ) \n",
    "    agent = NewsletterAgent(session_id=session_id, state=state, verbose=False, timeout=30)\n",
    "    state.serialize_to_db(\"initialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f05aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headlines': {'total': 0},\n",
       " 'sources': {'config_file': 'sources.yaml', 'loaded_sources': 0},\n",
       " 'topics': {'cluster_topics': 0, 'topics': []},\n",
       " 'workflow': {'current_step': 'step_01_fetch_urls',\n",
       "  'workflow_complete': False,\n",
       "  'workflow_status': 'not_started',\n",
       "  'workflow_status_message': '',\n",
       "  'progress_percentage': 0.0,\n",
       "  'max_edits': 2,\n",
       "  'concurrency': 16},\n",
       " 'processing': {'topic_clusters': 0,\n",
       "  'newsletter_sections': 0,\n",
       "  'final_newsletter_length': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fac5a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'step_01_fetch_urls'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.get_current_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1abddb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù User prompt: 'Show the workflow status'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:06 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting check_workflow_status\n",
      "11:53:06 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Completed check_workflow_status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚è±Ô∏è  Total execution time: 6.57s\n",
      "üìä Final result:\n",
      "Current workflow status:\n",
      "\n",
      "- Progress: 0.0% (0/9 complete)\n",
      "- Status summary: 0 complete, 0 started, 0 failed, 9 not started\n",
      "- Next step: Step 1 ‚Äî Fetch URLs\n",
      "\n",
      "Step details:\n",
      "- Step 1: Fetch Urls ‚Äî not_started\n",
      "- Step 2: Filter Urls ‚Äî not_started\n",
      "- Step 3: Download Articles ‚Äî not_started\n",
      "- Step 4: Extract Summaries ‚Äî not_started\n",
      "- Step 5: Cluster By Topic ‚Äî not_started\n",
      "- Step 6: Rate Articles ‚Äî not_started\n",
      "- Step 7: Select Sections ‚Äî not_started\n",
      "- Step 8: Draft Sections ‚Äî not_started\n",
      "- Step 9: Finalize Newsletter ‚Äî not_started\n",
      "\n",
      "What would you like me to do next? (Run all steps, start from a specific step, or resume/continue.)\n"
     ]
    }
   ],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Show the workflow status\"\n",
    "\n",
    "print(f\"\\nüìù User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e89a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù User prompt: 'Run step 1, fetch urls'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:14 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting check_workflow_status\n",
      "11:53:14 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Completed check_workflow_status\n",
      "11:53:16 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting Step 1: Gather URLs\n",
      "2025-09-25 11:53:16,009 - fetcher_5437863184 - INFO - [fetcher_init] Loading sources from sources.yaml\n",
      "2025-09-25 11:53:16,022 - fetcher_5437863184 - INFO - [fetcher_init] Loaded 17 sources: 7 RSS, 9 HTML, 1 API\n",
      "2025-09-25 11:53:16,024 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Ars Technica': type=RSS, url=https://arstechnica.com/ai/\n",
      "2025-09-25 11:53:16,024 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Bloomberg': type=RSS, url=https://www.bloomberg.com/ai\n",
      "2025-09-25 11:53:16,025 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Business Insider': type=html, url=https://www.businessinsider.com/tech\n",
      "2025-09-25 11:53:16,025 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'FT': type=RSS, url=https://www.ft.com/artificial-intelligence\n",
      "2025-09-25 11:53:16,026 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Feedly AI': type=html, url=https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ\n",
      "2025-09-25 11:53:16,026 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Hacker News': type=RSS, url=https://news.ycombinator.com/\n",
      "2025-09-25 11:53:16,027 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'HackerNoon': type=RSS, url=https://hackernoon.com/tagged/ai\n",
      "2025-09-25 11:53:16,027 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'New York Times': type=RSS, url=https://www.nytimes.com/section/technology\n",
      "2025-09-25 11:53:16,027 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Reddit': type=RSS, url=https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day\n",
      "2025-09-25 11:53:16,028 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Techmeme': type=RSS, url=https://www.techmeme.com/river\n",
      "2025-09-25 11:53:16,028 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'The Register': type=RSS, url=https://www.theregister.com/software/ai_ml/\n",
      "2025-09-25 11:53:16,029 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'The Verge': type=RSS, url=https://www.theverge.com/ai-artificial-intelligence\n",
      "2025-09-25 11:53:16,029 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'VentureBeat': type=RSS, url=https://venturebeat.com/category/ai/\n",
      "2025-09-25 11:53:16,029 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'WSJ': type=RSS, url=https://www.wsj.com/tech/ai\n",
      "2025-09-25 11:53:16,030 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'Washington Post': type=RSS, url=https://www.washingtonpost.com/technology/innovations/\n",
      "2025-09-25 11:53:16,030 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'The Guardian': type=RSS, url=https://www.theguardian.com/uk/technology\n",
      "2025-09-25 11:53:16,031 - fetcher_5437863184 - DEBUG - [fetcher_sources] Source 'NewsAPI': type=rest, url=https://newsapi.org/v2/everything\n",
      "2025-09-25 11:53:16,031 - fetcher_5437863184 - INFO - [fetcher_init] Fetcher initialized with max_concurrent=8\n",
      "2025-09-25 11:53:16,032 - fetcher_5437863184 - INFO - [fetch_all] Starting fetch_all for 17 sources\n",
      "2025-09-25 11:53:16,033 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from Ars Technica: https://arstechnica.com/ai/feed/\n",
      "2025-09-25 11:53:16,035 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from Bloomberg: https://www.bloomberg.com/ai\n",
      "2025-09-25 11:53:16,035 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/Bloomberg.html\n",
      "2025-09-25 11:53:16,074 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/Bloomberg.html\n",
      "2025-09-25 11:53:16,075 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for Bloomberg: 28 articles\n",
      "2025-09-25 11:53:16,075 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from Business Insider: https://www.businessinsider.com/tech\n",
      "2025-09-25 11:53:16,075 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/Business_Insider.html\n",
      "2025-09-25 11:53:16,103 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/Business_Insider.html\n",
      "2025-09-25 11:53:16,103 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for Business Insider: 17 articles\n",
      "2025-09-25 11:53:16,103 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from FT: https://www.ft.com/artificial-intelligence\n",
      "2025-09-25 11:53:16,103 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/FT.html\n",
      "2025-09-25 11:53:16,130 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/FT.html\n",
      "2025-09-25 11:53:16,130 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for FT: 99 articles\n",
      "2025-09-25 11:53:16,130 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from Feedly AI: https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ\n",
      "2025-09-25 11:53:16,131 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/Feedly_AI.html\n",
      "2025-09-25 11:53:16,168 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/Feedly_AI.html\n",
      "2025-09-25 11:53:16,169 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for Feedly AI: 75 articles\n",
      "2025-09-25 11:53:16,169 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from Hacker News: https://news.ycombinator.com/rss\n",
      "2025-09-25 11:53:16,169 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from HackerNoon: https://hackernoon.com/tagged/ai/feed\n",
      "2025-09-25 11:53:16,169 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from New York Times: https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\n",
      "2025-09-25 11:53:16,170 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from Reddit: https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day\n",
      "2025-09-25 11:53:16,170 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/Reddit.html\n",
      "2025-09-25 11:53:16,195 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/Reddit.html\n",
      "2025-09-25 11:53:16,196 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for Reddit: 62 articles\n",
      "2025-09-25 11:53:16,196 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from Techmeme: https://www.techmeme.com/feed.xml\n",
      "2025-09-25 11:53:16,196 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from The Register: https://www.theregister.com/software/ai_ml/headlines.atom\n",
      "2025-09-25 11:53:16,196 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from The Verge: https://www.theverge.com/ai-artificial-intelligence\n",
      "2025-09-25 11:53:16,196 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/The_Verge.html\n",
      "2025-09-25 11:53:16,332 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/The_Verge.html\n",
      "2025-09-25 11:53:16,333 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for The Verge: 32 articles\n",
      "2025-09-25 11:53:16,333 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from VentureBeat: https://venturebeat.com/category/ai/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 11:53:16,333 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/VentureBeat.html\n",
      "2025-09-25 11:53:16,340 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/VentureBeat.html\n",
      "2025-09-25 11:53:16,340 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for VentureBeat: 12 articles\n",
      "2025-09-25 11:53:16,341 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from WSJ: https://www.wsj.com/tech/ai\n",
      "2025-09-25 11:53:16,341 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/WSJ.html\n",
      "2025-09-25 11:53:16,363 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/WSJ.html\n",
      "2025-09-25 11:53:16,363 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for WSJ: 26 articles\n",
      "2025-09-25 11:53:16,363 - fetcher_5437863184 - INFO - [fetch_html] Using existing HTML file from Washington Post: https://www.washingtonpost.com/technology/innovations/\n",
      "2025-09-25 11:53:16,364 - fetcher_5437863184 - INFO - [fetch_html] Parsing HTML file: download/sources/Washington_Post.html\n",
      "2025-09-25 11:53:16,386 - fetcher_5437863184 - INFO - [fetch_html] Parsed HTML file: download/sources/Washington_Post.html\n",
      "2025-09-25 11:53:16,387 - fetcher_5437863184 - INFO - [fetch_html] HTML fetch successful for Washington Post: 31 articles\n",
      "2025-09-25 11:53:16,387 - fetcher_5437863184 - INFO - [fetch_rss] Fetching RSS from The Guardian: https://www.theguardian.com/uk/technology/rss\n",
      "2025-09-25 11:53:16,387 - fetcher_5437863184 - INFO - [newsapi] Fetching top 100 stories matching artificial intelligence since 2025-09-24T11:53:16 from NewsAPI\n",
      "2025-09-25 11:53:16,728 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for New York Times: 26 articles\n",
      "2025-09-25 11:53:16,744 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for The Guardian: 24 articles\n",
      "2025-09-25 11:53:16,760 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for The Register: 50 articles\n",
      "2025-09-25 11:53:16,846 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for HackerNoon: 50 articles\n",
      "2025-09-25 11:53:16,970 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for Ars Technica: 20 articles\n",
      "2025-09-25 11:53:16,975 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for Hacker News: 30 articles\n",
      "2025-09-25 11:53:17,096 - fetcher_5437863184 - INFO - [fetch_rss] RSS fetch successful for Techmeme: 15 articles\n",
      "2025-09-25 11:53:17,097 - fetcher_5437863184 - INFO - [fetch_all] fetch_all completed: 17 successful, 0 failed, 693 total articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FT</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedly AI</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hacker News</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HackerNoon</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New York Times</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Techmeme</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Guardian</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Register</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VentureBeat</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WSJ</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Washington Post</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source  url\n",
       "0       Ars Technica   20\n",
       "1          Bloomberg   25\n",
       "2   Business Insider   17\n",
       "3                 FT   48\n",
       "4          Feedly AI   73\n",
       "5        Hacker News   30\n",
       "6         HackerNoon   50\n",
       "7     New York Times   26\n",
       "8            NewsAPI   94\n",
       "9             Reddit   53\n",
       "10          Techmeme   15\n",
       "11      The Guardian   24\n",
       "12      The Register   50\n",
       "13         The Verge   30\n",
       "14       VentureBeat   12\n",
       "15               WSJ   18\n",
       "16   Washington Post   28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>rss_summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>DeepMind‚Äôs robotic ballet: An AI for coordinat...</td>\n",
       "      <td>https://arstechnica.com/science/2025/09/deepmi...</td>\n",
       "      <td>Thu, 25 Sep 2025 11:15:40 +0000</td>\n",
       "      <td>An AI figures out how robots can get jobs done...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Why does OpenAI need six giant data centers?</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/why-does-op...</td>\n",
       "      <td>Wed, 24 Sep 2025 16:06:03 +0000</td>\n",
       "      <td>OpenAI's new $400 billion announcement reveals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>When ‚Äúno‚Äù means ‚Äúyes‚Äù: Why AI chatbots can‚Äôt p...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/when-no-mea...</td>\n",
       "      <td>Tue, 23 Sep 2025 22:23:22 +0000</td>\n",
       "      <td>New study examines how a helpful AI response c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>OpenAI and Nvidia‚Äôs $100B AI plan will require...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/openai-and-...</td>\n",
       "      <td>Mon, 22 Sep 2025 19:17:28 +0000</td>\n",
       "      <td>\"This is a giant project,\" Nvidia CEO said of ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>DeepMind AI safety report explores the perils ...</td>\n",
       "      <td>https://arstechnica.com/google/2025/09/deepmin...</td>\n",
       "      <td>Mon, 22 Sep 2025 18:18:00 +0000</td>\n",
       "      <td>DeepMind releases version 3.0 of its AI Fronti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Step into the future: The full AI Stage at Tec...</td>\n",
       "      <td>https://biztoc.com/x/bca313645c5d345d</td>\n",
       "      <td>2025-09-24T14:41:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Bitcoin Miners Surge on Speculation of OpenAI-...</td>\n",
       "      <td>https://biztoc.com/x/a7b611dec662f738</td>\n",
       "      <td>2025-09-24T15:48:07Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Startup using AI to automate software testing ...</td>\n",
       "      <td>https://biztoc.com/x/856c60081122fcf0</td>\n",
       "      <td>2025-09-24T13:12:10Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>The importance of scientific research in build...</td>\n",
       "      <td>https://biztoc.com/x/e66df74bc285ad44</td>\n",
       "      <td>2025-09-24T13:12:13Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Chinese tech firms rush to list in Hong Kong a...</td>\n",
       "      <td>https://biztoc.com/x/97dfe4cdf98e1281</td>\n",
       "      <td>2025-09-24T12:50:06Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>613 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                              title  \\\n",
       "0    Ars Technica  DeepMind‚Äôs robotic ballet: An AI for coordinat...   \n",
       "1    Ars Technica       Why does OpenAI need six giant data centers?   \n",
       "2    Ars Technica  When ‚Äúno‚Äù means ‚Äúyes‚Äù: Why AI chatbots can‚Äôt p...   \n",
       "3    Ars Technica  OpenAI and Nvidia‚Äôs $100B AI plan will require...   \n",
       "4    Ars Technica  DeepMind AI safety report explores the perils ...   \n",
       "..            ...                                                ...   \n",
       "608       NewsAPI  Step into the future: The full AI Stage at Tec...   \n",
       "609       NewsAPI  Bitcoin Miners Surge on Speculation of OpenAI-...   \n",
       "610       NewsAPI  Startup using AI to automate software testing ...   \n",
       "611       NewsAPI  The importance of scientific research in build...   \n",
       "612       NewsAPI  Chinese tech firms rush to list in Hong Kong a...   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://arstechnica.com/science/2025/09/deepmi...   \n",
       "1    https://arstechnica.com/ai/2025/09/why-does-op...   \n",
       "2    https://arstechnica.com/ai/2025/09/when-no-mea...   \n",
       "3    https://arstechnica.com/ai/2025/09/openai-and-...   \n",
       "4    https://arstechnica.com/google/2025/09/deepmin...   \n",
       "..                                                 ...   \n",
       "608              https://biztoc.com/x/bca313645c5d345d   \n",
       "609              https://biztoc.com/x/a7b611dec662f738   \n",
       "610              https://biztoc.com/x/856c60081122fcf0   \n",
       "611              https://biztoc.com/x/e66df74bc285ad44   \n",
       "612              https://biztoc.com/x/97dfe4cdf98e1281   \n",
       "\n",
       "                           published  \\\n",
       "0    Thu, 25 Sep 2025 11:15:40 +0000   \n",
       "1    Wed, 24 Sep 2025 16:06:03 +0000   \n",
       "2    Tue, 23 Sep 2025 22:23:22 +0000   \n",
       "3    Mon, 22 Sep 2025 19:17:28 +0000   \n",
       "4    Mon, 22 Sep 2025 18:18:00 +0000   \n",
       "..                               ...   \n",
       "608             2025-09-24T14:41:00Z   \n",
       "609             2025-09-24T15:48:07Z   \n",
       "610             2025-09-24T13:12:10Z   \n",
       "611             2025-09-24T13:12:13Z   \n",
       "612             2025-09-24T12:50:06Z   \n",
       "\n",
       "                                           rss_summary   id  \n",
       "0    An AI figures out how robots can get jobs done...    0  \n",
       "1    OpenAI's new $400 billion announcement reveals...    1  \n",
       "2    New study examines how a helpful AI response c...    2  \n",
       "3    \"This is a giant project,\" Nvidia CEO said of ...    3  \n",
       "4    DeepMind releases version 3.0 of its AI Fronti...    4  \n",
       "..                                                 ...  ...  \n",
       "608                                                NaN  608  \n",
       "609                                                NaN  609  \n",
       "610                                                NaN  610  \n",
       "611                                                NaN  611  \n",
       "612                                                NaN  612  \n",
       "\n",
       "[613 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:17 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Completed Step 1: Gathered 693 articles\n",
      "11:53:19 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting check_workflow_status\n",
      "11:53:19 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Completed check_workflow_status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚è±Ô∏è  Total execution time: 11.57s\n",
      "üìä Final result:\n",
      "Step 1 (Fetch URLs) completed.\n",
      "\n",
      "Summary:\n",
      "- Collected 693 article records from 17 sources via RSS; 613 articles stored in persistent state.\n",
      "- Workflow progress: 1/9 steps complete (11.1%).\n",
      "- Next step: Step 2 ‚Äî Filter URLs (to AI-related content).\n",
      "\n",
      "Would you like me to proceed to Step 2 now?\n"
     ]
    }
   ],
   "source": [
    "# User prompt to run a workflow step\n",
    "user_prompt = \"Run step 1, fetch urls\"\n",
    "\n",
    "print(f\"\\nüìù User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56e5eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>rss_summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>DeepMind‚Äôs robotic ballet: An AI for coordinat...</td>\n",
       "      <td>https://arstechnica.com/science/2025/09/deepmi...</td>\n",
       "      <td>Thu, 25 Sep 2025 11:15:40 +0000</td>\n",
       "      <td>An AI figures out how robots can get jobs done...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Why does OpenAI need six giant data centers?</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/why-does-op...</td>\n",
       "      <td>Wed, 24 Sep 2025 16:06:03 +0000</td>\n",
       "      <td>OpenAI's new $400 billion announcement reveals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>When ‚Äúno‚Äù means ‚Äúyes‚Äù: Why AI chatbots can‚Äôt p...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/when-no-mea...</td>\n",
       "      <td>Tue, 23 Sep 2025 22:23:22 +0000</td>\n",
       "      <td>New study examines how a helpful AI response c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>OpenAI and Nvidia‚Äôs $100B AI plan will require...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/openai-and-...</td>\n",
       "      <td>Mon, 22 Sep 2025 19:17:28 +0000</td>\n",
       "      <td>\"This is a giant project,\" Nvidia CEO said of ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>DeepMind AI safety report explores the perils ...</td>\n",
       "      <td>https://arstechnica.com/google/2025/09/deepmin...</td>\n",
       "      <td>Mon, 22 Sep 2025 18:18:00 +0000</td>\n",
       "      <td>DeepMind releases version 3.0 of its AI Fronti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Step into the future: The full AI Stage at Tec...</td>\n",
       "      <td>https://biztoc.com/x/bca313645c5d345d</td>\n",
       "      <td>2025-09-24T14:41:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Bitcoin Miners Surge on Speculation of OpenAI-...</td>\n",
       "      <td>https://biztoc.com/x/a7b611dec662f738</td>\n",
       "      <td>2025-09-24T15:48:07Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Startup using AI to automate software testing ...</td>\n",
       "      <td>https://biztoc.com/x/856c60081122fcf0</td>\n",
       "      <td>2025-09-24T13:12:10Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>The importance of scientific research in build...</td>\n",
       "      <td>https://biztoc.com/x/e66df74bc285ad44</td>\n",
       "      <td>2025-09-24T13:12:13Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Chinese tech firms rush to list in Hong Kong a...</td>\n",
       "      <td>https://biztoc.com/x/97dfe4cdf98e1281</td>\n",
       "      <td>2025-09-24T12:50:06Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>613 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                              title  \\\n",
       "0    Ars Technica  DeepMind‚Äôs robotic ballet: An AI for coordinat...   \n",
       "1    Ars Technica       Why does OpenAI need six giant data centers?   \n",
       "2    Ars Technica  When ‚Äúno‚Äù means ‚Äúyes‚Äù: Why AI chatbots can‚Äôt p...   \n",
       "3    Ars Technica  OpenAI and Nvidia‚Äôs $100B AI plan will require...   \n",
       "4    Ars Technica  DeepMind AI safety report explores the perils ...   \n",
       "..            ...                                                ...   \n",
       "608       NewsAPI  Step into the future: The full AI Stage at Tec...   \n",
       "609       NewsAPI  Bitcoin Miners Surge on Speculation of OpenAI-...   \n",
       "610       NewsAPI  Startup using AI to automate software testing ...   \n",
       "611       NewsAPI  The importance of scientific research in build...   \n",
       "612       NewsAPI  Chinese tech firms rush to list in Hong Kong a...   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://arstechnica.com/science/2025/09/deepmi...   \n",
       "1    https://arstechnica.com/ai/2025/09/why-does-op...   \n",
       "2    https://arstechnica.com/ai/2025/09/when-no-mea...   \n",
       "3    https://arstechnica.com/ai/2025/09/openai-and-...   \n",
       "4    https://arstechnica.com/google/2025/09/deepmin...   \n",
       "..                                                 ...   \n",
       "608              https://biztoc.com/x/bca313645c5d345d   \n",
       "609              https://biztoc.com/x/a7b611dec662f738   \n",
       "610              https://biztoc.com/x/856c60081122fcf0   \n",
       "611              https://biztoc.com/x/e66df74bc285ad44   \n",
       "612              https://biztoc.com/x/97dfe4cdf98e1281   \n",
       "\n",
       "                           published  \\\n",
       "0    Thu, 25 Sep 2025 11:15:40 +0000   \n",
       "1    Wed, 24 Sep 2025 16:06:03 +0000   \n",
       "2    Tue, 23 Sep 2025 22:23:22 +0000   \n",
       "3    Mon, 22 Sep 2025 19:17:28 +0000   \n",
       "4    Mon, 22 Sep 2025 18:18:00 +0000   \n",
       "..                               ...   \n",
       "608             2025-09-24T14:41:00Z   \n",
       "609             2025-09-24T15:48:07Z   \n",
       "610             2025-09-24T13:12:10Z   \n",
       "611             2025-09-24T13:12:13Z   \n",
       "612             2025-09-24T12:50:06Z   \n",
       "\n",
       "                                           rss_summary   id  \n",
       "0    An AI figures out how robots can get jobs done...    0  \n",
       "1    OpenAI's new $400 billion announcement reveals...    1  \n",
       "2    New study examines how a helpful AI response c...    2  \n",
       "3    \"This is a giant project,\" Nvidia CEO said of ...    3  \n",
       "4    DeepMind releases version 3.0 of its AI Fronti...    4  \n",
       "..                                                 ...  ...  \n",
       "608                                                NaN  608  \n",
       "609                                                NaN  609  \n",
       "610                                                NaN  610  \n",
       "611                                                NaN  611  \n",
       "612                                                NaN  612  \n",
       "\n",
       "[613 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(state.headline_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b5dc883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedly AI</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HackerNoon</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Register</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FT</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hacker News</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Washington Post</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New York Times</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Guardian</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WSJ</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Techmeme</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VentureBeat</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source  count\n",
       "8            NewsAPI     94\n",
       "4          Feedly AI     73\n",
       "9             Reddit     53\n",
       "6         HackerNoon     50\n",
       "12      The Register     50\n",
       "3                 FT     48\n",
       "13         The Verge     30\n",
       "5        Hacker News     30\n",
       "16   Washington Post     28\n",
       "7     New York Times     26\n",
       "1          Bloomberg     25\n",
       "11      The Guardian     24\n",
       "0       Ars Technica     20\n",
       "15               WSJ     18\n",
       "2   Business Insider     17\n",
       "10          Techmeme     15\n",
       "14       VentureBeat     12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countdf = pd.DataFrame(state.headline_data) \\\n",
    "    .groupby(\"source\") \\\n",
    "    .count()[[\"id\"]] \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'id': 'count'}) \\\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "countdf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68c823e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:21 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting Step 2: Filter URLs\n",
      "INFO:llm:Initialized LangfuseClient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733280\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733394\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733460\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733526\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733580\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733650\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733709\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733826\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733889\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.733956\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734015\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734079\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734140\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734204\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734261\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734360\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734423\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734501\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734575\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734663\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734802\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.734939\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.735009\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.735101\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.735166\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.735241\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.735323\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:49:26.735389\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.342510\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.342698\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.342786\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.342856\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.342957\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343025\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343079\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343183\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343282\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343343\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343420\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343470\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343554\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343603\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343656\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343707\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343760\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343806\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343859\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343904\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.343982\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344028\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344081\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344127\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344181\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344227\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344281\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344332\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344417\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344487\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344536\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344581\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344635\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344680\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344750\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344799\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344865\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344914\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.344970\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345021\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345073\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345122\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345204\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345268\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345317\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345373\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345419\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345473\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345524\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345602\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345681\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345752\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345801\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.345877\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346007\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346186\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346235\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346305\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346355\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346399\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346446\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346511\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346570\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346614\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346666\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346709\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346759\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346802\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346847\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346911\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.346962\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347005\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347052\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347095\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347142\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347184\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347267\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347328\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347371\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347417\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347461\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347509\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347553\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347600\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347667\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347723\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347767\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347836\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.347996\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348058\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348156\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348217\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348282\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348340\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348455\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348609\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348684\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348767\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348877\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.348947\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349015\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349083\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349147\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349220\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349328\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349404\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349497\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349557\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349685\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349749\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349918\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.349967\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350035\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350088\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350136\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350187\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350236\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350293\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350342\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350418\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350491\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350536\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350587\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350632\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350687\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350732\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350820\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350869\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350913\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.350978\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351027\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351071\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351120\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351165\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351214\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351260\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351309\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351354\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351409\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351466\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351526\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351581\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351631\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351704\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351772\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351820\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351912\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.351996\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352051\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352107\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352182\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352327\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352379\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352476\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352537\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120479\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352588\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120670\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120747\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352787\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120796\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120876\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352831\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352882\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352933\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352987\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120945\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353849\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353035\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353085\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.120999\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353192\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353132\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.352729\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353306\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.121060\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.121096\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353241\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353459\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353359\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353408\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.121191\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353584\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353792\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.353532\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.354596\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.354651\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.354742\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.354945\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355002\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355057\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355112\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355182\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355259\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355309\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355364\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355426\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355518\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355572\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355628\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355705\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355756\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355833\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355880\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.355952\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356009\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356096\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356199\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356248\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356305\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356381\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356435\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356523\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356578\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356633\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356687\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356762\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356841\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356892\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.356952\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357005\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357061\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357135\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357189\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357260\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357335\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357393\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357444\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357539\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357592\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357645\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357711\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357760\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357812\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357862\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.121632\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357928\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.357985\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358036\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.121786\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.121854\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358128\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358079\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358173\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358253\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358303\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358359\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358411\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358547\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358595\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358664\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358725\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358780\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358835\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358904\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.358986\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359047\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359124\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359190\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359281\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359329\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359434\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359488\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359537\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359607\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359747\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359925\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.359975\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360078\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360140\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360207\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360251\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360302\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360344\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360405\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360451\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360492\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360537\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360579\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360625\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360671\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360726\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360776\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360822\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360902\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.360965\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361010\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361059\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361103\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361156\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361199\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361248\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361294\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361342\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361385\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361438\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361482\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361543\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361599\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361649\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361715\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361769\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361821\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361897\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.361941\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362008\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362055\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362096\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362141\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362182\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362253\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362298\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362348\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362388\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.122428\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.122534\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.122603\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.122683\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.122719\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.122794\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362434\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362475\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362528\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362596\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362663\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362712\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362755\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362803\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.362869\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363279\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363358\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363410\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363453\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363516\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363562\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363605\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363651\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363720\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363768\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363810\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363860\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363912\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.363982\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364033\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364079\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364149\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364197\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364262\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364315\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364385\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364455\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364537\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364584\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364643\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364685\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364731\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364772\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364819\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364886\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364927\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.364973\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.365015\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.365063\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.365125\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.365171\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.365215\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370590\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370647\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370683\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370733\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370771\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370827\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370884\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.370961\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371001\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371054\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371088\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371128\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371163\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371220\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371256\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371294\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371326\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371365\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371398\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371435\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371486\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371520\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371561\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371612\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371676\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371710\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371748\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371781\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371819\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371864\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371896\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371934\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.371967\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372004\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372035\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372071\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372102\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372139\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372170\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372206\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372236\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372272\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372304\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372339\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372371\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372406\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372438\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372473\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372504\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372540\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372582\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372614\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372649\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372681\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372717\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372753\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372791\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372826\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372865\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372899\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372954\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.372987\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373025\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373060\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373097\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373128\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373164\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373195\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373230\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373260\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373312\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373355\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373391\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373433\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373488\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373521\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373571\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373635\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373679\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373715\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373767\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373815\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373867\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373903\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.373963\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374000\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374037\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374073\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374114\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374181\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374214\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374259\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374295\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374341\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374382\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374434\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374470\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374508\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374543\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374580\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374612\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374649\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374684\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374720\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374763\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374794\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374841\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374876\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123292\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374924\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.374970\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375007\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375100\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375135\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375166\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375203\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375237\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375280\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375314\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375343\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375377\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375431\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375460\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375501\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375545\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123407\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375575\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375609\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375724\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375753\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123457\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123497\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123534\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375787\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123580\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123616\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375831\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375922\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.375958\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123687\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123751\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123790\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123833\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123871\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376027\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376071\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376120\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376159\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376209\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123932\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.123969\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124013\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376242\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376280\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376314\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376352\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124068\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376386\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124143\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376474\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124184\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124249\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376595\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124294\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376668\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376723\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124392\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124429\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376756\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376794\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376828\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376896\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.376930\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124514\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377003\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377056\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377125\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377213\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124581\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377247\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377310\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377281\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377345\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377415\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377376\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377448\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377490\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377530\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377570\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377648\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377700\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124656\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377731\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124700\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377795\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124759\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124811\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124878\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377897\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124940\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.377927\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.124992\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378055\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378087\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.125049\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378188\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378549\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378345\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.125107\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378453\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378422\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.125190\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378298\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378261\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.125237\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 11:53:17.125275\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378692\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378731\n",
      "<class 'datetime.datetime'>\n",
      "2025-09-25 09:56:31.378626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm:Successfully retrieved prompt 'newsagent/filter_urls' from Langfuse\n",
      "INFO:llm:Parsed prompt 'newsagent/filter_urls': model=gpt-4.1-mini, system_len=458, user_len=954\n",
      "11:53:57 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Completed Step 2: 336 AI-related articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚è±Ô∏è  Total execution time: 35.41s\n",
      "üìä Final result:\n",
      "‚úÖ Step 2 completed successfully! Filtered 613 headlines to 336 AI-related articles.\n",
      "\n",
      "üìä Results stored in persistent state. Current step: step_03_download_articles\n"
     ]
    }
   ],
   "source": [
    "# Run tool directly without LLM processing an input prompt or results\n",
    "# user_prompt = \"Run step 2, filter urls\"\n",
    "# print(f\"\\nüìù User prompt: '{user_prompt}'\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_tool_direct(\"filter_urls\")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:54:43 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting Step 3: Download Articles\n",
      "11:54:43 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Starting concurrent scraping of 336 AI-related articles\n",
      "11:54:43 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Launching browser for 336 URLs with 16 concurrent workers\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 0 fetching 1 of 336 https://go.theregister.com/feed/www.theregister.com/2025/09/23/kaspersky_revengehotels_checks_back_in/\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://go.theregister.com/feed/www.theregister.com/2025/09/23/kaspersky_revengehotels_checks_back_in/)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://go.theregister.com/feed/www.theregister.com/2025/09/23/kaspersky_revengehotels_checks_back_in/ to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://go.theregister.com/feed/www.theregister.com/2025/09/23/kaspersky_revengehotels_checks_back_in/\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 1 fetching 2 of 336 https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/ to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 2 fetching 3 of 336 https://www.bloomberg.com/news/audio/2025-09-25/tech-disruptors-linkedin-on-job-market-s-shift-in-ai-landscape\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Skipping ignored domain: www.bloomberg.com\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 2 completed https://www.bloomberg.com/news/audio/2025-09-25/tech-disruptors-linkedin-on-job-market-s-shift-in-ai-landscape with status: success\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 2 fetching 4 of 336 https://venturebeat.com/ai/ficos-answer-to-ai-risk-a-foundation-model-that-scores-every-output-for\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://venturebeat.com/ai/ficos-answer-to-ai-risk-a-foundation-model-that-scores-every-output-for)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://venturebeat.com/ai/ficos-answer-to-ai-risk-a-foundation-model-that-scores-every-output-for to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://venturebeat.com/ai/ficos-answer-to-ai-risk-a-foundation-model-that-scores-every-output-for\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 3 fetching 5 of 336 https://www.nytimes.com/2025/09/25/technology/grok-xai-government-elon-musk.html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.nytimes.com/2025/09/25/technology/grok-xai-government-elon-musk.html)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.nytimes.com/2025/09/25/technology/grok-xai-government-elon-musk.html to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.nytimes.com/2025/09/25/technology/grok-xai-government-elon-musk.html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 fetching 6 of 336 https://www.washingtonpost.com/business/2025/09/02/ai-skills-jobs-tips/\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Rate limiting domain www.washingtonpost.com, will retry later (need to wait 2.0s)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 re-queued rate-limited URL: https://www.washingtonpost.com/business/2025/09/02/ai-skills-jobs-tips/\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 5 fetching 6 of 336 https://hackernoon.com/gemini-might-be-the-only-actual-foundational-model-out-there?source=rss\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://hackernoon.com/gemini-might-be-the-only-actual-foundational-model-out-there?source=rss)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://hackernoon.com/gemini-might-be-the-only-actual-foundational-model-out-there?source=rss to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://hackernoon.com/gemini-might-be-the-only-actual-foundational-model-out-there?source=rss\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 6 fetching 7 of 336 https://www.theverge.com/news/784685/google-search-live-ai-voice-search-launch\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.theverge.com/news/784685/google-search-live-ai-voice-search-launch)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.theverge.com/news/784685/google-search-live-ai-voice-search-launch to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.theverge.com/news/784685/google-search-live-ai-voice-search-launch\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 7 fetching 8 of 336 https://www.techradar.com/ai-platforms-assistants/gemini/google-mixboard-is-your-new-ai-powered-creative-playground\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.techradar.com/ai-platforms-assistants/gemini/google-mixboard-is-your-new-ai-powered-creative-playground)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.techradar.com/ai-platforms-assistants/gemini/google-mixboard-is-your-new-ai-powered-creative-playground to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.techradar.com/ai-platforms-assistants/gemini/google-mixboard-is-your-new-ai-powered-creative-playground\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 8 fetching 9 of 336 https://finance.yahoo.com/news/nvidia-nvda-unveils-next-gen-124550903.html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://finance.yahoo.com/news/nvidia-nvda-unveils-next-gen-124550903.html)\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://finance.yahoo.com/news/nvidia-nvda-unveils-next-gen-124550903.html to download/html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://finance.yahoo.com/news/nvidia-nvda-unveils-next-gen-124550903.html\n",
      "11:54:45 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 9 fetching 10 of 336 https://go.theregister.com/feed/www.theregister.com/2025/09/16/fiverr_ai_layoff/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Rate limiting domain go.theregister.com, will retry later (need to wait 1.9s)\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 9 re-queued rate-limited URL: https://go.theregister.com/feed/www.theregister.com/2025/09/16/fiverr_ai_layoff/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 10 fetching 10 of 336 https://arstechnica.com/health/2025/09/ai-medical-tools-found-to-downplay-symptoms-of-women-ethnic-minorities/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://arstechnica.com/health/2025/09/ai-medical-tools-found-to-downplay-symptoms-of-women-ethnic-minorities/)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://arstechnica.com/health/2025/09/ai-medical-tools-found-to-downplay-symptoms-of-women-ethnic-minorities/ to download/html\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://arstechnica.com/health/2025/09/ai-medical-tools-found-to-downplay-symptoms-of-women-ethnic-minorities/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 11 fetching 11 of 336 https://biztoc.com/x/c31a2cf8b2a32750\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://biztoc.com/x/c31a2cf8b2a32750)\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://biztoc.com/x/c31a2cf8b2a32750 to download/html\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://biztoc.com/x/c31a2cf8b2a32750\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 12 fetching 12 of 336 https://www.wired.com/story/google-photos-conversational-photo-editor/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.wired.com/story/google-photos-conversational-photo-editor/)\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.wired.com/story/google-photos-conversational-photo-editor/ to download/html\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.wired.com/story/google-photos-conversational-photo-editor/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 13 fetching 13 of 336 https://ogletree.com/insights-resources/blog-posts/seasonal-hiring-concerns-how-pay-transparency-privacy-and-ai-laws-still-apply/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://ogletree.com/insights-resources/blog-posts/seasonal-hiring-concerns-how-pay-transparency-privacy-and-ai-laws-still-apply/)\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://ogletree.com/insights-resources/blog-posts/seasonal-hiring-concerns-how-pay-transparency-privacy-and-ai-laws-still-apply/ to download/html\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://ogletree.com/insights-resources/blog-posts/seasonal-hiring-concerns-how-pay-transparency-privacy-and-ai-laws-still-apply/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 14 fetching 14 of 336 https://blog.google/technology/developers/gemini-cli-code-assist-higher-limits/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://blog.google/technology/developers/gemini-cli-code-assist-higher-limits/)\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://blog.google/technology/developers/gemini-cli-code-assist-higher-limits/ to download/html\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://blog.google/technology/developers/gemini-cli-code-assist-higher-limits/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 15 fetching 15 of 336 https://www.zdnet.com/article/your-coworkers-are-sick-of-your-ai-workslop/\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.zdnet.com/article/your-coworkers-are-sick-of-your-ai-workslop/)\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.zdnet.com/article/your-coworkers-are-sick-of-your-ai-workslop/ to download/html\n",
      "11:54:46 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.zdnet.com/article/your-coworkers-are-sick-of-your-ai-workslop/\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Response: 200\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 fetching 16 of 336 https://www.bloomberg.com/news/articles/2025-09-25/ai-startup-nscale-raises-1-1-billion-for-data-center-expansion\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Skipping ignored domain: www.bloomberg.com\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 completed https://www.bloomberg.com/news/articles/2025-09-25/ai-startup-nscale-raises-1-1-billion-for-data-center-expansion with status: success\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 fetching 17 of 336 https://arstechnica.com/ai/2025/09/chatgpt-may-soon-require-id-verification-from-adults-ceo-says/\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Rate limiting domain arstechnica.com, will retry later (need to wait 0.0s)\n",
      "11:54:47 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 re-queued rate-limited URL: https://arstechnica.com/ai/2025/09/chatgpt-may-soon-require-id-verification-from-adults-ceo-says/\n",
      "11:54:48 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 9 fetching 17 of 336 https://www.nytimes.com/2025/09/22/technology/nvidia-openai-100-billion-investment.html\n",
      "11:54:48 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.nytimes.com/2025/09/22/technology/nvidia-openai-100-billion-investment.html)\n",
      "11:54:48 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.nytimes.com/2025/09/22/technology/nvidia-openai-100-billion-investment.html to download/html\n",
      "11:54:48 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.nytimes.com/2025/09/22/technology/nvidia-openai-100-billion-investment.html\n",
      "11:54:48 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Response: 200\n",
      "11:54:48 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Response: 200\n",
      "11:54:49 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Response: 200\n",
      "11:54:49 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Response: 200\n",
      "11:54:49 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Worker 4 fetching 18 of 336 https://www.morningstar.com/news/pr-newswire/20250925to83248/ramsoft-partners-with-koios-medical-to-provide-integrated-ai-powered-software-for-breast-and-thyroid-cancer-diagnosis\n",
      "11:54:49 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scrape_url(https://www.morningstar.com/news/pr-newswire/20250925to83248/ramsoft-partners-with-koios-medical-to-provide-integrated-ai-powered-software-for-breast-and-thyroid-cancer-diagnosis)\n",
      "11:54:49 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | scraping https://www.morningstar.com/news/pr-newswire/20250925to83248/ramsoft-partners-with-koios-medical-to-provide-integrated-ai-powered-software-for-breast-and-thyroid-cancer-diagnosis to download/html\n",
      "11:54:49 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Downloading https://www.morningstar.com/news/pr-newswire/20250925to83248/ramsoft-partners-with-koios-medical-to-provide-integrated-ai-powered-software-for-breast-and-thyroid-cancer-diagnosis\n",
      "11:54:50 | NewsletterAgent.test_newsletter_20250925115257412043 | INFO | Response: 200\n"
     ]
    }
   ],
   "source": [
    "# User prompt to run workflow\n",
    "# user_prompt = \"Run step 3, download full articles\"\n",
    "# print(f\"\\nüìù User prompt: '{user_prompt}'\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_tool_direct(\"download_articles\")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     initial_url: str\n",
    "#     final_url: str\n",
    "#     title: str\n",
    "#     isAI: bool\n",
    "#     created_at: Optional[datetime]\n",
    "headline_df = agent.state.headline_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf931ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import db\n",
    "\n",
    "with sqlite3.connect(\"newsletter_agent.db\") as conn:\n",
    "    db.Url.create_table(conn)\n",
    "    for row in headline_df.itertuples():\n",
    "        print(row.url, '', row.title, row.isAI, datetime.now())\n",
    "        myurl = db.Url(row.url, '', row.title, row.isAI, datetime.now())\n",
    "        myurl.insert(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "# user_prompt = \"Run step 4, Summarize articles\"\n",
    "# print(f\"\\nüìù User prompt: '{user_prompt}'\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "result = await agent.run_tool_direct(\"extract_summaries\")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "# user_prompt = \"Run step 5, Cluster articles by topic\"\n",
    "# print(f\"\\nüìù User prompt: '{user_prompt}'\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "result = await agent.run_tool_direct(\"cluster_by_topic\")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.headline_dict.loc[state.headline_dict[\"url\"] != state.headline_dict[\"final_url\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb36efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.headline_dict.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def create_extended_summary(row):\n",
    "        parts = []\n",
    "\n",
    "        # Add title if present\n",
    "        if 'title' in row and row['title']:\n",
    "            parts.append(str(row['title']).strip())\n",
    "\n",
    "        # Add description if present\n",
    "        if 'description' in row and row['description']:\n",
    "            parts.append(str(row['description']).strip())\n",
    "\n",
    "        # Add topics if present (join with commas)\n",
    "        if 'topics' in row and row['topics']:\n",
    "            if isinstance(row['topics'], list):\n",
    "                topics_str = \", \".join(str(topic).strip() for topic in row['topics'] if topic)\n",
    "            else:\n",
    "                topics_str = str(row['topics']).strip()\n",
    "            if topics_str:\n",
    "                parts.append(topics_str)\n",
    "\n",
    "        # Add summary if present\n",
    "        if pd.notna(row.get('summary')) and row.get('summary'):\n",
    "            parts.append(str(row['summary']).strip())\n",
    "\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "    async def _get_embeddings_df(self, headline_data: pd.DataFrame, embedding_model: str = \"text-embedding-3-large\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get embeddings for article summaries and return as DataFrame.\n",
    "\n",
    "        Args:\n",
    "            headline_data: DataFrame with articles containing summary column\n",
    "            embedding_model: OpenAI embedding model to use\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with embeddings for each extended summary\n",
    "        \"\"\"\n",
    "        from openai import OpenAI\n",
    "        from llm import paginate_df_async\n",
    "\n",
    "        # Create extended_summary column by concatenating available fields\n",
    "        headline_data_copy = headline_data.copy()\n",
    "\n",
    "        headline_data_copy['extended_summary'] = headline_data_copy.apply(create_extended_summary, axis=1)\n",
    "\n",
    "        # Filter to articles with non-empty extended summaries\n",
    "        articles_with_summaries = headline_data_copy[\n",
    "            (headline_data_copy['extended_summary'].notna()) &\n",
    "            (headline_data_copy['extended_summary'] != '')\n",
    "        ].copy()\n",
    "\n",
    "        all_embeddings = []\n",
    "        client = OpenAI()\n",
    "\n",
    "        # Use paginate_df_async similar to dedupe_by_cosine_similarity.py\n",
    "        async for batch_df in paginate_df_async(articles_with_summaries, 25):\n",
    "            text_batch = batch_df[\"extended_summary\"].to_list()\n",
    "            response = client.embeddings.create(input=text_batch, model=embedding_model)\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        # Create DataFrame with embeddings, preserving original index\n",
    "        embedding_df = pd.DataFrame(\n",
    "            all_embeddings,\n",
    "            index=articles_with_summaries.index\n",
    "        )\n",
    "\n",
    "        return embedding_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df = state.headline_dict\n",
    "headline_df['extended_summary'] = headline_df.apply(create_extended_summary, axis=1)\n",
    "\n",
    "\n",
    "embeddings_df = await _get_embeddings_df(_, state.headline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d653f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 60\n",
    "min_cluster_size = 4\n",
    "min_samples =3 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a85300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\n",
    "reduced_embeddings = svd.fit_transform(embeddings_df)\n",
    "# Re-normalize after SVD\n",
    "reduced_embeddings /= np.linalg.norm(reduced_embeddings, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07326b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit HDBSCAN\n",
    "print(\"=== HDBSCAN Parameters ===\")\n",
    "print(f\"min_cluster_size:   {min_cluster_size}\")\n",
    "print(f\"min_samples:        {min_samples}\")\n",
    "print(f\"n_components:       {n_components}\")\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(reduced_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac02dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clustering_metrics(embeddings_array, labels, clusterer=None):\n",
    "    \"\"\"\n",
    "    Calculate various clustering quality metrics for HDBSCAN results.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_array: Original normalized embeddings used for clustering\n",
    "        labels: Cluster labels from HDBSCAN\n",
    "        clusterer: Optional HDBSCAN clusterer object\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of clustering metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out noise points (-1 labels) for some metrics\n",
    "    non_noise_mask = labels != -1\n",
    "    non_noise_embeddings = embeddings_array[non_noise_mask]\n",
    "    non_noise_labels = labels[non_noise_mask]\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic cluster statistics\n",
    "    unique_labels = set(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    \n",
    "    metrics['n_clusters'] = n_clusters\n",
    "    metrics['n_noise_points'] = n_noise\n",
    "    metrics['noise_ratio'] = n_noise / len(labels)\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    cluster_sizes = Counter(labels[labels != -1])\n",
    "    if cluster_sizes:\n",
    "        metrics['avg_cluster_size'] = np.mean(list(cluster_sizes.values()))\n",
    "        metrics['std_cluster_size'] = np.std(list(cluster_sizes.values()))\n",
    "        metrics['min_cluster_size'] = min(cluster_sizes.values())\n",
    "        metrics['max_cluster_size'] = max(cluster_sizes.values())\n",
    "    \n",
    "    # Skip other metrics if we have too few clusters or too much noise\n",
    "    if n_clusters < 2 or len(non_noise_labels) < 2:\n",
    "        print(\"Warning: Too few clusters or too much noise for some metrics\")\n",
    "        return metrics\n",
    "    \n",
    "    # HDBSCAN-specific metrics\n",
    "    # gives some divide by 0 errors\n",
    "    if clusterer is not None:\n",
    "        try:\n",
    "            # Validity index (HDBSCAN's internal metric)\n",
    "            validity_idx = hdbscan.validity.validity_index(\n",
    "                embeddings_array, labels, metric='euclidean'\n",
    "            )\n",
    "            metrics['hdbscan_validity_index'] = validity_idx\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compute HDBSCAN validity index: {e}\")\n",
    "        \n",
    "        # Cluster persistence (stability)\n",
    "        if hasattr(clusterer, 'cluster_persistence_'):\n",
    "            metrics['cluster_persistence'] = clusterer.cluster_persistence_\n",
    "    \n",
    "    # Scikit-learn clustering metrics (excluding noise points)\n",
    "    try:\n",
    "        # Silhouette Score (higher is better, range [-1, 1])\n",
    "        sil_score = silhouette_score(non_noise_embeddings, non_noise_labels, metric='euclidean')\n",
    "        metrics['silhouette_score'] = sil_score\n",
    "        \n",
    "        # Calinski-Harabasz Index (higher is better)\n",
    "        ch_score = calinski_harabasz_score(non_noise_embeddings, non_noise_labels)\n",
    "        metrics['calinski_harabasz_score'] = ch_score\n",
    "        \n",
    "        # Davies-Bouldin Index (lower is better)\n",
    "        db_score = davies_bouldin_score(non_noise_embeddings, non_noise_labels)\n",
    "        metrics['davies_bouldin_score'] = db_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute sklearn metrics: {e}\")\n",
    "    \n",
    "    # Custom composite score balancing cluster quality and quantity\n",
    "    if 'silhouette_score' in metrics and n_clusters > 0:\n",
    "        # Penalize too many small clusters or too few large clusters\n",
    "        cluster_balance = 1 / (1 + abs(np.log(n_clusters / 10)))  # Optimal around 10 clusters\n",
    "        size_consistency = 1 / (1 + metrics.get('std_cluster_size', 0) / max(metrics.get('avg_cluster_size', 1), 1))\n",
    "        noise_penalty = 1 - min(metrics['noise_ratio'], 0.5)  # Penalize high noise\n",
    "        \n",
    "        composite_score = (\n",
    "            0.5 * max(metrics['silhouette_score'], 0) +  # Quality component\n",
    "            0.5 * max(metrics['hdbscan_validity_index'], 0)\n",
    "#             0.1 * cluster_balance +                       # Quantity component  \n",
    "#             0.1 * size_consistency +                      # Size consistency\n",
    "#             0.3 * noise_penalty                           # Noise penalty\n",
    "        )\n",
    "        metrics['composite_score'] = composite_score\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_clustering_summary(metrics):\n",
    "    \"\"\"Print a nice summary of clustering metrics.\"\"\"\n",
    "    print(\"=== Clustering Quality Metrics ===\")\n",
    "    print(f\"Number of clusters: {metrics.get('n_clusters', 'N/A')}\")\n",
    "    print(f\"Noise points: {metrics.get('n_noise_points', 'N/A')} ({metrics.get('noise_ratio', 0):.1%})\")\n",
    "    \n",
    "    if 'avg_cluster_size' in metrics:\n",
    "        print(f\"Average cluster size: {metrics['avg_cluster_size']:.1f} ¬± {metrics.get('std_cluster_size', 0):.1f}\")\n",
    "        print(f\"Cluster size range: {metrics.get('min_cluster_size', 'N/A')} - {metrics.get('max_cluster_size', 'N/A')}\")\n",
    "    \n",
    "    print(\"=== Quality Scores ===\")\n",
    "    if 'silhouette_score' in metrics:\n",
    "        print(f\"Silhouette Score: {metrics['silhouette_score']:.3f} (higher is better)\")\n",
    "    if 'calinski_harabasz_score' in metrics:\n",
    "        print(f\"Calinski-Harabasz Score: {metrics['calinski_harabasz_score']:.1f} (higher is better)\")\n",
    "    if 'davies_bouldin_score' in metrics:\n",
    "        print(f\"Davies-Bouldin Score: {metrics['davies_bouldin_score']:.3f} (lower is better)\")\n",
    "    if 'hdbscan_validity_index' in metrics:\n",
    "        print(f\"HDBSCAN Validity Index: {metrics['hdbscan_validity_index']:.3f}\")\n",
    "    if 'composite_score' in metrics:\n",
    "        print(f\"Composite Score: {metrics['composite_score']:.3f} (higher is better)\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db92fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import optuna\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_clustering_metrics(reduced_embeddings, labels, clusterer)\n",
    "print_clustering_summary(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea620430",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COMPONENTS = 20\n",
    "def objective(trial, embeddings_array):\n",
    "\n",
    "    n_components = trial.suggest_int('n_components', \n",
    "                                     MIN_COMPONENTS, \n",
    "                                     embeddings_array.shape[1] // 4)  \n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    reduced_embeddings = svd.fit_transform(embeddings_array)\n",
    "    # Re-normalize after SVD\n",
    "    reduced_embeddings /= np.linalg.norm(reduced_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # HDBSCAN hyperparameters to optimize\n",
    "    min_cluster_size = trial.suggest_int('min_cluster_size', 2, 10)\n",
    "    min_samples = trial.suggest_int('min_samples', 2, min_cluster_size)\n",
    "\n",
    "    # Fit HDBSCAN\n",
    "    print(\"=== HDBSCAN Parameters ===\")\n",
    "    print(f\"min_cluster_size:   {min_cluster_size}\")\n",
    "    print(f\"min_samples:        {min_samples}\")\n",
    "    print(f\"n_components:       {n_components}\")\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "    )\n",
    "\n",
    "    labels = clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_clustering_metrics(reduced_embeddings, labels, clusterer)\n",
    "    print_clustering_summary(metrics)\n",
    "\n",
    "    # Return negative composite score (Optuna minimizes)\n",
    "    composite_score = metrics.get('composite_score', -1.0)\n",
    "\n",
    "    # Penalize if no valid clusters found or too much noise\n",
    "    if metrics.get('n_clusters', 0) < 2 or metrics.get('noise_ratio', 1.0) > 0.8:\n",
    "        composite_score = -1.0\n",
    "\n",
    "    return -composite_score    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc877d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hdbscan(embeddings_array, n_trials=100, timeout=None):\n",
    "    \"\"\"\n",
    "    Optimize HDBSCAN hyperparameters using Optuna.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_array: Normalized embeddings array\n",
    "        n_trials: Number of optimization trials\n",
    "        timeout: Maximum time in seconds (None for no limit)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with best parameters and results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting optimization with {n_trials} trials...\")\n",
    "    print(f\"Original embedding shape: {embeddings_array.shape}\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',  # We return negative composite score\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=10)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, embeddings_array),\n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    best_score = -study.best_value  # Convert back to positive\n",
    "    \n",
    "    print(f\"\\nOptimization completed!\")\n",
    "    print(f\"Best composite score: {best_score:.4f}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Test best parameters\n",
    "    print(f\"\\n=== Results with Best Parameters ===\")\n",
    "        \n",
    "    # Apply best dimensionality reduction\n",
    "    if best_params['n_components'] < embeddings_array.shape[1]:\n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        best_embeddings = svd.fit_transform(embeddings_array)\n",
    "        # Re-normalize after SVD\n",
    "        best_embeddings /= np.linalg.norm(reduced_embeddings, axis=1, keepdims=True)\n",
    "        print(f\"Reduced dimensions from {embeddings_array.shape[1]} to {best_params['n_components']}\")\n",
    "    else:\n",
    "        best_embeddings = embeddings_array\n",
    "        reducer = None\n",
    "        print(\"No dimensionality reduction applied\")\n",
    "     \n",
    "    # Fit with best parameters\n",
    "    best_clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=best_params['min_cluster_size'],\n",
    "        min_samples=best_params['min_samples'],\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "    )\n",
    "    \n",
    "    best_labels = best_clusterer.fit_predict(best_embeddings)\n",
    "    best_metrics = calculate_clustering_metrics(best_embeddings, best_labels, best_clusterer)\n",
    "    \n",
    "    print_clustering_summary(best_metrics)\n",
    "    print()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'study': study,\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'best_clusterer': best_clusterer,\n",
    "        'best_labels': best_labels,\n",
    "        'best_embeddings': best_embeddings,\n",
    "        'best_metrics': best_metrics,\n",
    "        'svd_transformer': svd if best_params['n_components'] < embeddings_array.shape[1] else None\n",
    "    }\n",
    "\n",
    "results = optimize_hdbscan(embeddings_df, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f103e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = calculate_clustering_metrics(embeddings_df.values, labels, clusterer) \n",
    "print_clustering_summary(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Show the workflow status\"\n",
    "\n",
    "print(f\"\\nüìù User prompt: '{user_proampt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚è±Ô∏è  Total execution time: {duration:.2f}s\")\n",
    "print(f\"üìä Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb5b33",
   "metadata": {},
   "source": [
    "- cluster  articles\n",
    "- combine title, description, topics, summary if present \n",
    "- fetch embeddings for summaries \n",
    "- do dimensionality reduction\n",
    "- cluster with hdbscan\n",
    "- show metrics\n",
    "\n",
    "- tune dbscan\n",
    "- name the clusters with topic_writer\n",
    "- store the cluster names \n",
    "\n",
    "output is , df has topic list and topic_str, summary updated, df has cluster , state clusters updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7472f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await agent.run_step(\"get state\")\n",
    "state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a331961",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_result = await agent.run_step(\"inspect state\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c547291",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await agent.get_state_direct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(status_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdk",
   "language": "python",
   "name": "asdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
