{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71d27e",
   "metadata": {},
   "source": [
    "# Test OpenAI Agents SDK\n",
    "- basic usage\n",
    "- use langfuse for prompt repository, evaluation, observability, user feedback\n",
    "- run batches of prompt async against lists and dataframes\n",
    "- implement a workflow to write a daily AI newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d549de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import pydantic\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "from typing import Dict, TypedDict, Type, List, Optional, Any, Iterable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import langfuse\n",
    "from langfuse import get_client\n",
    "from langfuse import Langfuse\n",
    "from langfuse.openai import openai\n",
    "# from langfuse.openai import AsyncOpenAI\n",
    "import logfire\n",
    "from llm import LangfuseClient\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import agents\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "from agents import (Agent, Runner, Tool, OpenAIResponsesModel, \n",
    "                    ModelSettings, FunctionTool, InputGuardrail, GuardrailFunctionOutput,\n",
    "                    SQLiteSession, set_default_openai_api, set_default_openai_client\n",
    "                   )\n",
    "\n",
    "\n",
    "import tenacity\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from log_handler import SQLiteLogHandler, setup_sqlite_logging, sanitize_error_for_logging\n",
    "from config import LOGDB\n",
    "from llm import LLMagent  # methods to apply prompts async to large batches\n",
    "from fetch import Fetcher # fetch news urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cea80dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI:            1.107.0\n",
      "OpenAI Agents SDK  0.2.11\n",
      "Pydantic           2.11.7\n",
      "LangFuse           3.3.4\n",
      "Logfire            4.7.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"OpenAI:            {openai.__version__}\")\n",
    "print(f\"OpenAI Agents SDK  {agents.__version__}\")\n",
    "print(f\"Pydantic           {pydantic.__version__}\")\n",
    "print(f\"LangFuse           {langfuse.version.__version__}\")\n",
    "print(f\"Logfire            {logfire.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153c333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "# to run async in jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# verbose OpenAI console logging if something doesn't work\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# openai_logger = logging.getLogger(\"openai\")\n",
    "# openai_logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686ed01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:02:28 | NewsletterAgent.newsletter_agent | INFO | Test info message\n",
      "16:02:28 | NewsletterAgent.newsletter_agent | WARNING | Test warning message\n",
      "16:02:28 | NewsletterAgent.newsletter_agent | ERROR | Test error message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'log with some bad stuff for the filter: [API_KEY_REDACTED]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modules create a default logger, or we can pass this logger\n",
    "\n",
    "def setup_logging(session_id: str = \"default\", db_path: str = \"agent_logs.db\") -> logging.Logger:\n",
    "    \"\"\"Set up logging to console and SQLite database.\"\"\"\n",
    "\n",
    "    # Create logger\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logger = logging.getLogger(f\"NewsletterAgent.{session_id}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # SQLite handler\n",
    "    sqlite_handler = SQLiteLogHandler(db_path)\n",
    "    sqlite_handler.setLevel(logging.INFO)\n",
    "    sqlite_formatter = logging.Formatter('%(message)s')\n",
    "    sqlite_handler.setFormatter(sqlite_formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(sqlite_handler)\n",
    "\n",
    "    # Prevent propagation to root logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(\"newsletter_agent\", \"test_logs.db\")\n",
    "\n",
    "# Log some test messages\n",
    "logger.info(\"Test info message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.warning(\"Test warning message\", extra={\n",
    "    'step_name': 'test_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "logger.error(\"Test error message\", extra={\n",
    "    'step_name': 'error_step',\n",
    "    'agent_session': 'demo_session'\n",
    "})\n",
    "\n",
    "sanitize_error_for_logging(\"log with some bad stuff for the filter: sk-proj-123456789012345678901234567890123456789012345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced40b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logfire instrumentation.\n",
    "# OpenAI used logfire from Pydantic AI and we need this for langfuse to handle some traces sent to OpenAI by Agents SDK\n",
    "logfire.configure(\n",
    "    service_name=\"my_agent_service\", \n",
    "    send_to_logfire=False,\n",
    "    console=False,  # Disable console output\n",
    ")\n",
    "logging.getLogger(\"logfire\").setLevel(logging.WARNING)\n",
    "# Set logfire logger to WARNING level to reduce output\n",
    "logfire.instrument_openai_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22329d",
   "metadata": {},
   "source": [
    "# Langfuse Prompt Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc92c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize langfuse for observability\n",
    "# git clone https://github.com/langfuse/langfuse.git\n",
    "# cd langfuse\n",
    "# go to localhost:3000\n",
    "# set up an org, project, get API keys and put in .env\n",
    "\n",
    "lf_client = get_client()\n",
    " \n",
    "# Verify connection\n",
    "if lf_client.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")# Get production prompts\n",
    "prompt = lf_client.get_prompt(\"newsagent/headline_classifier\")\n",
    "\n",
    "# Get prompt from repository by label\n",
    "# You can use as many labels as you'd like to identify different deployment targets\n",
    "prompt = lf_client.get_prompt(\"newsagent/headline_classifier\", label=\"production\")\n",
    "print(prompt.prompt, \"\\n\")\n",
    "prompt = lf_client.get_prompt(\"newsagent/headline_classifier\", label=\"latest\")\n",
    "print(prompt.prompt, \"\\n\")\n",
    "\n",
    "# Get by version number, usually not recommended as it requires code changes to deploy new prompt versions\n",
    "prompt = lf_client.get_prompt(\"newsagent/headline_classifier\", version=1)\n",
    "print(prompt.prompt, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7869e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt, user_prompt, model = LangfuseClient().get_prompt(\"newsagent/headline_classifier\")\n",
    "print(\"system prompt\")\n",
    "print(system_prompt)\n",
    "print() \n",
    "\n",
    "print(\"user prompt\")\n",
    "print(user_prompt)\n",
    "print() \n",
    "\n",
    "print(\"model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742842e",
   "metadata": {},
   "source": [
    "# Basic usage\n",
    "- Run a prompt using agents\n",
    "- Sessions\n",
    "- Route through Langfuse for observability\n",
    "- Save logs\n",
    "- View traces and evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current `production` version of the prompt via raw langfuse client\n",
    "system_prompt = lf_client.get_prompt(\"swallow/system\")\n",
    " \n",
    "# Insert variables into prompt template\n",
    "# compiled_prompt = prompt.compile(criticlevel=\"expert\", movie=\"Dune 2\")\n",
    "\n",
    "system_prompt.prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49680b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this local db to store agent sessions \n",
    "[os.remove(f) for f in glob.glob('swallow.db*') if os.path.exists(f)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a chat using the OpenAI Agent class with a session (and through langfuse for observability)\n",
    "\n",
    "user_prompt = lf_client.get_prompt(\"swallow/user1\").compile()\n",
    "print(user_prompt)\n",
    "\n",
    "openai_client = AsyncOpenAI()\n",
    "\n",
    "# async def main():\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=system_prompt.prompt,\n",
    "    model=OpenAIResponsesModel(model=\"gpt-4.1\", openai_client=openai_client),\n",
    ")\n",
    "\n",
    "# 1) Create (or reuse) a session. Use a durable DB path if you want persistence.\n",
    "session = SQLiteSession(\"test_swallow_chat\", \"swallow.db\")\n",
    "\n",
    "result = await Runner.run(agent, user_prompt, session=session)\n",
    "display(Markdown(result.final_output))\n",
    "\n",
    "# loop = asyncio.get_running_loop()\n",
    "# await loop.create_task(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Next turns — just keep reusing the same session\n",
    "result = await Runner.run(agent, \"explain how that number was measured / computed\", session=session)\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84482ae",
   "metadata": {},
   "source": [
    "### View langfuse trace in the langfuse console\n",
    "\n",
    "![langfuse_trace.png](langfuse_trace.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1194bf7",
   "metadata": {},
   "source": [
    "# More advanced usage\n",
    "- Structured JSON outputs, enables validation and safe passing downstream over long pipelines\n",
    "- Map prompts to larger data sets asynchronously (e.g. send parallel batches of 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f123315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic output class for classifying headlines - request returning values in this class\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"A single headline classification result\"\"\"\n",
    "    input_str: str = Field(description=\"The original headline text\")\n",
    "    output: bool = Field(description=\"Whether the headline is AI-related\")\n",
    "\n",
    "class ClassificationResultList(BaseModel):\n",
    "    \"\"\"List of ClassificationResult for batch processing\"\"\"\n",
    "    results_list: list[ClassificationResult] = Field(description=\"List of classification results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_name = 'newsagent/headline_classifier'\n",
    "lf_prompt = lf_client.get_prompt(prompt_name)\n",
    "print(lf_prompt.prompt, end=\"\\n\")\n",
    "print()\n",
    "\n",
    "system_prompt = lf_prompt.prompt[0]['content']\n",
    "print('system prompt\\n', system_prompt, end=\"\\n\")\n",
    "print()\n",
    "\n",
    "user_prompt = lf_prompt.prompt[1]['content']\n",
    "print('user prompt\\n', user_prompt, end=\"\\n\")\n",
    "\n",
    "config = lf_prompt.config if hasattr(lf_prompt, 'config') else {}\n",
    "print ('config\\n', config, end=\"\\n\")\n",
    "\n",
    "model = config.get(\"model\", 'gpt-5')\n",
    "print('model\\n', model, end=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(manza='tetas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send single prompts via LLMAgent asking for ClassificationResult structured output\n",
    "system_prompt, user_prompt, model = LangfuseClient().get_prompt(\"newsagent/headline_classifier\")\n",
    "\n",
    "classifier = LLMagent(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    output_type=ClassificationResult,\n",
    "    model=model,\n",
    "    verbose=True,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "test_headlines = [\n",
    "    \"AI Is Replacing Online Moderators, But It's Bad at the Job\",\n",
    "    \"Baby Trapped in Refrigerator Eats Own Foot\",\n",
    "    \"Machine Learning Breakthrough in Medical Diagnosis\",\n",
    "    \"Local Restaurant Opens New Location\",\n",
    "    \"ChatGPT Usage Soars in Educational Settings\"\n",
    "]\n",
    "\n",
    "result = await classifier.prompt_dict({'input_str': test_headlines[0]})\n",
    "print(result)\n",
    "result = await classifier.prompt_dict({'input_str': test_headlines[1]})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMAgent in llm.py has multiple ways to request stuff\n",
    "# Suppose we have 1000 headlines in a dataframe and we want to apply a prompt to each one.\n",
    "# Some stuff we might want\n",
    "# - structured output, like ideally apply prompts to this column and put results in a new column\n",
    "# - output validation, so llm doesn't e.g. transpose rows or skip rows\n",
    "# - batching , don't send 1000 at once but don't send a single headline with a large prompt 1000 times\n",
    "# - concurrency / async processing, send many batches at once (but maybe specify some max concurrency)\n",
    "# - retry logic with exponential backoff\n",
    "# LLMagent supports\n",
    "# - prompt_dict to return an object or list\n",
    "# - prompt_batch to map prompt to a list and return structured object\n",
    "# - filter_dataframe, map prompt to a Pandas DataFrame and return a Series for assignment\n",
    "\n",
    "# note different output type\n",
    "classifier = LLMagent(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    output_type=ClassificationResultList,\n",
    "    model=model,\n",
    "    verbose=True,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Format headlines as a single string for batch processing\n",
    "headlines_str = str(test_headlines)\n",
    "result = await classifier.prompt_dict({'input_str': headlines_str})\n",
    "print(f\"Batch result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4498281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batches and send multiple in parallel\n",
    "headlines_df = pd.read_csv(\"test_headlines.csv\")\n",
    "headlines_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb897b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_dataframe\n",
    "\n",
    "FILTER_SYSTEM_PROMPT, FILTER_USER_PROMPT, model = LangfuseClient().get_prompt(\"newsagent/filter_system_prompt\")\n",
    "\n",
    "# output class for classifying headlines\n",
    "class ClassificationResultId(BaseModel):\n",
    "    \"\"\"A single headline classification result\"\"\"\n",
    "    id: int = Field(\"The news item id\")\n",
    "    input_str: str = Field(description=\"The original headline title\")\n",
    "    output: bool = Field(description=\"Whether the headline title is AI-related\")\n",
    "\n",
    "class ClassificationResultIdList(BaseModel):\n",
    "    \"\"\"List of ClassificationResult for batch processing\"\"\"\n",
    "    results_list: list[ClassificationResultId] = Field(description=\"List of classification results\")\n",
    "\n",
    "\n",
    "classifier = LLMagent(\n",
    "    system_prompt=FILTER_SYSTEM_PROMPT,\n",
    "    user_prompt=FILTER_USER_PROMPT,\n",
    "    output_type=ClassificationResultIdList,  \n",
    "    model=model,  # Use a valid model\n",
    "    verbose=False,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "headlines_df['isAI'] = await classifier.filter_dataframe(headlines_df[[\"id\", \"title\"]])\n",
    "\n",
    "headlines_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(headlines_df.loc[headlines_df['isAI']])\n",
    "display(headlines_df.loc[~headlines_df['isAI']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c08467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test various models for speed, accuracy, cost\n",
    "# see costs under tracing\n",
    "# http://localhost:3000/\n",
    "# 'gpt-5-mini' 9.5¢' 16.8¢, 'gpt-4.1-mini 3.2¢' \n",
    "models = ['gpt-5-mini', 'gpt-5-nano', 'gpt-4.1', 'gpt-4.1-mini',]\n",
    "\n",
    "# ground truth to compare\n",
    "correct_df = pd.read_csv(\"headline_classifier_ground_truth.csv\")\n",
    "\n",
    "result_tuples = []\n",
    "\n",
    "for m in models:\n",
    "    print(f\"Starting evaluation for {m}...\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    \n",
    "    with lf_client.start_as_current_span(name=f\"batch_classification_{m}_{timestamp}\") as span:\n",
    "        classifier = LLMagent(system_prompt,\n",
    "                              user_prompt,\n",
    "                              ClassificationResultList,\n",
    "                              m,\n",
    "                              verbose=False)\n",
    "    \n",
    "        # Run classification with tracing\n",
    "        classification_result = await classifier.prompt_batch(\n",
    "            list(headlines_df['title'].to_list())\n",
    "        )\n",
    "        \n",
    "        # Add span metadata\n",
    "        span.update(\n",
    "            input={\"headlines_count\": len(headlines_df)},\n",
    "            metadata={\"model\": m}\n",
    "        )\n",
    "        \n",
    "    # Calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # analyze results\n",
    "    result_df = pd.DataFrame([(z.input_str, z.output) \n",
    "                              for z in classification_result], \n",
    "                             columns=[\"input\", \"output\"])\n",
    "    \n",
    "    \n",
    "    # Merge with ground truth to compare results\n",
    "    comparison_df = result_df.merge(correct_df, on='input', suffixes=('_predicted', '_correct'))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    comparison_df['is_correct'] = comparison_df['output_predicted'] == comparison_df['output_correct']\n",
    "    accuracy = comparison_df['is_correct'].mean()\n",
    "    correct_count = comparison_df['is_correct'].sum()\n",
    "    total_count = len(comparison_df)\n",
    "    \n",
    "    # Find differences\n",
    "    differences_df = comparison_df[~comparison_df['is_correct']].copy()\n",
    "    \n",
    "    print(f\"Completed {m} in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {correct_count}/{total_count} = {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "    if len(differences_df) > 0:\n",
    "        print(f\"Found {len(differences_df)} incorrect predictions:\")\n",
    "        print(\"-\" * 80)\n",
    "        for idx, row in differences_df.iterrows():\n",
    "            print(f\"Input: {row['input']}\")\n",
    "            print(f\"Predicted: {row['output_predicted']}\")\n",
    "            print(f\"Correct:   {row['output_correct']}\")\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"🎉 Perfect accuracy! No incorrect predictions.\")\n",
    "    \n",
    "    print()  # Empty line for readability\n",
    "    \n",
    "    # Create tuple with (model_name, df, elapsed_time, accuracy, differences_df)\n",
    "    result_tuples.append((m, result_df, elapsed_time, accuracy, differences_df))\n",
    "\n",
    "# Summary comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "summary_data = []\n",
    "for model_name, df, elapsed_time, accuracy, differences_df in result_tuples:\n",
    "    rate = len(df) / elapsed_time if elapsed_time > 0 else 0\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{accuracy:.3f}\",\n",
    "        'Accuracy %': f\"{accuracy*100:.1f}%\", \n",
    "        'Correct': f\"{int(accuracy * len(df))}/{len(df)}\",\n",
    "        'Time (s)': f\"{elapsed_time:.2f}\",\n",
    "        'Rate (pred/s)': f\"{rate:.1f}\",\n",
    "        'Errors': len(differences_df)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Find most common errors across all models\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOST COMMON ERRORS ACROSS ALL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "all_errors = []\n",
    "for model_name, _, _, _, differences_df in result_tuples:\n",
    "    for _, row in differences_df.iterrows():\n",
    "        all_errors.append({\n",
    "            'input': row['input'],\n",
    "            'predicted': row['output_predicted'],\n",
    "            'correct': row['output_correct'],\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "if all_errors:\n",
    "    error_df = pd.DataFrame(all_errors)\n",
    "    error_counts = error_df.groupby('input').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Headlines that multiple models got wrong:\")\n",
    "    for headline, count in error_counts.head(10).items():\n",
    "        if count > 1:  # Only show errors made by multiple models\n",
    "            models_wrong = error_df[error_df['input'] == headline]['model'].tolist()\n",
    "            predicted_values = error_df[error_df['input'] == headline]['predicted'].unique()\n",
    "            correct_value = error_df[error_df['input'] == headline]['correct'].iloc[0]\n",
    "            \n",
    "            print(f\"\\n❌ Error in {count}/{len(models)} models: {', '.join(models_wrong)}\")\n",
    "            print(f\"   Headline: {headline}\")\n",
    "            print(f\"   Predicted: {predicted_values}\")\n",
    "            print(f\"   Correct: {correct_value}\")\n",
    "\n",
    "lf_client.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access results:\n",
    "# 'gpt-5-mini' 9.5¢' gpt-4.1 16.8¢, 'gpt-4.1-mini 3.2¢' \n",
    "# note that you can get faster cheaper results with gpt-4.1 mini with good accuracy\n",
    "for model_name, df, elapsed_time, accuracy, error_df in result_tuples:\n",
    "    print(f\"{model_name}: {len(df)} results in {elapsed_time:.2f}s, accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdad2f",
   "metadata": {},
   "source": [
    "# Run Agent Worfklow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6bd46",
   "metadata": {},
   "source": [
    "# test agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abddb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating NewsletterAgent...\n",
      "Initialized NewsletterAgent with persistent state and 9-step workflow\n",
      "Session ID: test_newsletter_20250918160250733084\n",
      "\n",
      "📝 User prompt: 'Show the workflow status'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:02:52 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Starting check_workflow_status\n",
      "16:02:52 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Completed check_workflow_status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "⏱️  Total execution time: 6.84s\n",
      "📊 Final result:\n",
      "Here’s the current newsletter workflow status:\n",
      "\n",
      "Overall progress: 0.0% (0/9 complete)\n",
      "Status summary: 0 complete, 0 started, 0 failed, 9 not started\n",
      "Next step to run: Step 1 — Gather URLs\n",
      "\n",
      "Step-by-step:\n",
      "- Step 1: Gather URLs — not_started\n",
      "- Step 2: Filter URLs — not_started\n",
      "- Step 3: Download Articles — not_started\n",
      "- Step 4: Extract Summaries — not_started\n",
      "- Step 5: Cluster By Topic — not_started\n",
      "- Step 6: Rate Articles — not_started\n",
      "- Step 7: Select Sections — not_started\n",
      "- Step 8: Draft Sections — not_started\n",
      "- Step 9: Finalize Newsletter — not_started\n",
      "\n",
      "Would you like me to start Step 1 (gather URLs) now, run all remaining steps in sequence, or resume from a specific step?\n"
     ]
    }
   ],
   "source": [
    "from news_agent import NewsletterAgent\n",
    "\"\"\"Main function to create agent and run complete workflow\"\"\"\n",
    "print(\"🚀 Creating NewsletterAgent...\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
    "\n",
    "# Set up OpenAI client for the agents SDK\n",
    "set_default_openai_client(AsyncOpenAI(api_key=api_key))\n",
    "\n",
    "# Create agent with persistent state\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "session_id = f\"test_newsletter_{timestamp}\"\n",
    "agent = NewsletterAgent(session_id=session_id, verbose=True)\n",
    "\n",
    "# User prompt to run workflow\n",
    "user_prompt = \"Show the workflow status\"\n",
    "\n",
    "print(f\"\\n📝 User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e89a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 User prompt: 'Run step 1, fetch urls'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:03:01 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Starting check_workflow_status\n",
      "16:03:01 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Completed check_workflow_status\n",
      "16:03:02 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Starting Step 1: Gather URLs\n",
      "2025-09-18 16:03:02,769 - fetcher_4856280208 - INFO - [fetcher_init] Loading sources from sources.yaml\n",
      "2025-09-18 16:03:02,774 - fetcher_4856280208 - INFO - [fetcher_init] Loaded 17 sources: 7 RSS, 9 HTML, 1 API\n",
      "2025-09-18 16:03:02,775 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Ars Technica': type=RSS, url=https://arstechnica.com/ai/\n",
      "2025-09-18 16:03:02,775 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Bloomberg': type=RSS, url=https://www.bloomberg.com/ai\n",
      "2025-09-18 16:03:02,775 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Business Insider': type=html, url=https://www.businessinsider.com/tech\n",
      "2025-09-18 16:03:02,775 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'FT': type=RSS, url=https://www.ft.com/artificial-intelligence\n",
      "2025-09-18 16:03:02,775 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Feedly AI': type=html, url=https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Hacker News': type=RSS, url=https://news.ycombinator.com/\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'HackerNoon': type=RSS, url=https://hackernoon.com/tagged/ai\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'New York Times': type=RSS, url=https://www.nytimes.com/section/technology\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Reddit': type=RSS, url=https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Techmeme': type=RSS, url=https://www.techmeme.com/river\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'The Register': type=RSS, url=https://www.theregister.com/software/ai_ml/\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'The Verge': type=RSS, url=https://www.theverge.com/ai-artificial-intelligence\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'VentureBeat': type=RSS, url=https://venturebeat.com/category/ai/\n",
      "2025-09-18 16:03:02,776 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'WSJ': type=RSS, url=https://www.wsj.com/tech/ai\n",
      "2025-09-18 16:03:02,777 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'Washington Post': type=RSS, url=https://www.washingtonpost.com/technology/innovations/\n",
      "2025-09-18 16:03:02,777 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'The Guardian': type=RSS, url=https://www.theguardian.com/uk/technology\n",
      "2025-09-18 16:03:02,777 - fetcher_4856280208 - DEBUG - [fetcher_sources] Source 'NewsAPI': type=rest, url=https://newsapi.org/v2/everything\n",
      "2025-09-18 16:03:02,777 - fetcher_4856280208 - INFO - [fetcher_init] Fetcher initialized with max_concurrent=8\n",
      "2025-09-18 16:03:02,777 - fetcher_4856280208 - INFO - [fetch_all] Starting fetch_all for 17 sources\n",
      "2025-09-18 16:03:02,777 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from Ars Technica: https://arstechnica.com/ai/feed/\n",
      "2025-09-18 16:03:02,778 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from Bloomberg: https://www.bloomberg.com/ai\n",
      "2025-09-18 16:03:02,778 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from Business Insider: https://www.businessinsider.com/tech\n",
      "2025-09-18 16:03:02,778 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from FT: https://www.ft.com/artificial-intelligence\n",
      "2025-09-18 16:03:02,778 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from Feedly AI: https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ\n",
      "2025-09-18 16:03:02,778 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from Hacker News: https://news.ycombinator.com/rss\n",
      "2025-09-18 16:03:02,778 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from HackerNoon: https://hackernoon.com/tagged/ai/feed\n",
      "2025-09-18 16:03:02,779 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from New York Times: https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\n",
      "2025-09-18 16:03:02,928 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for New York Times: 31 articles\n",
      "2025-09-18 16:03:02,928 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from Reddit: https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day\n",
      "2025-09-18 16:03:03,033 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for HackerNoon: 50 articles\n",
      "2025-09-18 16:03:03,034 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from Techmeme: https://www.techmeme.com/feed.xml\n",
      "2025-09-18 16:03:03,213 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for Hacker News: 30 articles\n",
      "2025-09-18 16:03:03,218 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from The Register: https://www.theregister.com/software/ai_ml/headlines.atom\n",
      "2025-09-18 16:03:03,321 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for The Register: 50 articles\n",
      "2025-09-18 16:03:03,321 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from The Verge: https://www.theverge.com/ai-artificial-intelligence\n",
      "2025-09-18 16:03:03,380 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for Ars Technica: 20 articles\n",
      "2025-09-18 16:03:03,380 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from VentureBeat: https://venturebeat.com/category/ai/\n",
      "2025-09-18 16:03:04,266 - fetcher_4856280208 - INFO - [fetch_html] Source dict for Feedly AI: {'type': 'html', 'url': 'https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ', 'filename': 'Feedly_AI', 'exclude': ['^https://feedly.com', '^https://s1.feedly.com', '^https://blog.feedly.com'], 'scroll': 3, 'initial_sleep': 20, 'scroll_div': '#feedlyFrame'}\n",
      "2025-09-18 16:03:04,266 - fetcher_4856280208 - INFO - Starting scrape_source https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ, Feedly_AI\n",
      "2025-09-18 16:03:04,267 - fetcher_4856280208 - INFO - scrape_url(https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ)\n",
      "2025-09-18 16:03:04,267 - fetcher_4856280208 - INFO - scraping https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ to download/sources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 16:03:04,268 - fetcher_4856280208 - INFO - File already exists: download/sources/Feedly_AI.html\n",
      "2025-09-18 16:03:04,268 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/Feedly_AI.html\n",
      "2025-09-18 16:03:04,322 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/Feedly_AI.html\n",
      "2025-09-18 16:03:04,323 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for Feedly AI: 71 articles\n",
      "2025-09-18 16:03:04,323 - fetcher_4856280208 - INFO - [fetch_html] Source dict for Business Insider: {'type': 'html', 'url': 'https://www.businessinsider.com/tech', 'filename': 'Business_Insider', 'exclude': ['^https://www.insider.com', '^https://www.passionfroot.me']}\n",
      "2025-09-18 16:03:04,323 - fetcher_4856280208 - INFO - Starting scrape_source https://www.businessinsider.com/tech, Business_Insider\n",
      "2025-09-18 16:03:04,324 - fetcher_4856280208 - INFO - scrape_url(https://www.businessinsider.com/tech)\n",
      "2025-09-18 16:03:04,324 - fetcher_4856280208 - INFO - scraping https://www.businessinsider.com/tech to download/sources\n",
      "2025-09-18 16:03:04,324 - fetcher_4856280208 - INFO - File already exists: download/sources/Business_Insider.html\n",
      "2025-09-18 16:03:04,324 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/Business_Insider.html\n",
      "2025-09-18 16:03:04,344 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/Business_Insider.html\n",
      "2025-09-18 16:03:04,345 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for Business Insider: 17 articles\n",
      "2025-09-18 16:03:04,345 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from WSJ: https://www.wsj.com/tech/ai\n",
      "2025-09-18 16:03:04,345 - fetcher_4856280208 - INFO - [fetch_html] Source dict for WSJ: {'type': 'html', 'url': 'https://www.wsj.com/tech/ai', 'filename': 'WSJ', 'rss': 'https://feeds.a.dj.com/rss/RSSWSJD.xml'}\n",
      "2025-09-18 16:03:04,345 - fetcher_4856280208 - INFO - Starting scrape_source https://www.wsj.com/tech/ai, WSJ\n",
      "2025-09-18 16:03:04,345 - fetcher_4856280208 - INFO - scrape_url(https://www.wsj.com/tech/ai)\n",
      "2025-09-18 16:03:04,346 - fetcher_4856280208 - INFO - scraping https://www.wsj.com/tech/ai to download/sources\n",
      "2025-09-18 16:03:04,346 - fetcher_4856280208 - INFO - File already exists: download/sources/WSJ.html\n",
      "2025-09-18 16:03:04,346 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/WSJ.html\n",
      "2025-09-18 16:03:04,469 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/WSJ.html\n",
      "2025-09-18 16:03:04,469 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for WSJ: 28 articles\n",
      "2025-09-18 16:03:04,469 - fetcher_4856280208 - INFO - [fetch_html] Source dict for Bloomberg: {'type': 'html', 'url': 'https://www.bloomberg.com/ai', 'filename': 'Bloomberg', 'include': ['^https://www.bloomberg.com/news/'], 'rss': 'https://feeds.bloomberg.com/technology/news.rss'}\n",
      "2025-09-18 16:03:04,469 - fetcher_4856280208 - INFO - Starting scrape_source https://www.bloomberg.com/ai, Bloomberg\n",
      "2025-09-18 16:03:04,470 - fetcher_4856280208 - INFO - scrape_url(https://www.bloomberg.com/ai)\n",
      "2025-09-18 16:03:04,470 - fetcher_4856280208 - INFO - scraping https://www.bloomberg.com/ai to download/sources\n",
      "2025-09-18 16:03:04,470 - fetcher_4856280208 - INFO - File already exists: download/sources/Bloomberg.html\n",
      "2025-09-18 16:03:04,470 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/Bloomberg.html\n",
      "2025-09-18 16:03:04,493 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/Bloomberg.html\n",
      "2025-09-18 16:03:04,494 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for Bloomberg: 33 articles\n",
      "2025-09-18 16:03:04,494 - fetcher_4856280208 - INFO - [fetch_html] Fetching HTML from Washington Post: https://www.washingtonpost.com/technology/innovations/\n",
      "2025-09-18 16:03:04,494 - fetcher_4856280208 - INFO - [fetch_html] Source dict for Washington Post: {'type': 'html', 'url': 'https://www.washingtonpost.com/technology/innovations/', 'filename': 'Washington_Post', 'include': ['https://www.washingtonpost.com/(\\\\w+)/(\\\\d+)/(\\\\d+)/(\\\\d+)/'], 'rss': 'https://feeds.washingtonpost.com/rss/business/technology'}\n",
      "2025-09-18 16:03:04,494 - fetcher_4856280208 - INFO - Starting scrape_source https://www.washingtonpost.com/technology/innovations/, Washington_Post\n",
      "2025-09-18 16:03:04,495 - fetcher_4856280208 - INFO - scrape_url(https://www.washingtonpost.com/technology/innovations/)\n",
      "2025-09-18 16:03:04,495 - fetcher_4856280208 - INFO - scraping https://www.washingtonpost.com/technology/innovations/ to download/sources\n",
      "2025-09-18 16:03:04,495 - fetcher_4856280208 - INFO - File already exists: download/sources/Washington_Post.html\n",
      "2025-09-18 16:03:04,495 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/Washington_Post.html\n",
      "2025-09-18 16:03:04,516 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/Washington_Post.html\n",
      "2025-09-18 16:03:04,516 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for Washington Post: 31 articles\n",
      "2025-09-18 16:03:04,516 - fetcher_4856280208 - INFO - [fetch_rss] Fetching RSS from The Guardian: https://www.theguardian.com/uk/technology/rss\n",
      "2025-09-18 16:03:04,517 - fetcher_4856280208 - INFO - [fetch_html] Source dict for FT: {'type': 'html', 'url': 'https://www.ft.com/artificial-intelligence', 'filename': 'FT', 'include': ['https://www.ft.com/content/'], 'rss': 'https://www.ft.com/artificial-intelligence?format=rss'}\n",
      "2025-09-18 16:03:04,517 - fetcher_4856280208 - INFO - Starting scrape_source https://www.ft.com/artificial-intelligence, FT\n",
      "2025-09-18 16:03:04,517 - fetcher_4856280208 - INFO - scrape_url(https://www.ft.com/artificial-intelligence)\n",
      "2025-09-18 16:03:04,517 - fetcher_4856280208 - INFO - scraping https://www.ft.com/artificial-intelligence to download/sources\n",
      "2025-09-18 16:03:04,518 - fetcher_4856280208 - INFO - File already exists: download/sources/FT.html\n",
      "2025-09-18 16:03:04,519 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/FT.html\n",
      "2025-09-18 16:03:04,546 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/FT.html\n",
      "2025-09-18 16:03:04,546 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for FT: 103 articles\n",
      "2025-09-18 16:03:04,546 - fetcher_4856280208 - INFO - [newsapi] Fetching top 100 stories matching artificial intelligence since 2025-09-17T16:03:04 from NewsAPI\n",
      "2025-09-18 16:03:04,767 - fetcher_4856280208 - INFO - [fetch_html] Source dict for Reddit: {'type': 'html', 'url': 'https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day', 'filename': 'Reddit', 'exclude': ['^https://www.reddit.com/user/', '^https://www.reddit.com/domain/', '^https://chat.reddit.com/', '^https://i.redd.it/', '^https://redditblog.com/', '^https://www.redditinc.com/', '^https://www.reddithelp.com/', '^https://itunes.apple.com/', '^https://play.google.com/', '^https?://(?:www\\\\.)?reddit\\\\.com/r/([^/]+)/?$', '^https://www.reddit.com/help/', '^https://www.reddit.com/contact/', '^https://www.reddit.com/rules/'], 'scroll': 5, 'minlength': 8, 'rss': 'https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/.rss?sort=top&t=day'}\n",
      "2025-09-18 16:03:04,767 - fetcher_4856280208 - INFO - Starting scrape_source https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day, Reddit\n",
      "2025-09-18 16:03:04,767 - fetcher_4856280208 - INFO - scrape_url(https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 16:03:04,767 - fetcher_4856280208 - INFO - scraping https://www.reddit.com/r/AI_Agents+ArtificialInteligence+Automate+ChatGPT+ChatGPTCoding+Futurology+MachineLearning+OpenAI+ProgrammerHumor+accelerate+aiArt+aivideo+artificial+deeplearning+learnmachinelearning+programming+singularity+tech+technews+technology/top/?sort=top&t=day to download/sources\n",
      "2025-09-18 16:03:04,767 - fetcher_4856280208 - INFO - File already exists: download/sources/Reddit.html\n",
      "2025-09-18 16:03:04,768 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/Reddit.html\n",
      "2025-09-18 16:03:04,795 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/Reddit.html\n",
      "2025-09-18 16:03:04,796 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for Reddit: 57 articles\n",
      "2025-09-18 16:03:04,796 - fetcher_4856280208 - INFO - [fetch_html] Source dict for The Verge: {'type': 'html', 'url': 'https://www.theverge.com/ai-artificial-intelligence', 'filename': 'The_Verge', 'include': ['^https://www.theverge.com/news'], 'rss': 'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml'}\n",
      "2025-09-18 16:03:04,796 - fetcher_4856280208 - INFO - Starting scrape_source https://www.theverge.com/ai-artificial-intelligence, The_Verge\n",
      "2025-09-18 16:03:04,796 - fetcher_4856280208 - INFO - scrape_url(https://www.theverge.com/ai-artificial-intelligence)\n",
      "2025-09-18 16:03:04,796 - fetcher_4856280208 - INFO - scraping https://www.theverge.com/ai-artificial-intelligence to download/sources\n",
      "2025-09-18 16:03:04,797 - fetcher_4856280208 - INFO - File already exists: download/sources/The_Verge.html\n",
      "2025-09-18 16:03:04,797 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/The_Verge.html\n",
      "2025-09-18 16:03:04,839 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/The_Verge.html\n",
      "2025-09-18 16:03:04,839 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for The Verge: 27 articles\n",
      "2025-09-18 16:03:04,839 - fetcher_4856280208 - INFO - [fetch_html] Source dict for VentureBeat: {'type': 'html', 'url': 'https://venturebeat.com/category/ai/', 'filename': 'VentureBeat', 'rss': 'https://venturebeat.com/category/ai/feed/'}\n",
      "2025-09-18 16:03:04,839 - fetcher_4856280208 - INFO - Starting scrape_source https://venturebeat.com/category/ai/, VentureBeat\n",
      "2025-09-18 16:03:04,839 - fetcher_4856280208 - INFO - scrape_url(https://venturebeat.com/category/ai/)\n",
      "2025-09-18 16:03:04,840 - fetcher_4856280208 - INFO - scraping https://venturebeat.com/category/ai/ to download/sources\n",
      "2025-09-18 16:03:04,840 - fetcher_4856280208 - INFO - File already exists: download/sources/VentureBeat.html\n",
      "2025-09-18 16:03:04,840 - fetcher_4856280208 - INFO - [fetch_html] Parsing HTML file: download/sources/VentureBeat.html\n",
      "2025-09-18 16:03:04,847 - fetcher_4856280208 - INFO - [fetch_html] Parsed HTML file: download/sources/VentureBeat.html\n",
      "2025-09-18 16:03:04,848 - fetcher_4856280208 - INFO - [fetch_html] HTML fetch successful for VentureBeat: 13 articles\n",
      "2025-09-18 16:03:04,922 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for The Guardian: 24 articles\n",
      "2025-09-18 16:03:12,013 - fetcher_4856280208 - INFO - [fetch_rss] RSS fetch successful for Techmeme: 15 articles\n",
      "2025-09-18 16:03:12,013 - fetcher_4856280208 - INFO - [fetch_all] fetch_all completed: 17 successful, 0 failed, 700 total articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FT</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedly AI</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hacker News</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HackerNoon</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New York Times</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Techmeme</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Guardian</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Register</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VentureBeat</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WSJ</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Washington Post</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source  url\n",
       "0       Ars Technica   20\n",
       "1          Bloomberg   33\n",
       "2   Business Insider   17\n",
       "3                 FT  103\n",
       "4          Feedly AI   71\n",
       "5        Hacker News   30\n",
       "6         HackerNoon   50\n",
       "7     New York Times   31\n",
       "8            NewsAPI  100\n",
       "9             Reddit   57\n",
       "10          Techmeme   15\n",
       "11      The Guardian   24\n",
       "12      The Register   50\n",
       "13         The Verge   27\n",
       "14       VentureBeat   13\n",
       "15               WSJ   28\n",
       "16   Washington Post   31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Step 1: Gathered 700 URLs from 17 RSS sources\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>rss_summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Google announces massive expansion of AI featu...</td>\n",
       "      <td>https://arstechnica.com/google/2025/09/google-...</td>\n",
       "      <td>Thu, 18 Sep 2025 19:27:58 +0000</td>\n",
       "      <td>Chrome's future as an AI browser starts today.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>New attack on ChatGPT research agent pilfers s...</td>\n",
       "      <td>https://arstechnica.com/information-technology...</td>\n",
       "      <td>Thu, 18 Sep 2025 16:29:22 +0000</td>\n",
       "      <td>Unlike most prompt injections, ShadowLeak exec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>White House officials reportedly frustrated by...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/white-house...</td>\n",
       "      <td>Wed, 17 Sep 2025 22:03:11 +0000</td>\n",
       "      <td>Officials say Claude chatbot usage policies bl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Gemini AI solves coding problem that stumped 1...</td>\n",
       "      <td>https://arstechnica.com/google/2025/09/google-...</td>\n",
       "      <td>Wed, 17 Sep 2025 17:00:32 +0000</td>\n",
       "      <td>Gemini shows off at another high-level academi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>After child’s trauma, chatbot maker allegedly ...</td>\n",
       "      <td>https://arstechnica.com/tech-policy/2025/09/af...</td>\n",
       "      <td>Wed, 17 Sep 2025 16:45:48 +0000</td>\n",
       "      <td>\"I know my kid\": Parents urge lawmakers to shu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>TA558 Uses AI-Generated Scripts to Deploy Veno...</td>\n",
       "      <td>https://thehackernews.com/2025/09/ta558-uses-a...</td>\n",
       "      <td>2025-09-17T18:30:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Supercharge your organization’s productivity w...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>2025-09-17T19:37:32Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Data center developer asks judge to block Lanc...</td>\n",
       "      <td>https://lancasteronline.com/news/local/data-ce...</td>\n",
       "      <td>2025-09-17T17:15:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>AI could drive 40% growth in global trade by 2...</td>\n",
       "      <td>https://www.citizen.digital/business/ai-could-...</td>\n",
       "      <td>2025-09-17T16:10:02Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>AI can forecast your future health – just like...</td>\n",
       "      <td>https://www.bbc.co.uk/news/articles/cx2pj502ev6o</td>\n",
       "      <td>2025-09-17T18:35:30Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                              title  \\\n",
       "0    Ars Technica  Google announces massive expansion of AI featu...   \n",
       "1    Ars Technica  New attack on ChatGPT research agent pilfers s...   \n",
       "2    Ars Technica  White House officials reportedly frustrated by...   \n",
       "3    Ars Technica  Gemini AI solves coding problem that stumped 1...   \n",
       "4    Ars Technica  After child’s trauma, chatbot maker allegedly ...   \n",
       "..            ...                                                ...   \n",
       "695       NewsAPI  TA558 Uses AI-Generated Scripts to Deploy Veno...   \n",
       "696       NewsAPI  Supercharge your organization’s productivity w...   \n",
       "697       NewsAPI  Data center developer asks judge to block Lanc...   \n",
       "698       NewsAPI  AI could drive 40% growth in global trade by 2...   \n",
       "699       NewsAPI  AI can forecast your future health – just like...   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://arstechnica.com/google/2025/09/google-...   \n",
       "1    https://arstechnica.com/information-technology...   \n",
       "2    https://arstechnica.com/ai/2025/09/white-house...   \n",
       "3    https://arstechnica.com/google/2025/09/google-...   \n",
       "4    https://arstechnica.com/tech-policy/2025/09/af...   \n",
       "..                                                 ...   \n",
       "695  https://thehackernews.com/2025/09/ta558-uses-a...   \n",
       "696  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "697  https://lancasteronline.com/news/local/data-ce...   \n",
       "698  https://www.citizen.digital/business/ai-could-...   \n",
       "699   https://www.bbc.co.uk/news/articles/cx2pj502ev6o   \n",
       "\n",
       "                           published  \\\n",
       "0    Thu, 18 Sep 2025 19:27:58 +0000   \n",
       "1    Thu, 18 Sep 2025 16:29:22 +0000   \n",
       "2    Wed, 17 Sep 2025 22:03:11 +0000   \n",
       "3    Wed, 17 Sep 2025 17:00:32 +0000   \n",
       "4    Wed, 17 Sep 2025 16:45:48 +0000   \n",
       "..                               ...   \n",
       "695             2025-09-17T18:30:00Z   \n",
       "696             2025-09-17T19:37:32Z   \n",
       "697             2025-09-17T17:15:00Z   \n",
       "698             2025-09-17T16:10:02Z   \n",
       "699             2025-09-17T18:35:30Z   \n",
       "\n",
       "                                           rss_summary   id  \n",
       "0       Chrome's future as an AI browser starts today.    0  \n",
       "1    Unlike most prompt injections, ShadowLeak exec...    1  \n",
       "2    Officials say Claude chatbot usage policies bl...    2  \n",
       "3    Gemini shows off at another high-level academi...    3  \n",
       "4    \"I know my kid\": Parents urge lawmakers to shu...    4  \n",
       "..                                                 ...  ...  \n",
       "695                                                NaN  695  \n",
       "696                                                NaN  696  \n",
       "697                                                NaN  697  \n",
       "698                                                NaN  698  \n",
       "699                                                NaN  699  \n",
       "\n",
       "[700 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:03:12 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Completed Step 1: Gathered 700 articles\n",
      "16:03:13 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Starting check_workflow_status\n",
      "16:03:13 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Completed check_workflow_status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "⏱️  Total execution time: 17.78s\n",
      "📊 Final result:\n",
      "Step 1 completed: I fetched 700 articles from 17 RSS sources and stored them in persistent state.\n",
      "\n",
      "Current workflow status:\n",
      "- Progress: 11.1% (1/9 complete)\n",
      "- Next step: Step 2 — Filter URLs (not started)\n",
      "- Data summary: Total articles = 700; AI-related = 0 (not yet filtered)\n",
      "\n",
      "Would you like me to proceed to Step 2 (filter to AI-related content) now?\n"
     ]
    }
   ],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Run step 1, fetch urls\"\n",
    "\n",
    "print(f\"\\n📝 User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd97adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await agent.get_state_direct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04860b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>rss_summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Google announces massive expansion of AI featu...</td>\n",
       "      <td>https://arstechnica.com/google/2025/09/google-...</td>\n",
       "      <td>Thu, 18 Sep 2025 19:27:58 +0000</td>\n",
       "      <td>Chrome's future as an AI browser starts today.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>New attack on ChatGPT research agent pilfers s...</td>\n",
       "      <td>https://arstechnica.com/information-technology...</td>\n",
       "      <td>Thu, 18 Sep 2025 16:29:22 +0000</td>\n",
       "      <td>Unlike most prompt injections, ShadowLeak exec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>White House officials reportedly frustrated by...</td>\n",
       "      <td>https://arstechnica.com/ai/2025/09/white-house...</td>\n",
       "      <td>Wed, 17 Sep 2025 22:03:11 +0000</td>\n",
       "      <td>Officials say Claude chatbot usage policies bl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Gemini AI solves coding problem that stumped 1...</td>\n",
       "      <td>https://arstechnica.com/google/2025/09/google-...</td>\n",
       "      <td>Wed, 17 Sep 2025 17:00:32 +0000</td>\n",
       "      <td>Gemini shows off at another high-level academi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>After child’s trauma, chatbot maker allegedly ...</td>\n",
       "      <td>https://arstechnica.com/tech-policy/2025/09/af...</td>\n",
       "      <td>Wed, 17 Sep 2025 16:45:48 +0000</td>\n",
       "      <td>\"I know my kid\": Parents urge lawmakers to shu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>TA558 Uses AI-Generated Scripts to Deploy Veno...</td>\n",
       "      <td>https://thehackernews.com/2025/09/ta558-uses-a...</td>\n",
       "      <td>2025-09-17T18:30:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Supercharge your organization’s productivity w...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>2025-09-17T19:37:32Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>Data center developer asks judge to block Lanc...</td>\n",
       "      <td>https://lancasteronline.com/news/local/data-ce...</td>\n",
       "      <td>2025-09-17T17:15:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>AI could drive 40% growth in global trade by 2...</td>\n",
       "      <td>https://www.citizen.digital/business/ai-could-...</td>\n",
       "      <td>2025-09-17T16:10:02Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>AI can forecast your future health – just like...</td>\n",
       "      <td>https://www.bbc.co.uk/news/articles/cx2pj502ev6o</td>\n",
       "      <td>2025-09-17T18:35:30Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                              title  \\\n",
       "0    Ars Technica  Google announces massive expansion of AI featu...   \n",
       "1    Ars Technica  New attack on ChatGPT research agent pilfers s...   \n",
       "2    Ars Technica  White House officials reportedly frustrated by...   \n",
       "3    Ars Technica  Gemini AI solves coding problem that stumped 1...   \n",
       "4    Ars Technica  After child’s trauma, chatbot maker allegedly ...   \n",
       "..            ...                                                ...   \n",
       "695       NewsAPI  TA558 Uses AI-Generated Scripts to Deploy Veno...   \n",
       "696       NewsAPI  Supercharge your organization’s productivity w...   \n",
       "697       NewsAPI  Data center developer asks judge to block Lanc...   \n",
       "698       NewsAPI  AI could drive 40% growth in global trade by 2...   \n",
       "699       NewsAPI  AI can forecast your future health – just like...   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://arstechnica.com/google/2025/09/google-...   \n",
       "1    https://arstechnica.com/information-technology...   \n",
       "2    https://arstechnica.com/ai/2025/09/white-house...   \n",
       "3    https://arstechnica.com/google/2025/09/google-...   \n",
       "4    https://arstechnica.com/tech-policy/2025/09/af...   \n",
       "..                                                 ...   \n",
       "695  https://thehackernews.com/2025/09/ta558-uses-a...   \n",
       "696  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "697  https://lancasteronline.com/news/local/data-ce...   \n",
       "698  https://www.citizen.digital/business/ai-could-...   \n",
       "699   https://www.bbc.co.uk/news/articles/cx2pj502ev6o   \n",
       "\n",
       "                           published  \\\n",
       "0    Thu, 18 Sep 2025 19:27:58 +0000   \n",
       "1    Thu, 18 Sep 2025 16:29:22 +0000   \n",
       "2    Wed, 17 Sep 2025 22:03:11 +0000   \n",
       "3    Wed, 17 Sep 2025 17:00:32 +0000   \n",
       "4    Wed, 17 Sep 2025 16:45:48 +0000   \n",
       "..                               ...   \n",
       "695             2025-09-17T18:30:00Z   \n",
       "696             2025-09-17T19:37:32Z   \n",
       "697             2025-09-17T17:15:00Z   \n",
       "698             2025-09-17T16:10:02Z   \n",
       "699             2025-09-17T18:35:30Z   \n",
       "\n",
       "                                           rss_summary   id  \n",
       "0       Chrome's future as an AI browser starts today.    0  \n",
       "1    Unlike most prompt injections, ShadowLeak exec...    1  \n",
       "2    Officials say Claude chatbot usage policies bl...    2  \n",
       "3    Gemini shows off at another high-level academi...    3  \n",
       "4    \"I know my kid\": Parents urge lawmakers to shu...    4  \n",
       "..                                                 ...  ...  \n",
       "695                                                NaN  695  \n",
       "696                                                NaN  696  \n",
       "697                                                NaN  697  \n",
       "698                                                NaN  698  \n",
       "699                                                NaN  699  \n",
       "\n",
       "[700 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(state.headline_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81182012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:04:06 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Starting check_workflow_status\n",
      "16:04:06 | NewsletterAgent.test_newsletter_20250918160250733084 | INFO | Completed check_workflow_status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKFLOW STATUS (FROM PERSISTENT STATE)\n",
      "Progress: 11.1% (1/9 complete)\n",
      "Status Summary: 1 complete, 0 started, 0 failed, 8 not started\n",
      "Next Step: Step  1: Fetch Urls\n",
      "\n",
      "Step Details:\n",
      "  Step  1: Fetch Urls: complete\n",
      "  Step  2: Filter Urls: not_started\n",
      "  Step  3: Download Articles: not_started\n",
      "  Step  4: Extract Summaries: not_started\n",
      "  Step  5: Cluster By Topic: not_started\n",
      "  Step  6: Rate Articles: not_started\n",
      "  Step  7: Select Sections: not_started\n",
      "  Step  8: Draft Sections: not_started\n",
      "  Step  9: Finalize Newsletter: not_started\n",
      "\n",
      "Data Summary:\n",
      "  Total articles: 700\n",
      "  AI-related: 0\n",
      "  Clusters: 0\n",
      "  Sections: 0\n"
     ]
    }
   ],
   "source": [
    "print(await agent.run_tool_direct(\"check_workflow_status\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c823e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Run step 2, filter urls\"\n",
    "\n",
    "print(f\"\\n📝 User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Run step 3, download full articles\"\n",
    "\n",
    "print(f\"\\n📝 User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Run step 4, Summarize articles\"\n",
    "\n",
    "print(f\"\\n📝 User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt to run workflow\n",
    "user_prompt = \"Show the workflow status\"\n",
    "\n",
    "print(f\"\\n📝 User prompt: '{user_prompt}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the agent with persistent state\n",
    "start_time = time.time()\n",
    "result = await agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7472f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await agent.run_step(\"get state\")\n",
    "state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c547291",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await agent.get_state_direct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = NewsletterAgent(session_id=\"test_newsletter_20250918142308630453\", verbose=True)\n",
    "status_result = await agent.run_step(\"check workflow status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a331961",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_result = await agent.run_step(\"inspect state\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(status_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos\n",
    "# be less verbose\n",
    "# show current headlines \n",
    "# try to call detailed state inspection tool from a prompt , showing count by source \n",
    "# get prompts from langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc786240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store agent state from step to step\n",
    "from newsletter_state import NewsletterAgentState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ba5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "# if 'fetch' in sys.modules:\n",
    "#     del sys.modules['fetch']\n",
    "#     # Delete the reference\n",
    "#     del Fetcher\n",
    "from fetch import Fetcher\n",
    "\n",
    "# should probably do this in the initialization based on parameters --nofetch\n",
    "destination = \"download/sources/\"\n",
    "for file in os.listdir(destination):\n",
    "    file_path = os.path.join(destination, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n",
    "        logger.info(f\"Removed existing file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29484488",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with Fetcher() as f:\n",
    "     z = await f.fetch_all()\n",
    "z \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5838ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in z:\n",
    "    print(src['source'])\n",
    "    print(src['status'])\n",
    "    print(src['metadata'])\n",
    "    print(len(src['results']))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_results = z\n",
    "successful_sources = []\n",
    "failed_sources = []\n",
    "all_articles = []\n",
    "\n",
    "for result in sources_results:\n",
    "    if result['status'] == 'success' and result['results']:\n",
    "        # Add source info to each article\n",
    "        successful_sources.append(result['source'])\n",
    "        all_articles.extend(result['results'])\n",
    "    else:\n",
    "        failed_sources.append(result['source'])\n",
    "        \n",
    "headline_data = all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "len(all_articles)\n",
    "headline_df = pd.DataFrame(all_articles)\n",
    "display(headline_df[[\"source\", \"url\"]].groupby(\"source\") \\\n",
    "    .count() \\\n",
    "    .reset_index() \\\n",
    "    .rename({'url': 'count'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.sources.get('Ars Technica')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_agent import NewsletterAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_agent = NewsletterAgent(session_id=f\"newsletter_{random.randint(10000000, 99999999)}\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ca518",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Can you show me the current status of the newsletter workflow\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = await news_agent.run_step(user_prompt)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"⏱️  Total execution time: {duration:.2f}s\")\n",
    "print(f\"📊 Final result:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f470a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock context\n",
    "class MockContext:\n",
    "    def __init__(self):\n",
    "        self.context = news_agent.default_state\n",
    "\n",
    "ctx = MockContext()\n",
    "current_state = ctx.context  # From your previous run, or reload it\n",
    "df = current_state.headline_df\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    current_state = news_agent.session.get_state()\n",
    "except:\n",
    "    current_state = news_agent.default_state\n",
    "\n",
    "print(current_state)\n",
    "print()\n",
    "\n",
    "print(f\"Current Step: {current_state.current_step}/9\")\n",
    "print(f\"Workflow Complete: {current_state.workflow_complete}\")\n",
    "print(f\"Progress: {(current_state.current_step/9)*100:.1f}%\")\n",
    "print(f\"Total articles: {len(current_state.headline_data)}\")\n",
    "\n",
    "if current_state.headline_data:\n",
    "    ai_related = sum(1 for a in current_state.headline_data if a.get('ai_related') is True)\n",
    "    print(f\"AI-related articles: {ai_related}\")\n",
    "    print(f\"Summaries: {len(current_state.article_summaries)}\")\n",
    "    print(f\"Clusters: {len(current_state.topic_clusters)}\")\n",
    "    print(f\"Sections: {len(current_state.newsletter_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review slides\n",
    "\n",
    "# review workflow status, move to a moadule\n",
    "# all prints should be logs\n",
    "# section writing and composition will have the critic /optimizer loop\n",
    "# add batch with async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_news_dataframe():\n",
    "    \"\"\"\n",
    "    Creates an empty DataFrame to support headline/article analysis\n",
    "    - URLs, source tracking and metadata\n",
    "    - Topic classification and clustering\n",
    "    - Content quality ratings and rankings\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Empty DataFrame with predefined column structure\n",
    "    \"\"\"\n",
    "\n",
    "    # column structure\n",
    "    column_dict = {\n",
    "        # Core identifiers and source info\n",
    "        'article_id': 'object',              # Unique identifier for each article\n",
    "        'source':     'object',              # Source category\n",
    "        'headline_title': 'object',          # Article headline/title\n",
    "        'original_url': 'object',            # Initial URL before redirects\n",
    "        'final_url': 'object',               # URL after following redirects\n",
    "        'domain_name': 'category',           # Website domain\n",
    "        'site_name': 'category',             # Human-readable site name\n",
    "        'site_reputation_score': 'float32',  # Reputation/trustworthiness score for the site\n",
    "        'keep_flag': 'boolean',\n",
    "\n",
    "        # File paths and storage\n",
    "        'html_file_path': 'object',          # Path to stored HTML content\n",
    "        'text_file_path': 'object',          # Path to extracted text content\n",
    "\n",
    "        # Time information\n",
    "        'last_updated_timestamp': 'datetime64[ns]',  # When article was last updated\n",
    "        'article_age_days': 'int32',         # Age of article in days\n",
    "        'recency_score': 'float32',          # Calculated recency score (higher = more recent)\n",
    "\n",
    "        # Content analysis\n",
    "        'content_summary': 'object',         # Generated summary of article content\n",
    "        'bullet_points': 'object',           # Key points extracted as bullets\n",
    "        'article_length_chars': 'int32',     # Character count of article content\n",
    "\n",
    "        # Rating flags (LLM-generated probabilities)\n",
    "        'is_high_quality': 'float32',        # LLM probability for low-quality content\n",
    "        'is_off_topic': 'float32',           # LLM probability for off-topic content\n",
    "        'is_low_importance': 'float32',      # 1-LLM probability for high-importance content\n",
    "\n",
    "        # Other ratings\n",
    "        'bradley_terry_score': 'float32',    # Bradley-Terry rating from pairwise article comparisons\n",
    "        'bradley_terry_rank': 'int32',       # Ordinal rank based on Bradley-Terry scores (1 = highest rated)\n",
    "        'adjusted_length_score': 'float32',  # Length-adjusted quality score\n",
    "        'final_composite_rating': 'float32', # Final weighted rating combining multiple factors\n",
    "\n",
    "        # Topic classification\n",
    "        'topic_string': 'object',            # Topic labels as comma-separated string\n",
    "        'topic_list': 'object',              # Topic labels as list/array structure (same topics, different format)\n",
    "\n",
    "        # Organization and clustering (HDBSCAN-based)\n",
    "        'display_order': 'int32',            # Order for display/presentation\n",
    "        'cluster_id': 'int32',               # HDBSCAN cluster identifier (-1 = noise/outlier)\n",
    "        'cluster_label': 'category'          # Human-readable cluster name/description\n",
    "    }\n",
    "\n",
    "    # Create empty DataFrame from column dictionary\n",
    "    df = pd.DataFrame(columns=list(column_dict.keys())).astype(column_dict)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a22a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsletterState:\n",
    "    \"\"\"\n",
    "    Maintains session state for the OpenAI Agents SDK workflow.\n",
    "\n",
    "    Attributes:\n",
    "        headline_df: DataFrame containing headline data for processing\n",
    "        sources_file: Path to YAML file containing source configurations\n",
    "        sources: Dictionary of source configurations loaded from YAML\n",
    "        cluster_topics: List of clean topic names for headline categorization\n",
    "        max_edits: Maximum number of critic optimizer editing iterations allowed\n",
    "        edit_complete: Boolean flag indicating if editing process is finished\n",
    "        n_browsers: Number of concurrent Playwright browser instances for downloads\n",
    "    \"\"\"\n",
    "\n",
    "    status: WorkflowStatus = WorkflowStatus()\n",
    "    headline_df: pd.DataFrame = field(default_factory=create_news_dataframe)\n",
    "    sources_file: str = field(default=\"sources.yaml\")\n",
    "    sources: Dict[str, Any] = field(default_factory=dict)\n",
    "    cluster_topics: List[str] = field(default_factory=list)\n",
    "    max_edits: int = field(default=3)\n",
    "    edit_complete: bool = field(default=False)\n",
    "    n_browsers: int = field(default=8)\n",
    "    verbose: bool = field(default=True)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization validation and setup.\n",
    "\n",
    "        Validates that the configuration makes sense and performs\n",
    "        any necessary initialization steps.\n",
    "        \"\"\"\n",
    "        # Validate max_edits is reasonable\n",
    "        if self.max_edits < 1 or self.max_edits > 10:\n",
    "            raise ValueError(f\"max_edits should be between 1-10, got {self.max_edits}\")\n",
    "\n",
    "        # Validate n_browsers is reasonable\n",
    "        if self.n_browsers < 1 or self.n_browsers > 32:\n",
    "            raise ValueError(f\"n_browsers should be between 1-32, got {self.n_browsers}\")\n",
    "\n",
    "        # Validate sources_file exists and load sources from file automatically\n",
    "        try:\n",
    "            sources_path = Path(self.sources_file)\n",
    "            with open(sources_path, 'r', encoding='utf-8') as file:\n",
    "                self.sources = yaml.safe_load(file) or {}\n",
    "            if self.verbose:\n",
    "                print(f\"Loaded {len(self.sources)} sources from {self.sources_file}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Sources file not found: {self.sources_file}\")\n",
    "        except yaml.YAMLError as e:\n",
    "            raise ValueError(f\"Error parsing YAML file {self.sources_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = NewsletterState()\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, SQLiteSession, function_tool, RunContextWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c475f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsletterAgent(Agent[NewsletterState]):\n",
    "    \"\"\"AI newsletter writing agent with structured workflow\"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str = \"newsletter_agent\"):\n",
    "        self.session = SQLiteSession(session_id, \"newsletter.db\")\n",
    "        self.state = NewsletterState()\n",
    "\n",
    "        super().__init__(\n",
    "            name=\"AINewsletterAgent\",\n",
    "            instructions=\"\"\"\n",
    "            You are an AI newsletter writing agent. Your role is to:\n",
    "            1. Scrape headlines and URLs from various sources\n",
    "            2. Filter the headlines to ones that are about AI\n",
    "            3. Fetch the URLs and save them as plain text\n",
    "            4. Summarize each article to 3 bullet points containing the key facts\n",
    "            5. Extract topics from each article and cluster articles by topic\n",
    "            6. Rate each article according to the provided rubric\n",
    "            7. Identify 6-15 thematic sections + \"Other News\", assign articles to sections and deduplicate\n",
    "            8. Write each section\n",
    "            9. Combine sections and polish\n",
    "\n",
    "            Use the tools available to accomplish these tasks in order.\n",
    "            Always maintain context about workflow progress and data.\n",
    "            Guide users through the workflow steps systematically.\n",
    "            \"\"\",\n",
    "            tools=[\n",
    "                self.step1_scrape_headlines,\n",
    "                self.step2_filter_ai_headlines,\n",
    "                self.step3_fetch_article_texts,\n",
    "                self.step4_summarize_articles,\n",
    "                self.step5_extract_and_cluster_topics,\n",
    "                self.step6_rate_articles,\n",
    "                self.step7_organize_sections,\n",
    "                self.step8_write_sections,\n",
    "                self.step9_finalize_newsletter,\n",
    "                self.get_workflow_status,\n",
    "                self.run_complete_workflow,\n",
    "                self.reset_workflow\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @function_tool\n",
    "    async def step1_scrape_headlines(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        sources: List[str] = None,\n",
    "        max_articles_per_source: int = 50\n",
    "    ) -> str:\n",
    "        \"\"\"Step 1: Scrape headlines and URLs from various sources\"\"\"\n",
    "        if sources is None:\n",
    "            sources = [\"techcrunch\", \"arstechnica\", \"theverge\", \"wired\", \"venturebeat\"]\n",
    "\n",
    "        scraped_data = []\n",
    "\n",
    "        # Mock scraping implementation (replace with real RSS/API scraping)\n",
    "        for source in sources:\n",
    "            for i in range(max_articles_per_source):\n",
    "                article = {\n",
    "                    'title': f\"{source} AI Article {i+1}: Latest developments in machine learning\",\n",
    "                    'url': f\"https://{source}.com/ai-article-{i+1}\",\n",
    "                    'source': source,\n",
    "                    'published_at': (datetime.now() - timedelta(hours=i)).isoformat(),\n",
    "                    'description': f\"AI-related content from {source}\"\n",
    "                }\n",
    "                scraped_data.append(article)\n",
    "\n",
    "        wrapper.context.raw_headlines = scraped_data\n",
    "        wrapper.context.scraped_urls = [article['url'] for article in scraped_data]\n",
    "        wrapper.context.current_step = 1\n",
    "\n",
    "        return f\"✅ Step 1 Complete: Scraped {len(scraped_data)} headlines from {len(sources)} sources\"\n",
    "\n",
    "\n",
    "    @function_tool\n",
    "    async def step2_filter_ai_content(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        ai_keywords: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Step 2: Filter headlines to AI-related content only\"\"\"\n",
    "        if not wrapper.context.raw_headlines:\n",
    "            return \"❌ No headlines to filter. Run step 1 first.\"\n",
    "\n",
    "        if ai_keywords is None:\n",
    "            ai_keywords = [\n",
    "                'ai', 'artificial intelligence', 'machine learning', 'deep learning',\n",
    "                'neural network', 'llm', 'gpt', 'transformer', 'chatbot', 'automation',\n",
    "                'computer vision', 'nlp', 'natural language', 'algorithm', 'model'\n",
    "            ]\n",
    "\n",
    "        ai_articles = []\n",
    "        for article in wrapper.context.raw_headlines:\n",
    "            title_lower = article['title'].lower()\n",
    "            desc_lower = article['description'].lower()\n",
    "\n",
    "            # Check if any AI keywords are present\n",
    "            if any(keyword in title_lower or keyword in desc_lower for keyword in ai_keywords):\n",
    "                ai_articles.append(article)\n",
    "\n",
    "        wrapper.context.ai_headlines = pd.DataFrame(ai_articles)\n",
    "        wrapper.context.current_step = 2\n",
    "\n",
    "        return f\"✅ Step 2 Complete: Filtered to {len(ai_articles)} AI-related headlines from {len(wrapper.context.raw_headlines)} total\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step3_fetch_article_texts(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 3: Fetch full article texts from URLs\"\"\"\n",
    "        if wrapper.context.ai_headlines.empty:\n",
    "            return \"❌ No AI headlines to fetch. Complete steps 1-2 first.\"\n",
    "\n",
    "        # Mock article fetching (replace with actual web scraping)\n",
    "        article_texts = {}\n",
    "\n",
    "        for _, row in wrapper.context.ai_headlines.iterrows():\n",
    "            url = row['url']\n",
    "            # Mock article content\n",
    "            article_texts[url] = f\"\"\"\n",
    "            {row['title']}\n",
    "\n",
    "            This is a mock article about AI developments. In a real implementation,\n",
    "            you would use libraries like requests + BeautifulSoup or newspaper3k\n",
    "            to extract the full article text from the URL.\n",
    "\n",
    "            Key points about this AI story:\n",
    "            - Advancement in machine learning techniques\n",
    "            - Impact on industry applications\n",
    "            - Future implications for AI development\n",
    "\n",
    "            This content would be much longer in practice, containing the full\n",
    "            article text that needs to be summarized and analyzed.\n",
    "            \"\"\"\n",
    "\n",
    "        wrapper.context.article_texts = article_texts\n",
    "        wrapper.context.current_step = 3\n",
    "\n",
    "        return f\"✅ Step 3 Complete: Fetched full text for {len(article_texts)} articles\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step4_summarize_articles(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 4: Summarize each article to 3 key bullet points\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No article texts to summarize. Complete steps 1-3 first.\"\n",
    "\n",
    "        summaries = {}\n",
    "\n",
    "        for url, text in wrapper.context.article_texts.items():\n",
    "            # Mock summarization (replace with actual LLM summarization)\n",
    "            summaries[url] = [\n",
    "                \"• Key development in AI technology or research\",\n",
    "                \"• Practical implications for businesses or developers\",\n",
    "                \"• Future outlook or next steps in this area\"\n",
    "            ]\n",
    "\n",
    "        wrapper.context.article_summaries = summaries\n",
    "        wrapper.context.current_step = 4\n",
    "\n",
    "        return f\"✅ Step 4 Complete: Generated 3-point summaries for {len(summaries)} articles\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step5_extract_and_cluster_topics(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        max_clusters: int = 8\n",
    "    ) -> str:\n",
    "        \"\"\"Step 5: Extract topics and cluster articles\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No articles to analyze. Complete steps 1-4 first.\"\n",
    "\n",
    "        # Extract topics from each article (mock implementation)\n",
    "        article_topics = {}\n",
    "        all_topics = []\n",
    "\n",
    "        for url, text in wrapper.context.article_texts.items():\n",
    "            # Mock topic extraction (replace with NLP)\n",
    "            topics = ['machine learning', 'business applications', 'research', 'ethics']\n",
    "            article_topics[url] = topics\n",
    "            all_topics.extend(topics)\n",
    "\n",
    "        # Cluster articles by common topics\n",
    "        topic_counts = Counter(all_topics)\n",
    "        main_topics = [topic for topic, count in topic_counts.most_common(max_clusters)]\n",
    "\n",
    "        topic_clusters = {}\n",
    "        for topic in main_topics:\n",
    "            topic_clusters[topic] = [\n",
    "                url for url, topics in article_topics.items()\n",
    "                if topic in topics\n",
    "            ]\n",
    "\n",
    "        wrapper.context.article_topics = article_topics\n",
    "        wrapper.context.topic_clusters = topic_clusters\n",
    "        wrapper.context.current_step = 5\n",
    "\n",
    "        return f\"✅ Step 5 Complete: Extracted topics and created {len(topic_clusters)} clusters\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step6_rate_articles(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        custom_rubric: Dict[str, str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Step 6: Rate articles according to rubric\"\"\"\n",
    "        if not wrapper.context.article_texts:\n",
    "            return \"❌ No articles to rate. Complete previous steps first.\"\n",
    "\n",
    "        if custom_rubric:\n",
    "            wrapper.context.rating_rubric.update(custom_rubric)\n",
    "\n",
    "        # Mock rating (replace with actual evaluation)\n",
    "        ratings = {}\n",
    "        for url in wrapper.context.article_texts.keys():\n",
    "            # Mock scoring based on rubric criteria\n",
    "            relevance_score = 0.8\n",
    "            novelty_score = 0.7\n",
    "            impact_score = 0.9\n",
    "            credibility_score = 0.8\n",
    "\n",
    "            overall_rating = (relevance_score + novelty_score + impact_score + credibility_score) / 4\n",
    "            ratings[url] = overall_rating\n",
    "\n",
    "        wrapper.context.article_ratings = ratings\n",
    "        wrapper.context.current_step = 6\n",
    "\n",
    "        avg_rating = sum(ratings.values()) / len(ratings)\n",
    "        return f\"✅ Step 6 Complete: Rated {len(ratings)} articles. Average rating: {avg_rating:.2f}\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step7_organize_sections(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        target_sections: int = 10\n",
    "    ) -> str:\n",
    "        \"\"\"Step 7: Organize articles into thematic sections\"\"\"\n",
    "        if not wrapper.context.topic_clusters:\n",
    "            return \"❌ No topic clusters available. Complete steps 1-6 first.\"\n",
    "\n",
    "        # Create thematic sections based on clusters and ratings\n",
    "        sections = {}\n",
    "\n",
    "        # Main thematic sections from top clusters\n",
    "        top_clusters = sorted(\n",
    "            wrapper.context.topic_clusters.items(),\n",
    "            key=lambda x: len(x[1]),  # Sort by cluster size\n",
    "            reverse=True\n",
    "        )[:target_sections-1]  # Reserve space for \"Other News\"\n",
    "\n",
    "        for topic, urls in top_clusters:\n",
    "            # Only include high-rated articles\n",
    "            high_rated_urls = [\n",
    "                url for url in urls\n",
    "                if wrapper.context.article_ratings.get(url, 0) >= 0.6\n",
    "            ]\n",
    "            if high_rated_urls:\n",
    "                section_name = topic.title().replace('_', ' ')\n",
    "                sections[section_name] = high_rated_urls\n",
    "\n",
    "        # \"Other News\" section for remaining articles\n",
    "        assigned_urls = set()\n",
    "        for urls in sections.values():\n",
    "            assigned_urls.update(urls)\n",
    "\n",
    "        other_urls = [\n",
    "            url for url in wrapper.context.article_texts.keys()\n",
    "            if url not in assigned_urls and wrapper.context.article_ratings.get(url, 0) >= 0.5\n",
    "        ]\n",
    "\n",
    "        if other_urls:\n",
    "            sections[\"Other News\"] = other_urls\n",
    "\n",
    "        wrapper.context.thematic_sections = sections\n",
    "        wrapper.context.section_names = list(sections.keys())\n",
    "        wrapper.context.current_step = 7\n",
    "\n",
    "        section_summary = \"\\n\".join([\n",
    "            f\"• {name}: {len(urls)} articles\"\n",
    "            for name, urls in sections.items()\n",
    "        ])\n",
    "\n",
    "        return f\"✅ Step 7 Complete: Organized into {len(sections)} sections:\\n{section_summary}\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step8_write_sections(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Step 8: Write content for each thematic section\"\"\"\n",
    "        if not wrapper.context.thematic_sections:\n",
    "            return \"❌ No sections to write. Complete steps 1-7 first.\"\n",
    "\n",
    "        section_drafts = {}\n",
    "\n",
    "        for section_name, urls in wrapper.context.thematic_sections.items():\n",
    "            # Gather content for this section\n",
    "            section_articles = []\n",
    "\n",
    "            for url in urls:\n",
    "                summary = wrapper.context.article_summaries.get(url, [])\n",
    "                rating = wrapper.context.article_ratings.get(url, 0)\n",
    "\n",
    "                # Get article title from DataFrame\n",
    "                article_row = wrapper.context.ai_headlines[\n",
    "                    wrapper.context.ai_headlines['url'] == url\n",
    "                ]\n",
    "                title = article_row['title'].iloc[0] if not article_row.empty else \"Unknown Title\"\n",
    "\n",
    "                section_articles.append({\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'summary': summary,\n",
    "                    'rating': rating\n",
    "                })\n",
    "\n",
    "            # Write section content (mock implementation)\n",
    "            section_content = f\"## {section_name}\\n\\n\"\n",
    "\n",
    "            for article in sorted(section_articles, key=lambda x: x['rating'], reverse=True):\n",
    "                section_content += f\"**{article['title']}**\\n\"\n",
    "                for bullet in article['summary']:\n",
    "                    section_content += f\"{bullet}\\n\"\n",
    "                section_content += f\"[Read more]({article['url']})\\n\\n\"\n",
    "\n",
    "            section_drafts[section_name] = section_content\n",
    "\n",
    "        wrapper.context.section_drafts = section_drafts\n",
    "        wrapper.context.current_step = 8\n",
    "\n",
    "        return f\"✅ Step 8 Complete: Wrote content for {len(section_drafts)} sections\"\n",
    "\n",
    "    @function_tool\n",
    "    async def step9_finalize_newsletter(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        newsletter_title: str = \"AI Weekly Newsletter\"\n",
    "    ) -> str:\n",
    "        \"\"\"Step 9: Combine sections and polish final newsletter\"\"\"\n",
    "        if not wrapper.context.section_drafts:\n",
    "            return \"❌ No section drafts available. Complete steps 1-8 first.\"\n",
    "\n",
    "        # Combine all sections\n",
    "        newsletter_content = f\"# {newsletter_title}\\n\"\n",
    "        newsletter_content += f\"*Generated on {datetime.now().strftime('%B %d, %Y')}*\\n\\n\"\n",
    "\n",
    "        # Add introduction\n",
    "        total_articles = len(wrapper.context.article_texts)\n",
    "        newsletter_content += f\"This week's AI newsletter covers {total_articles} key developments across {len(wrapper.context.section_drafts)} areas of AI.\\n\\n\"\n",
    "\n",
    "        # Add each section\n",
    "        for section_name in wrapper.context.section_names:\n",
    "            if section_name in wrapper.context.section_drafts:\n",
    "                newsletter_content += wrapper.context.section_drafts[section_name]\n",
    "                newsletter_content += \"\\n---\\n\\n\"\n",
    "\n",
    "        # Add footer\n",
    "        newsletter_content += \"*Thank you for reading! This newsletter was generated using AI curation and analysis.*\"\n",
    "\n",
    "        wrapper.context.final_newsletter = newsletter_content\n",
    "        wrapper.context.workflow_complete = True\n",
    "        wrapper.context.current_step = 9\n",
    "\n",
    "        return f\"✅ Step 9 Complete: Finalized newsletter with {len(wrapper.context.section_drafts)} sections\"\n",
    "\n",
    "    @function_tool\n",
    "    async def get_workflow_status(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Get detailed workflow progress status\"\"\"\n",
    "        state = wrapper.context\n",
    "\n",
    "        status = {\n",
    "            'current_step': state.current_step,\n",
    "            'steps_completed': [\n",
    "                f\"1. Scraping: {len(state.raw_headlines)} headlines\" if state.raw_headlines else \"1. Scraping: Pending\",\n",
    "                f\"2. AI Filtering: {len(state.ai_headlines)} AI articles\" if not state.ai_headlines.empty else \"2. AI Filtering: Pending\",\n",
    "                f\"3. Text Fetching: {len(state.article_texts)} articles\" if state.article_texts else \"3. Text Fetching: Pending\",\n",
    "                f\"4. Summarization: {len(state.article_summaries)} summaries\" if state.article_summaries else \"4. Summarization: Pending\",\n",
    "                f\"5. Topic Clustering: {len(state.topic_clusters)} clusters\" if state.topic_clusters else \"5. Topic Clustering: Pending\",\n",
    "                f\"6. Article Rating: {len(state.article_ratings)} rated\" if state.article_ratings else \"6. Article Rating: Pending\",\n",
    "                f\"7. Section Organization: {len(state.thematic_sections)} sections\" if state.thematic_sections else \"7. Section Organization: Pending\",\n",
    "                f\"8. Section Writing: {len(state.section_drafts)} drafts\" if state.section_drafts else \"8. Section Writing: Pending\",\n",
    "                f\"9. Newsletter Finalization: {'Complete' if state.final_newsletter else 'Pending'}\"\n",
    "            ],\n",
    "            'workflow_complete': state.workflow_complete\n",
    "        }\n",
    "\n",
    "        return f\"Newsletter Workflow Status:\\n\\n\" + \"\\n\".join(status['steps_completed'])\n",
    "\n",
    "    @function_tool\n",
    "    async def run_complete_workflow(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        sources: List[str] = None,\n",
    "        ai_keywords: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Run the complete 9-step workflow automatically\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Execute each step in sequence\n",
    "        result1 = await self.step1_scrape_headlines(wrapper, sources)\n",
    "        results.append(result1)\n",
    "\n",
    "        result2 = await self.step2_filter_ai_content(wrapper, ai_keywords)\n",
    "        results.append(result2)\n",
    "\n",
    "        result3 = await self.step3_fetch_article_texts(wrapper)\n",
    "        results.append(result3)\n",
    "\n",
    "        result4 = await self.step4_summarize_articles(wrapper)\n",
    "        results.append(result4)\n",
    "\n",
    "        result5 = await self.step5_extract_and_cluster_topics(wrapper)\n",
    "        results.append(result5)\n",
    "\n",
    "        result6 = await self.step6_rate_articles(wrapper)\n",
    "        results.append(result6)\n",
    "\n",
    "        result7 = await self.step7_organize_sections(wrapper)\n",
    "        results.append(result7)\n",
    "\n",
    "        result8 = await self.step8_write_sections(wrapper)\n",
    "        results.append(result8)\n",
    "\n",
    "        result9 = await self.step9_finalize_newsletter(wrapper)\n",
    "        results.append(result9)\n",
    "\n",
    "        newsletter_length = len(wrapper.context.final_newsletter)\n",
    "\n",
    "        return \"\\n\".join(results) + f\"\\n\\n🎉 Complete workflow finished! Newsletter ready ({newsletter_length} characters)\"\n",
    "\n",
    "    @function_tool\n",
    "    async def reset_workflow(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState]\n",
    "    ) -> str:\n",
    "        \"\"\"Reset workflow to start fresh\"\"\"\n",
    "        wrapper.context.__dict__.update(NewsletterState().__dict__)\n",
    "        return \"🔄 Workflow reset. Ready to start step 1.\"\n",
    "\n",
    "    @function_tool\n",
    "    async def get_newsletter_preview(\n",
    "        self,\n",
    "        wrapper: RunContextWrapper[NewsletterState],\n",
    "        max_chars: int = 500\n",
    "    ) -> str:\n",
    "        \"\"\"Get a preview of the current newsletter\"\"\"\n",
    "        if not wrapper.context.final_newsletter:\n",
    "            return \"Newsletter not ready yet. Complete the full workflow first.\"\n",
    "\n",
    "        preview = wrapper.context.final_newsletter[:max_chars]\n",
    "        if len(wrapper.context.final_newsletter) > max_chars:\n",
    "            preview += \"...\"\n",
    "\n",
    "        return f\"Newsletter Preview:\\n\\n{preview}\"\n",
    "\n",
    "    async def run_step(self, user_input: str) -> str:\n",
    "        \"\"\"Run a workflow step with persistent state\"\"\"\n",
    "        result = await Runner.run(\n",
    "            self,\n",
    "            user_input,\n",
    "            session=self.session,\n",
    "            context=self.state\n",
    "        )\n",
    "        return result.final_output\n",
    "\n",
    "    def save_newsletter(self, filepath: str = None):\n",
    "        \"\"\"Save the final newsletter to file\"\"\"\n",
    "        if not self.state.final_newsletter:\n",
    "            print(\"No newsletter to save. Complete workflow first.\")\n",
    "            return\n",
    "\n",
    "        if filepath is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"ai_newsletter_{timestamp}.md\"\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(self.state.final_newsletter)\n",
    "\n",
    "        print(f\"Newsletter saved to {filepath}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"http://localhost:8787/v1\",\n",
    "  api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "  default_headers={\"x-portkey-provider\": \"openai\"}\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from portkey_ai import Portkey\n",
    "\n",
    "client = Portkey(\n",
    "    provider=\"openai\",\n",
    "    Authorization=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Example: Send a chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61793fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State of the LangGraph agent.\n",
    "    Each node in the graph is a function that takes the current state and returns the updated state.\n",
    "    \"\"\"\n",
    "\n",
    "    # the current working set of headlines (pandas dataframe not supported)\n",
    "    AIdf: list[dict]\n",
    "    # ignore stories before this date for deduplication (force reprocess since)\n",
    "    model_low: str     # cheap fast model like gpt-4o-mini or flash\n",
    "    model_medium: str  # medium model like gpt-4o or gemini-1.5-pro\n",
    "    model_high: str    # slow expensive thinking model like o3-mini\n",
    "    sources: dict  # sources to scrap\n",
    "    sources_reverse: dict[str, str]  # map file names to sources\n",
    "\n",
    "state = AgentState()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES_FILE = \"sources.yaml\"\n",
    "\n",
    "def initialize(state, sources_file=SOURCES_FILE) -> Dict[str, Any]:\n",
    "    \"\"\"Read and parse the sources.yaml file.\"\"\"\n",
    "    try:\n",
    "        with open(sources_file, 'r', encoding='utf-8') as file:\n",
    "            state[\"sources\"] =  yaml.safe_load(file)\n",
    "        state[\"sources_reverse\"] = {v[\"title\"]+\".html\":k for k,v in state[\"sources\"].items()}\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Sources file '{self.sources_file}' not found\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Error parsing YAML file: {e}\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4112b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = initialize(state)\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdk",
   "language": "python",
   "name": "asdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
